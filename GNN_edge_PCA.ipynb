{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "from functions_refactor import *\n",
    "from pytorch_util import *\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "reuse = False\n",
    "block = MEGNet_block\n",
    "head = feedforwardHead_Update\n",
    "data = '../Data/{}_data_ACSF_expand_PCA.pickle'\n",
    "batch_size = 32\n",
    "dim = 128\n",
    "epochs = 20\n",
    "clip = 0.4\n",
    "layer1 = 3\n",
    "layer2 = 3\n",
    "factor = 2\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +0.411, val_loss: -0.013, \n",
      "train_vector: +4.31|+1.21|+0.15|-0.37|-0.59|+0.12|-0.59|-0.95, \n",
      "val_vector  : +4.06|+0.39|-0.25|-0.88|-0.98|-0.18|-0.99|-1.29\n",
      "\n",
      "epoch:1, train_loss: -0.201, val_loss: -0.523, \n",
      "train_vector: +3.08|+0.34|-0.32|-0.95|-1.12|-0.24|-1.04|-1.37, \n",
      "val_vector  : +0.98|+0.24|-0.48|-0.74|-1.22|-0.37|-1.20|-1.41\n",
      "\n",
      "epoch:2, train_loss: -0.629, val_loss: -0.729, \n",
      "train_vector: +0.92|+0.13|-0.48|-1.19|-1.31|-0.38|-1.18|-1.53, \n",
      "val_vector  : +0.45|+0.46|-0.64|-1.36|-1.37|-0.46|-1.34|-1.56\n",
      "\n",
      "epoch:3, train_loss: -0.750, val_loss: -0.849, \n",
      "train_vector: +0.81|-0.06|-0.60|-1.31|-1.44|-0.47|-1.29|-1.65, \n",
      "val_vector  : +0.71|-0.23|-0.67|-1.52|-1.42|-0.54|-1.41|-1.72\n",
      "\n",
      "epoch:4, train_loss: -0.826, val_loss: -0.980, \n",
      "train_vector: +0.77|-0.13|-0.68|-1.42|-1.52|-0.53|-1.36|-1.74, \n",
      "val_vector  : +0.29|-0.51|-0.82|-1.32|-1.64|-0.61|-1.46|-1.78\n",
      "\n",
      "epoch:5, train_loss: -0.893, val_loss: -1.079, \n",
      "train_vector: +0.73|-0.23|-0.74|-1.50|-1.60|-0.58|-1.43|-1.80, \n",
      "val_vector  : +0.27|-0.49|-0.84|-1.81|-1.69|-0.66|-1.55|-1.84\n",
      "\n",
      "epoch:6, train_loss: -0.942, val_loss: -1.040, \n",
      "train_vector: +0.71|-0.25|-0.79|-1.57|-1.66|-0.62|-1.48|-1.86, \n",
      "val_vector  : +0.33|-0.56|-0.86|-1.33|-1.74|-0.70|-1.58|-1.89\n",
      "\n",
      "epoch:7, train_loss: -0.998, val_loss: -0.930, \n",
      "train_vector: +0.68|-0.36|-0.85|-1.63|-1.72|-0.67|-1.54|-1.91, \n",
      "val_vector  : +0.67|+0.37|-0.91|-1.51|-1.70|-0.74|-1.65|-1.97\n",
      "\n",
      "epoch:8, train_loss: -1.035, val_loss: -1.096, \n",
      "train_vector: +0.65|-0.40|-0.89|-1.67|-1.75|-0.70|-1.57|-1.96, \n",
      "val_vector  : +0.07|-0.30|-0.97|-1.51|-1.78|-0.76|-1.57|-1.95\n",
      "\n",
      "epoch:9, train_loss: -1.077, val_loss: -1.099, \n",
      "train_vector: +0.59|-0.45|-0.92|-1.73|-1.79|-0.73|-1.60|-1.99, \n",
      "val_vector  : +0.70|-0.31|-1.01|-1.82|-1.87|-0.78|-1.70|-1.99\n",
      "\n",
      "epoch:10, train_loss: -1.104, val_loss: -1.254, \n",
      "train_vector: +0.57|-0.45|-0.95|-1.76|-1.83|-0.76|-1.63|-2.03, \n",
      "val_vector  : +0.11|-0.80|-1.05|-1.95|-1.85|-0.77|-1.71|-2.02\n",
      "\n",
      "epoch:11, train_loss: -1.132, val_loss: -1.209, \n",
      "train_vector: +0.54|-0.49|-0.98|-1.78|-1.85|-0.78|-1.66|-2.06, \n",
      "val_vector  : +0.20|-0.73|-1.03|-1.56|-1.92|-0.84|-1.71|-2.08\n",
      "\n",
      "epoch:12, train_loss: -1.166, val_loss: -1.243, \n",
      "train_vector: +0.45|-0.53|-1.00|-1.80|-1.88|-0.80|-1.68|-2.09, \n",
      "val_vector  : +0.19|-0.70|-1.10|-1.89|-1.91|-0.88|-1.59|-2.07\n",
      "\n",
      "epoch:13, train_loss: -1.203, val_loss: -1.245, \n",
      "train_vector: +0.37|-0.57|-1.02|-1.83|-1.91|-0.82|-1.71|-2.12, \n",
      "val_vector  : +0.23|-0.48|-1.08|-1.92|-1.94|-0.88|-1.77|-2.11\n",
      "\n",
      "epoch:14, train_loss: -1.231, val_loss: -1.270, \n",
      "train_vector: +0.31|-0.60|-1.05|-1.85|-1.93|-0.84|-1.73|-2.15, \n",
      "val_vector  : +0.37|-0.76|-1.07|-2.10|-1.90|-0.91|-1.68|-2.12\n",
      "\n",
      "epoch:15, train_loss: -1.259, val_loss: -1.345, \n",
      "train_vector: +0.26|-0.62|-1.07|-1.88|-1.97|-0.86|-1.75|-2.17, \n",
      "val_vector  : -0.06|-0.51|-1.14|-2.06|-2.01|-0.92|-1.86|-2.19\n",
      "\n",
      "epoch:16, train_loss: -1.282, val_loss: -1.383, \n",
      "train_vector: +0.20|-0.64|-1.10|-1.88|-1.99|-0.88|-1.78|-2.20, \n",
      "val_vector  : -0.03|-0.88|-1.15|-2.01|-2.06|-0.93|-1.84|-2.17\n",
      "\n",
      "epoch:17, train_loss: -1.299, val_loss: -1.422, \n",
      "train_vector: +0.19|-0.64|-1.11|-1.93|-2.01|-0.90|-1.79|-2.21, \n",
      "val_vector  : -0.21|-0.89|-1.20|-1.99|-2.01|-0.96|-1.92|-2.18\n",
      "\n",
      "epoch:18, train_loss: -1.328, val_loss: -1.383, \n",
      "train_vector: +0.13|-0.70|-1.13|-1.94|-2.03|-0.92|-1.82|-2.23, \n",
      "val_vector  : -0.03|-0.70|-1.19|-2.08|-1.97|-0.98|-1.91|-2.19\n",
      "\n",
      "epoch:19, train_loss: -1.353, val_loss: -1.352, \n",
      "train_vector: +0.09|-0.73|-1.14|-1.96|-2.05|-0.93|-1.84|-2.26, \n",
      "val_vector  : +0.24|-0.62|-1.21|-2.15|-1.98|-0.98|-1.96|-2.15\n",
      "\n",
      "Training completed in 1681.2952332496643s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GNN_edgeUpdate:\n\tMissing key(s) in state_dict: \"head.linear.0.weight\", \"head.linear.0.bias\", \"head.linear.2.weight\", \"head.linear.2.bias\". \n\tUnexpected key(s) in state_dict: \"head.linear.4.weight\", \"head.linear.4.bias\", \"head.linear.1.weight\", \"head.linear.1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-07b4ad535920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbestWeight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/kaggle/QM/Code/functions_refactor.py\u001b[0m in \u001b[0;36mtrain_type\u001b[0;34m(opt, model, epochs, train_dl, val_dl, paras, clip, typeTrain, train_loss_list, val_loss_list)\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;31m# load best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Model/tmp.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbestWeight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 777\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GNN_edgeUpdate:\n\tMissing key(s) in state_dict: \"head.linear.0.weight\", \"head.linear.0.bias\", \"head.linear.2.weight\", \"head.linear.2.bias\". \n\tUnexpected key(s) in state_dict: \"head.linear.4.weight\", \"head.linear.4.bias\", \"head.linear.1.weight\", \"head.linear.1.bias\". "
     ]
    }
   ],
   "source": [
    "train_dl,val_dl = get_data(data,batch_size)\n",
    "\n",
    "model = GNN_edgeUpdate(reuse,block,head,dim,layer1,layer2,factor,**data_dict[data]).to('cuda:0')\n",
    "\n",
    "paras = trainable_parameter(model)\n",
    "opt = Adam(paras,lr=lr)\n",
    "\n",
    "model,train_loss_list,val_loss_list,bestWeight = train_type(opt,model,epochs,train_dl,val_dl,paras,clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(train_loss_perType,val_loss_perType,reuse,block,\\\n",
    "             head,data,batch_size,dim,clip,layer1,layer2,factor,epochs)\n",
    "save_model_type(bestWeight,opt,reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +0.342, val_loss: -0.065, \n",
      "train_vector: +4.23|+0.92|+0.10|-0.45|-0.58|+0.11|-0.64|-0.96, \n",
      "val_vector  : +3.92|+0.12|-0.29|-0.89|-0.97|-0.18|-0.99|-1.24\n",
      "\n",
      "epoch:1, train_loss: -0.261, val_loss: -0.644, \n",
      "train_vector: +2.65|+0.30|-0.34|-1.01|-1.09|-0.23|-1.06|-1.32, \n",
      "val_vector  : +0.58|-0.09|-0.52|-1.00|-1.14|-0.36|-1.21|-1.41\n",
      "\n",
      "epoch:2, train_loss: -0.631, val_loss: -0.785, \n",
      "train_vector: +0.88|+0.14|-0.51|-1.24|-1.28|-0.36|-1.20|-1.48, \n",
      "val_vector  : +0.61|-0.26|-0.65|-1.47|-1.35|-0.43|-1.18|-1.55\n",
      "\n",
      "epoch:3, train_loss: -0.747, val_loss: -0.930, \n",
      "train_vector: +0.80|-0.04|-0.62|-1.36|-1.40|-0.45|-1.31|-1.59, \n",
      "val_vector  : +0.23|-0.13|-0.75|-1.61|-1.47|-0.54|-1.47|-1.69\n",
      "\n",
      "epoch:4, train_loss: -0.832, val_loss: -0.942, \n",
      "train_vector: +0.75|-0.17|-0.69|-1.47|-1.50|-0.52|-1.39|-1.67, \n",
      "val_vector  : +0.29|-0.09|-0.75|-1.62|-1.55|-0.59|-1.48|-1.74\n",
      "\n",
      "epoch:5, train_loss: -0.887, val_loss: -1.066, \n",
      "train_vector: +0.73|-0.20|-0.76|-1.54|-1.56|-0.57|-1.45|-1.75, \n",
      "val_vector  : +0.31|-0.57|-0.85|-1.74|-1.66|-0.65|-1.55|-1.81\n",
      "\n",
      "epoch:6, train_loss: -0.936, val_loss: -1.085, \n",
      "train_vector: +0.71|-0.27|-0.81|-1.60|-1.61|-0.62|-1.49|-1.81, \n",
      "val_vector  : +0.15|-0.36|-0.90|-1.70|-1.71|-0.70|-1.60|-1.86\n",
      "\n",
      "epoch:7, train_loss: -0.981, val_loss: -1.165, \n",
      "train_vector: +0.68|-0.31|-0.85|-1.67|-1.66|-0.66|-1.52|-1.85, \n",
      "val_vector  : +0.15|-0.68|-0.94|-1.85|-1.72|-0.73|-1.68|-1.88\n",
      "\n",
      "epoch:8, train_loss: -1.011, val_loss: -1.116, \n",
      "train_vector: +0.67|-0.34|-0.89|-1.69|-1.70|-0.69|-1.56|-1.90, \n",
      "val_vector  : +0.30|-0.32|-0.91|-1.92|-1.70|-0.77|-1.67|-1.94\n",
      "\n",
      "epoch:9, train_loss: -1.050, val_loss: -1.102, \n",
      "train_vector: +0.65|-0.41|-0.93|-1.72|-1.74|-0.72|-1.60|-1.94, \n",
      "val_vector  : +0.28|-0.27|-0.99|-1.70|-1.71|-0.79|-1.70|-1.94\n",
      "\n",
      "epoch:10, train_loss: -1.084, val_loss: -1.184, \n",
      "train_vector: +0.60|-0.43|-0.96|-1.77|-1.78|-0.75|-1.62|-1.97, \n",
      "val_vector  : +0.06|-0.30|-1.06|-1.73|-1.86|-0.82|-1.76|-2.01\n",
      "\n",
      "epoch:11, train_loss: -1.117, val_loss: -1.102, \n",
      "train_vector: +0.56|-0.47|-0.99|-1.81|-1.81|-0.77|-1.64|-2.00, \n",
      "val_vector  : +0.05|+0.25|-1.03|-1.91|-1.70|-0.83|-1.61|-2.02\n",
      "\n",
      "epoch:12, train_loss: -1.149, val_loss: -1.220, \n",
      "train_vector: +0.51|-0.50|-1.01|-1.83|-1.84|-0.79|-1.68|-2.04, \n",
      "val_vector  : +0.03|-0.70|-1.09|-1.57|-1.78|-0.84|-1.78|-2.04\n",
      "\n",
      "epoch:13, train_loss: -1.181, val_loss: -1.227, \n",
      "train_vector: +0.43|-0.54|-1.04|-1.85|-1.87|-0.81|-1.70|-2.07, \n",
      "val_vector  : +0.39|-0.52|-1.06|-2.09|-1.86|-0.87|-1.75|-2.04\n",
      "\n",
      "epoch:14, train_loss: -1.201, val_loss: -1.311, \n",
      "train_vector: +0.38|-0.52|-1.06|-1.86|-1.89|-0.84|-1.73|-2.09, \n",
      "val_vector  : +0.08|-0.69|-1.15|-2.01|-1.97|-0.90|-1.74|-2.10\n",
      "\n",
      "epoch:15, train_loss: -1.235, val_loss: -1.343, \n",
      "train_vector: +0.31|-0.58|-1.08|-1.89|-1.91|-0.85|-1.75|-2.12, \n",
      "val_vector  : +0.00|-0.75|-1.17|-2.02|-1.94|-0.90|-1.86|-2.12\n",
      "\n",
      "epoch:16, train_loss: -1.267, val_loss: -1.321, \n",
      "train_vector: +0.24|-0.62|-1.10|-1.92|-1.94|-0.87|-1.77|-2.14, \n",
      "val_vector  : +0.03|-0.63|-1.15|-2.05|-1.93|-0.92|-1.82|-2.10\n",
      "\n",
      "epoch:17, train_loss: -1.284, val_loss: -1.322, \n",
      "train_vector: +0.21|-0.62|-1.12|-1.95|-1.96|-0.89|-1.78|-2.16, \n",
      "val_vector  : +0.25|-0.88|-1.18|-1.94|-1.98|-0.93|-1.78|-2.13\n",
      "\n",
      "epoch:18, train_loss: -1.302, val_loss: -1.443, \n",
      "train_vector: +0.17|-0.63|-1.13|-1.95|-1.98|-0.90|-1.80|-2.19, \n",
      "val_vector  : -0.13|-0.94|-1.21|-2.18|-1.98|-0.97|-1.93|-2.19\n",
      "\n",
      "epoch:19, train_loss: -1.323, val_loss: -1.468, \n",
      "train_vector: +0.14|-0.65|-1.15|-1.95|-2.01|-0.92|-1.82|-2.21, \n",
      "val_vector  : -0.25|-1.03|-1.20|-2.16|-2.03|-0.97|-1.94|-2.16\n",
      "\n",
      "Training completed in 1666.8049392700195s\n"
     ]
    }
   ],
   "source": [
    "# this is using old definition of GNN_edgeUpdate shown below\n",
    "train_dl,val_dl = get_data(data,batch_size)\n",
    "\n",
    "model = GNN_edgeUpdate(reuse,block,dim,layer1,layer2,factor,**data_dict[data]).to('cuda:0')\n",
    "\n",
    "paras = trainable_parameter(model)\n",
    "opt = Adam(paras,lr=lr)\n",
    "\n",
    "model,train_loss_perType,val_loss_perType = train(opt,model,epochs,train_dl,val_dl,paras,clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(train_loss_perType,val_loss_perType,reuse,block,\\\n",
    "             head,data,batch_size,dim,clip,layer1,layer2,factor,epochs)\n",
    "save_model(model,opt,reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,val_dl = get_data(data,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_edgeUpdate(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,reuse,block,dim,layer1,layer2,factor,\\\n",
    "                 node_in,edge_in,edge_in4,edge_in3=8):\n",
    "        # block,head are nn.Module\n",
    "        # node_in,edge_in are dim for bonding and edge_in4,edge_in3 for coupling\n",
    "        super(GNN_edgeUpdate, self).__init__()\n",
    "        self.lin_node = Sequential(BatchNorm1d(node_in),Linear(node_in, dim*factor),LeakyReLU(), \\\n",
    "                                   BatchNorm1d(dim*factor),Linear(dim*factor, dim),LeakyReLU())\n",
    "\n",
    "        self.edge1 = Sequential(BatchNorm1d(edge_in),Linear(edge_in, dim*factor),LeakyReLU(), \\\n",
    "                                   BatchNorm1d(dim*factor),Linear(dim*factor, dim),LeakyReLU())\n",
    "\n",
    "        self.edge2 = Sequential(BatchNorm1d(edge_in4+edge_in3),Linear(edge_in4+edge_in3, dim*factor),LeakyReLU(), \\\n",
    "                                   BatchNorm1d(dim*factor),Linear(dim*factor, dim),LeakyReLU())        \n",
    "        if reuse:\n",
    "            self.conv1 = block(dim=dim,edge_dim=edge_in)\n",
    "            self.conv2 = block(dim=dim,edge_dim=edge_in3+edge_in4)\n",
    "        else:\n",
    "            self.conv1 = nn.ModuleList([block(dim=dim) for _ in range(layer1)])\n",
    "            self.conv2 = nn.ModuleList([block(dim=dim) for _ in range(layer2)])            \n",
    "        \n",
    "        self.head = Sequential(Linear(dim, dim*factor),ReLU(), \\\n",
    "                               Linear(dim*factor, 1))\n",
    "        \n",
    "    def forward(self, data,IsTrain=False,typeTrain=False,weight=None):\n",
    "        out = self.lin_node(data.x)\n",
    "        # edge_*3 only does not repeat for undirected graph. Hence need to add (j,i) to (i,j) in edges\n",
    "        edge_index3 = torch.cat([data.edge_index3,data.edge_index3[[1,0]]],1)\n",
    "        n = data.edge_attr3.shape[0]\n",
    "        temp_ = self.edge2(torch.cat([data.edge_attr3,data.edge_attr4],1))\n",
    "        edge_attr3 = torch.cat([temp_,temp_],0)\n",
    "        \n",
    "        edge_attr = self.edge1(data.edge_attr)\n",
    "        for conv in self.conv1:\n",
    "            out,edge_attr = conv(out,data.edge_index,edge_attr)\n",
    "        \n",
    "        for conv in self.conv2:\n",
    "            out,edge_attr3 = conv(out,edge_index3,edge_attr3)    \n",
    "        \n",
    "        edge_attr3 = edge_attr3[:n]\n",
    "        if typeTrain:\n",
    "            if IsTrain:\n",
    "                y = data.y[data.type_attr]\n",
    "            edge_attr3 = edge_attr3[data.type_attr]\n",
    "            edge_attr3_old = data.edge_attr3[data.type_attr]\n",
    "        else:\n",
    "            if IsTrain:\n",
    "                y = data.y\n",
    "            edge_attr3_old = data.edge_attr3\n",
    "            \n",
    "        yhat = self.head(edge_attr3).squeeze(1)\n",
    "        \n",
    "        if IsTrain:\n",
    "            k = torch.sum(edge_attr3_old,0)\n",
    "            nonzeroIndex = torch.nonzero(k).squeeze(1)\n",
    "            abs_ = torch.abs(y-yhat).unsqueeze(1)\n",
    "            loss_perType = torch.zeros(8,device='cuda:0')\n",
    "            loss_perType[nonzeroIndex] = torch.log(torch.sum(abs_ * edge_attr3_old[:,nonzeroIndex],0)/k[nonzeroIndex])\n",
    "            loss = torch.sum(loss_perType)/nonzeroIndex.shape[0] if weight is None else torch.sum(loss_perType*weight)/nonzeroIndex.shape[0]\n",
    "            return loss,loss_perType\n",
    "        else:\n",
    "            return yhat\n",
    "        \n",
    "def train(opt,model,epochs,train_dl,val_dl,paras,clip,typeTrain=False,train_loss_list=None,val_loss_list=None,weight=None):\n",
    "    since = time.time()\n",
    "    \n",
    "    lossBest = 1e6\n",
    "    if train_loss_list is None:\n",
    "        train_loss_list,val_loss_list = [],[]\n",
    "        epoch0 = 0\n",
    "    else:\n",
    "        epoch0 = len(train_loss_list)\n",
    "        \n",
    "    opt.zero_grad()\n",
    "    for epoch in range(epochs):\n",
    "        # training #\n",
    "        model.train()\n",
    "        np.random.seed()\n",
    "        train_loss = 0\n",
    "        train_loss_perType = np.zeros(8)\n",
    "        val_loss = 0\n",
    "        val_loss_perType = np.zeros(8)\n",
    "        \n",
    "        for i,data in enumerate(train_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss,loss_perType = model(data,True,typeTrain,weight)\n",
    "            loss.backward()\n",
    "            clip_grad_value_(paras,clip)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "            train_loss_perType += loss_perType.cpu().detach().numpy()\n",
    "            \n",
    "        # evaluating #\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for j,data in enumerate(val_dl):\n",
    "                data = data.to('cuda:0')\n",
    "                loss,loss_perType = model(data,True,typeTrain)\n",
    "                val_loss += loss.item()\n",
    "                val_loss_perType += loss_perType.cpu().detach().numpy()\n",
    "        \n",
    "        # save model\n",
    "        if loss.item()<lossBest:\n",
    "            lossBest = loss.item()\n",
    "            torch.save({'model_state_dict': model.state_dict()},'../Model/tmp.tar')\n",
    "            \n",
    "        print('epoch:{}, train_loss: {:+.3f}, val_loss: {:+.3f}, \\ntrain_vector: {}, \\nval_vector  : {}\\n'.format(epoch+epoch0,train_loss/i,val_loss/j,\\\n",
    "                                                            '|'.join(['%+.2f'%i for i in train_loss_perType/i]),\\\n",
    "                                                            '|'.join(['%+.2f'%i for i in val_loss_perType/j])))\n",
    "        train_loss_list.append(train_loss_perType/i)\n",
    "        val_loss_list.append(val_loss_perType/j)\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training completed in {}s'.format(time_elapsed))\n",
    "    \n",
    "    # load best model\n",
    "    checkpoint = torch.load('../Model/tmp.tar')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model,train_loss_list,val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_edgeUpdate(reuse,block,dim,layer1,layer2,factor,**data_dict[data]).to('cuda:0')\n",
    "\n",
    "paras = trainable_parameter(model)\n",
    "opt = Adam(paras,lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"False_<class 'functions_refactor.MEGNet_block'>_None__data_ACSF_expand.pickle_32_128_0.4_3_3_2_20_base.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../Model/'+file_name)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: -1.343, val_loss: -1.381, \n",
      "train_vector: +0.11|-0.69|-1.17|-1.98|-2.02|-0.94|-1.84|-2.22, \n",
      "val_vector  : +0.07|-0.75|-1.23|-2.06|-1.96|-0.99|-1.94|-2.20\n",
      "\n",
      "epoch:1, train_loss: -1.356, val_loss: -1.422, \n",
      "train_vector: +0.10|-0.68|-1.18|-1.99|-2.03|-0.95|-1.86|-2.25, \n",
      "val_vector  : -0.11|-0.88|-1.24|-2.09|-2.03|-0.96|-1.89|-2.18\n",
      "\n",
      "epoch:2, train_loss: -1.382, val_loss: -1.394, \n",
      "train_vector: +0.05|-0.74|-1.20|-2.01|-2.05|-0.96|-1.87|-2.26, \n",
      "val_vector  : -0.08|-0.97|-1.27|-1.57|-2.05|-1.01|-1.99|-2.21\n",
      "\n",
      "epoch:3, train_loss: -1.399, val_loss: -1.469, \n",
      "train_vector: +0.03|-0.76|-1.21|-2.03|-2.07|-0.97|-1.89|-2.28, \n",
      "val_vector  : -0.13|-0.96|-1.28|-2.12|-2.05|-1.03|-2.00|-2.19\n",
      "\n",
      "epoch:4, train_loss: -1.415, val_loss: -1.440, \n",
      "train_vector: -0.00|-0.75|-1.23|-2.04|-2.10|-0.99|-1.91|-2.30, \n",
      "val_vector  : -0.33|-0.50|-1.26|-2.15|-2.00|-1.03|-2.01|-2.24\n",
      "\n",
      "epoch:5, train_loss: -1.425, val_loss: -1.465, \n",
      "train_vector: -0.00|-0.76|-1.24|-2.07|-2.11|-0.99|-1.92|-2.31, \n",
      "val_vector  : +0.21|-1.05|-1.30|-2.12|-2.12|-1.04|-2.02|-2.28\n",
      "\n",
      "epoch:6, train_loss: -1.447, val_loss: -1.417, \n",
      "train_vector: -0.04|-0.80|-1.25|-2.09|-2.12|-1.01|-1.93|-2.33, \n",
      "val_vector  : -0.18|-0.62|-1.26|-2.01|-1.96|-1.05|-1.97|-2.28\n",
      "\n",
      "epoch:7, train_loss: -1.463, val_loss: -1.480, \n",
      "train_vector: -0.06|-0.82|-1.26|-2.11|-2.14|-1.02|-1.95|-2.35, \n",
      "val_vector  : -0.19|-0.91|-1.30|-2.08|-2.12|-1.08|-1.87|-2.30\n",
      "\n",
      "epoch:8, train_loss: -1.427, val_loss: -1.476, \n",
      "train_vector: -0.04|-0.63|-1.25|-2.09|-2.13|-1.01|-1.94|-2.33, \n",
      "val_vector  : -0.29|-0.60|-1.33|-2.09|-2.14|-1.07|-2.00|-2.28\n",
      "\n",
      "epoch:9, train_loss: -1.483, val_loss: -1.545, \n",
      "train_vector: -0.09|-0.83|-1.29|-2.12|-2.17|-1.04|-1.97|-2.37, \n",
      "val_vector  : -0.16|-1.02|-1.33|-2.29|-2.16|-1.07|-2.02|-2.31\n",
      "\n",
      "Training completed in 828.8543310165405s\n"
     ]
    }
   ],
   "source": [
    "model,train_loss_perType,val_loss_perType = train(opt,model,10,train_dl,val_dl,paras,clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: -1.494, val_loss: -1.456, \n",
      "train_vector: -0.10|-0.84|-1.30|-2.12|-2.17|-1.05|-1.98|-2.39, \n",
      "val_vector  : +0.24|-0.93|-1.35|-2.01|-2.17|-1.08|-2.02|-2.33\n",
      "\n",
      "epoch:1, train_loss: -1.507, val_loss: -1.568, \n",
      "train_vector: -0.12|-0.86|-1.31|-2.13|-2.19|-1.06|-1.99|-2.40, \n",
      "val_vector  : -0.22|-1.14|-1.35|-2.20|-2.14|-1.10|-2.08|-2.31\n",
      "\n",
      "epoch:2, train_loss: -1.519, val_loss: -1.583, \n",
      "train_vector: -0.14|-0.87|-1.31|-2.16|-2.20|-1.06|-2.00|-2.41, \n",
      "val_vector  : -0.15|-1.19|-1.37|-2.29|-2.12|-1.10|-2.11|-2.35\n",
      "\n",
      "epoch:3, train_loss: -1.532, val_loss: -1.603, \n",
      "train_vector: -0.14|-0.90|-1.33|-2.16|-2.22|-1.07|-2.02|-2.42, \n",
      "val_vector  : -0.44|-0.93|-1.39|-2.33|-2.18|-1.12|-2.08|-2.35\n",
      "\n",
      "epoch:4, train_loss: -1.539, val_loss: -1.499, \n",
      "train_vector: -0.16|-0.88|-1.34|-2.17|-2.22|-1.08|-2.03|-2.44, \n",
      "val_vector  : -0.04|-0.47|-1.39|-2.35|-2.19|-1.11|-2.10|-2.34\n",
      "\n",
      "epoch:5, train_loss: -1.548, val_loss: -1.523, \n",
      "train_vector: -0.16|-0.88|-1.34|-2.19|-2.24|-1.09|-2.03|-2.45, \n",
      "val_vector  : -0.04|-0.70|-1.39|-2.32|-2.15|-1.12|-2.09|-2.37\n",
      "\n",
      "epoch:6, train_loss: -1.553, val_loss: -1.567, \n",
      "train_vector: -0.17|-0.89|-1.35|-2.18|-2.24|-1.10|-2.04|-2.46, \n",
      "val_vector  : -0.43|-0.85|-1.37|-2.22|-2.10|-1.12|-2.13|-2.32\n",
      "\n",
      "epoch:7, train_loss: -1.570, val_loss: -1.615, \n",
      "train_vector: -0.19|-0.93|-1.36|-2.19|-2.26|-1.11|-2.05|-2.47, \n",
      "val_vector  : -0.13|-1.24|-1.41|-2.22|-2.25|-1.14|-2.16|-2.37\n",
      "\n",
      "epoch:8, train_loss: -1.576, val_loss: -1.604, \n",
      "train_vector: -0.19|-0.92|-1.37|-2.21|-2.26|-1.11|-2.06|-2.48, \n",
      "val_vector  : -0.36|-0.82|-1.42|-2.41|-2.19|-1.15|-2.11|-2.37\n",
      "\n",
      "epoch:9, train_loss: -1.586, val_loss: -1.621, \n",
      "train_vector: -0.20|-0.94|-1.38|-2.21|-2.28|-1.12|-2.07|-2.49, \n",
      "val_vector  : -0.12|-1.17|-1.43|-2.31|-2.26|-1.16|-2.14|-2.38\n",
      "\n",
      "Training completed in 828.2205407619476s\n"
     ]
    }
   ],
   "source": [
    "model,train_loss_perType,val_loss_perType = train(opt,model,10,train_dl,val_dl,paras,clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: -1.593, val_loss: -1.631, \n",
      "train_vector: -0.21|-0.96|-1.38|-2.23|-2.27|-1.12|-2.07|-2.50, \n",
      "val_vector  : -0.32|-1.06|-1.40|-2.37|-2.25|-1.16|-2.11|-2.38\n",
      "\n",
      "epoch:1, train_loss: -1.597, val_loss: -1.569, \n",
      "train_vector: -0.22|-0.94|-1.39|-2.21|-2.29|-1.13|-2.08|-2.51, \n",
      "val_vector  : -0.21|-0.65|-1.43|-2.38|-2.17|-1.17|-2.17|-2.38\n",
      "\n",
      "epoch:2, train_loss: -1.608, val_loss: -1.581, \n",
      "train_vector: -0.24|-0.98|-1.39|-2.24|-2.29|-1.14|-2.09|-2.51, \n",
      "val_vector  : +0.19|-1.08|-1.42|-2.34|-2.24|-1.17|-2.19|-2.39\n",
      "\n",
      "epoch:3, train_loss: -1.615, val_loss: -1.624, \n",
      "train_vector: -0.24|-0.98|-1.40|-2.23|-2.31|-1.14|-2.09|-2.52, \n",
      "val_vector  : -0.39|-1.15|-1.43|-2.37|-2.17|-1.17|-1.92|-2.40\n",
      "\n",
      "epoch:4, train_loss: -1.620, val_loss: -1.612, \n",
      "train_vector: -0.24|-0.96|-1.41|-2.24|-2.31|-1.15|-2.11|-2.54, \n",
      "val_vector  : -0.45|-0.78|-1.43|-2.26|-2.24|-1.17|-2.16|-2.41\n",
      "\n",
      "epoch:5, train_loss: -1.627, val_loss: -1.453, \n",
      "train_vector: -0.24|-0.98|-1.42|-2.25|-2.32|-1.16|-2.11|-2.54, \n",
      "val_vector  : +0.81|-0.72|-1.43|-2.42|-2.23|-1.18|-2.05|-2.41\n",
      "\n",
      "epoch:6, train_loss: -1.641, val_loss: -1.655, \n",
      "train_vector: -0.27|-1.01|-1.42|-2.25|-2.34|-1.16|-2.12|-2.56, \n",
      "val_vector  : -0.09|-1.22|-1.43|-2.46|-2.28|-1.19|-2.14|-2.43\n",
      "\n",
      "epoch:7, train_loss: -1.642, val_loss: -1.649, \n",
      "train_vector: -0.25|-1.00|-1.43|-2.26|-2.34|-1.17|-2.12|-2.56, \n",
      "val_vector  : -0.18|-1.21|-1.44|-2.25|-2.32|-1.19|-2.16|-2.44\n",
      "\n",
      "epoch:8, train_loss: -1.652, val_loss: -1.522, \n",
      "train_vector: -0.27|-1.02|-1.44|-2.27|-2.35|-1.18|-2.13|-2.57, \n",
      "val_vector  : +0.07|-0.61|-1.47|-2.37|-2.21|-1.20|-2.02|-2.37\n",
      "\n",
      "epoch:9, train_loss: -1.655, val_loss: -1.572, \n",
      "train_vector: -0.26|-1.00|-1.44|-2.28|-2.36|-1.18|-2.14|-2.58, \n",
      "val_vector  : -0.34|-0.30|-1.48|-2.42|-2.29|-1.20|-2.12|-2.43\n",
      "\n",
      "Training completed in 829.4752721786499s\n"
     ]
    }
   ],
   "source": [
    "model,train_loss_perType,val_loss_perType = train(opt,model,10,train_dl,val_dl,paras,clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(train_loss_perType,val_loss_perType,reuse,block,\\\n",
    "             'None',data,batch_size,dim,clip,layer1,layer2,factor,epochs)\n",
    "save_model(model,opt,reuse,block,'None',data,batch_size,dim,clip,layer1,layer2,factor,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.2*torch.ones(8).to('cuda:0')\n",
    "weight[6] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: -0.558, val_loss: -1.607, \n",
      "train_vector: -0.38|-1.26|-1.45|-2.41|-2.50|-1.17|-2.10|-2.68, \n",
      "val_vector  : -0.17|-0.81|-1.45|-2.40|-2.30|-1.18|-2.06|-2.48\n",
      "\n",
      "epoch:1, train_loss: -0.541, val_loss: -1.569, \n",
      "train_vector: -0.31|-1.14|-1.41|-2.32|-2.43|-1.14|-2.06|-2.62, \n",
      "val_vector  : +0.25|-1.14|-1.43|-2.23|-2.33|-1.15|-2.05|-2.48\n",
      "\n",
      "epoch:2, train_loss: -0.534, val_loss: -1.611, \n",
      "train_vector: -0.27|-1.09|-1.38|-2.28|-2.39|-1.12|-2.05|-2.59, \n",
      "val_vector  : -0.17|-1.06|-1.42|-2.26|-2.31|-1.14|-2.08|-2.46\n",
      "\n",
      "epoch:3, train_loss: -0.536, val_loss: -1.586, \n",
      "train_vector: -0.27|-1.09|-1.38|-2.27|-2.38|-1.11|-2.07|-2.57, \n",
      "val_vector  : -0.28|-0.85|-1.39|-2.15|-2.32|-1.14|-2.13|-2.44\n",
      "\n",
      "epoch:4, train_loss: -0.540, val_loss: -1.418, \n",
      "train_vector: -0.27|-1.10|-1.38|-2.27|-2.38|-1.11|-2.10|-2.58, \n",
      "val_vector  : +0.66|-0.36|-1.40|-2.27|-2.28|-1.14|-2.12|-2.44\n",
      "\n",
      "epoch:5, train_loss: -0.537, val_loss: -1.653, \n",
      "train_vector: -0.24|-0.94|-1.38|-2.25|-2.38|-1.11|-2.12|-2.57, \n",
      "val_vector  : -0.32|-1.14|-1.42|-2.34|-2.27|-1.14|-2.14|-2.47\n",
      "\n",
      "epoch:6, train_loss: -0.544, val_loss: -1.563, \n",
      "train_vector: -0.26|-1.06|-1.38|-2.28|-2.38|-1.11|-2.14|-2.58, \n",
      "val_vector  : +0.31|-1.12|-1.42|-2.19|-2.34|-1.15|-2.15|-2.44\n",
      "\n",
      "epoch:7, train_loss: -0.546, val_loss: -1.539, \n",
      "train_vector: -0.26|-1.08|-1.38|-2.28|-2.39|-1.11|-2.15|-2.58, \n",
      "val_vector  : +0.26|-0.93|-1.39|-2.16|-2.33|-1.14|-2.15|-2.47\n",
      "\n",
      "epoch:8, train_loss: -0.550, val_loss: -1.549, \n",
      "train_vector: -0.26|-1.09|-1.38|-2.29|-2.39|-1.11|-2.18|-2.59, \n",
      "val_vector  : +0.14|-0.79|-1.40|-2.35|-2.30|-1.13|-2.15|-2.42\n",
      "\n",
      "epoch:9, train_loss: -0.553, val_loss: -1.660, \n",
      "train_vector: -0.28|-1.12|-1.39|-2.29|-2.39|-1.12|-2.19|-2.59, \n",
      "val_vector  : -0.42|-1.20|-1.41|-2.23|-2.31|-1.14|-2.13|-2.44\n",
      "\n",
      "Training completed in 846.8230912685394s\n"
     ]
    }
   ],
   "source": [
    "model,train_loss_perType,val_loss_perType = train(opt,model,10,train_dl,val_dl,paras,clip,weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(train_loss_perType,val_loss_perType,reuse,block,\\\n",
    "             'None',data,batch_size,dim,clip,layer1,layer2,factor,epochs)\n",
    "save_model(model,opt,reuse,block,'None',data,batch_size,dim,clip,layer1,layer2,factor,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = cat3Head_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs,'_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = cat3Head_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs,'_2stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
