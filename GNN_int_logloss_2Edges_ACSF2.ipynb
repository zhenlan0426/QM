{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "from functions import *\n",
    "from pytorch_util import *\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/train_data_ACSF.pickle', 'rb') as handle:\n",
    "    train_data = pickle.load(handle)\n",
    "with open('../Data/val_data_ACSF.pickle', 'rb') as handle:\n",
    "    val_data = pickle.load(handle)\n",
    "with open('../Data/test_data_ACSF.pickle', 'rb') as handle:\n",
    "    test_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters ###\n",
    "batch_size = 32\n",
    "dim = 64\n",
    "edge_dim = 12\n",
    "epochs = 5\n",
    "clip = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [Data(**d) for d in train_data]\n",
    "train_dl = DataLoader(train_list,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = [Data(**d) for d in val_data]\n",
    "valid_dl = DataLoader(val_list,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [Data(**d) for d in test_data]\n",
    "test_dl = DataLoader(test_list,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[36, 19], edge_attr3=[58, 8], edge_attr4=[58, 1], edge_index=[2, 36], edge_index3=[2, 58], x=[17, 89], y=[58])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change layers for bonding from 2 to 3\n",
    "class Net_int_2Edges(torch.nn.Module):\n",
    "    # use both types of edges\n",
    "    def __init__(self,dim=64,edge_dim=12,node_in=8,edge_in=19,edge_in3=8):\n",
    "        super(Net_int_2Edges, self).__init__()\n",
    "        self.lin_node = torch.nn.Linear(node_in, dim)\n",
    "        self.lin_edge_attr = torch.nn.Linear(edge_in, edge_dim)\n",
    "        \n",
    "        nn1 = Linear(edge_dim, dim * dim, bias=False)\n",
    "        nn2 = Linear(edge_in3, dim * dim * 2 * 2, bias=False)\n",
    "        \n",
    "        self.conv1 = NNConv(dim, dim, nn1, aggr='mean', root_weight=False)\n",
    "        self.gru1 = GRU(dim, dim)\n",
    "        self.lin_covert = Sequential(BatchNorm1d(dim),Linear(dim, dim*2), \\\n",
    "                                     RReLU(), Dropout(),Linear(dim*2, dim*2),RReLU())\n",
    "        \n",
    "        self.conv2 = NNConv(dim*2, dim*2, nn2, aggr='mean', root_weight=False)\n",
    "        self.gru2 = GRU(dim*2, dim*2)\n",
    "        \n",
    "        self.lin_weight = Linear(8, dim*3*2, bias=False)\n",
    "        self.lin_bias = Linear(8, 1, bias=False)\n",
    "        self.norm = BatchNorm1d(dim*3*2)\n",
    "        self.norm_x = BatchNorm1d(node_in)\n",
    "        \n",
    "    def forward(self, data,IsTrain=False):\n",
    "        out = F.rrelu(self.lin_node(self.norm_x(data.x)))\n",
    "        edge_attr = F.rrelu(self.lin_edge_attr(data.edge_attr))\n",
    "        h = out.unsqueeze(0)\n",
    "        # edge_*3 only does not repeat for undirected graph. Hence need to add (j,i) to (i,j) in edges\n",
    "        edge_index3 = torch.cat([data.edge_index3,data.edge_index3[[1,0]]],1)\n",
    "        edge_attr3 = torch.cat([data.edge_attr3,data.edge_attr3],0)\n",
    "        \n",
    "        for i in range(3):\n",
    "            # using bonding as edge\n",
    "            m = F.rrelu(self.conv1(out, data.edge_index, edge_attr))\n",
    "            out, h = self.gru1(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "        \n",
    "        out = self.lin_covert(out)\n",
    "        h = out.unsqueeze(0)\n",
    "        for i in range(2):\n",
    "            # using couping as edge\n",
    "            m = F.rrelu(self.conv2(out, edge_index3, edge_attr3))\n",
    "            out, h = self.gru2(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)     \n",
    "            \n",
    "        temp = out[data.edge_index3] # (2,N,d)\n",
    "        yhat = torch.cat([temp.mean(0),temp[0]*temp[1],(temp[0]-temp[1])**2],1)\n",
    "        yhat = self.norm(yhat)\n",
    "        weight = self.lin_weight(data.edge_attr3)\n",
    "        bias = self.lin_bias(data.edge_attr3)\n",
    "        yhat = torch.sum(yhat * weight,1,keepdim=True) + bias\n",
    "        yhat = yhat.squeeze(1)\n",
    "        \n",
    "        if IsTrain:\n",
    "            k = torch.sum(data.edge_attr3,0)\n",
    "            nonzeroIndex = torch.nonzero(k).squeeze(1)\n",
    "            abs_ = torch.abs(data.y-yhat).unsqueeze(1)\n",
    "            loss = torch.sum(torch.log(torch.sum(abs_ * data.edge_attr3[:,nonzeroIndex],0)/k[nonzeroIndex]))/nonzeroIndex.shape[0]\n",
    "            return loss\n",
    "        else:\n",
    "            return yhat    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model and set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Net_int_2Edges(dim=dim,edge_dim=edge_dim,node_in=89).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = trainable_parameter(model)\n",
    "opt = Adam(paras,lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:1.1547897267570548, val_loss:0.8259136740468506\n",
      "epoch:1, train_loss:0.7587013766408021, val_loss:0.5219322294633613\n",
      "epoch:2, train_loss:0.4495059319920446, val_loss:0.2876564690158663\n",
      "epoch:3, train_loss:0.330116195947436, val_loss:0.1743541665904574\n",
      "epoch:4, train_loss:0.26789794926998767, val_loss:0.10732892579120448\n",
      "Training completed in 189.63176155090332s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change layers for coupling from 2 to 3\n",
    "class Net_int_2Edges(torch.nn.Module):\n",
    "    # use both types of edges\n",
    "    def __init__(self,dim=64,edge_dim=12,node_in=8,edge_in=19,edge_in3=8):\n",
    "        super(Net_int_2Edges, self).__init__()\n",
    "        self.lin_node = torch.nn.Linear(node_in, dim)\n",
    "        self.lin_edge_attr = torch.nn.Linear(edge_in, edge_dim)\n",
    "        \n",
    "        nn1 = Linear(edge_dim, dim * dim, bias=False)\n",
    "        nn2 = Linear(edge_in3, dim * dim * 2 * 2, bias=False)\n",
    "        \n",
    "        self.conv1 = NNConv(dim, dim, nn1, aggr='mean', root_weight=False)\n",
    "        self.gru1 = GRU(dim, dim)\n",
    "        self.lin_covert = Sequential(BatchNorm1d(dim),Linear(dim, dim*2), \\\n",
    "                                     RReLU(), Dropout(),Linear(dim*2, dim*2),RReLU())\n",
    "        \n",
    "        self.conv2 = NNConv(dim*2, dim*2, nn2, aggr='mean', root_weight=False)\n",
    "        self.gru2 = GRU(dim*2, dim*2)\n",
    "        \n",
    "        self.lin_weight = Linear(8, dim*3*2, bias=False)\n",
    "        self.lin_bias = Linear(8, 1, bias=False)\n",
    "        self.norm = BatchNorm1d(dim*3*2)\n",
    "        self.norm_x = BatchNorm1d(node_in)\n",
    "        \n",
    "    def forward(self, data,IsTrain=False):\n",
    "        out = F.rrelu(self.lin_node(self.norm_x(data.x)))\n",
    "        edge_attr = F.rrelu(self.lin_edge_attr(data.edge_attr))\n",
    "        h = out.unsqueeze(0)\n",
    "        # edge_*3 only does not repeat for undirected graph. Hence need to add (j,i) to (i,j) in edges\n",
    "        edge_index3 = torch.cat([data.edge_index3,data.edge_index3[[1,0]]],1)\n",
    "        edge_attr3 = torch.cat([data.edge_attr3,data.edge_attr3],0)\n",
    "        \n",
    "        for i in range(2):\n",
    "            # using bonding as edge\n",
    "            m = F.rrelu(self.conv1(out, data.edge_index, edge_attr))\n",
    "            out, h = self.gru1(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "        \n",
    "        out = self.lin_covert(out)\n",
    "        h = out.unsqueeze(0)\n",
    "        for i in range(3):\n",
    "            # using couping as edge\n",
    "            m = F.rrelu(self.conv2(out, edge_index3, edge_attr3))\n",
    "            out, h = self.gru2(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)     \n",
    "            \n",
    "        temp = out[data.edge_index3] # (2,N,d)\n",
    "        yhat = torch.cat([temp.mean(0),temp[0]*temp[1],(temp[0]-temp[1])**2],1)\n",
    "        yhat = self.norm(yhat)\n",
    "        weight = self.lin_weight(data.edge_attr3)\n",
    "        bias = self.lin_bias(data.edge_attr3)\n",
    "        yhat = torch.sum(yhat * weight,1,keepdim=True) + bias\n",
    "        yhat = yhat.squeeze(1)\n",
    "        \n",
    "        if IsTrain:\n",
    "            k = torch.sum(data.edge_attr3,0)\n",
    "            nonzeroIndex = torch.nonzero(k).squeeze(1)\n",
    "            abs_ = torch.abs(data.y-yhat).unsqueeze(1)\n",
    "            loss = torch.sum(torch.log(torch.sum(abs_ * data.edge_attr3[:,nonzeroIndex],0)/k[nonzeroIndex]))/nonzeroIndex.shape[0]\n",
    "            return loss\n",
    "        else:\n",
    "            return yhat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Net_int_2Edges(dim=dim,edge_dim=edge_dim,node_in=89).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = trainable_parameter(model)\n",
    "opt = Adam(paras,lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:1.1632127671357317, val_loss:0.7997429537722188\n",
      "epoch:1, train_loss:0.7755193334221022, val_loss:0.5500625839345475\n",
      "epoch:2, train_loss:0.4783027646786337, val_loss:0.2719283985037707\n",
      "epoch:3, train_loss:0.3302032805478458, val_loss:0.19090469108305425\n",
      "epoch:4, train_loss:0.2653944992980267, val_loss:0.1330681057790151\n",
      "Training completed in 207.56409883499146s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untie weights\n",
    "class Net_int_2Edges(torch.nn.Module):\n",
    "    # use both types of edges\n",
    "    def __init__(self,dim=64,edge_dim=12,node_in=8,edge_in=19,edge_in3=8):\n",
    "        super(Net_int_2Edges, self).__init__()\n",
    "        self.lin_node = torch.nn.Linear(node_in, dim)\n",
    "        self.lin_edge_attr = torch.nn.Linear(edge_in, edge_dim)\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([NNConv(dim, dim, Linear(edge_dim, dim * dim, bias=False), \\\n",
    "                                           aggr='mean', root_weight=False) for _ in range(2)])\n",
    "        self.gru1 = nn.ModuleList([GRU(dim, dim) for _ in range(2)])\n",
    "        \n",
    "        self.lin_covert = Sequential(BatchNorm1d(dim),Linear(dim, dim*2), \\\n",
    "                                     RReLU(), Dropout(),Linear(dim*2, dim*2),RReLU())\n",
    "        \n",
    "        self.conv2 = nn.ModuleList([NNConv(dim*2, dim*2, Linear(edge_in3, dim * dim * 2 * 2, bias=False),\\\n",
    "                                           aggr='mean', root_weight=False) for _ in range(2)])\n",
    "        self.gru2 = nn.ModuleList([GRU(dim*2, dim*2) for _ in range(2)])\n",
    "        \n",
    "        self.lin_weight = Linear(8, dim*3*2, bias=False)\n",
    "        self.lin_bias = Linear(8, 1, bias=False)\n",
    "        self.norm = BatchNorm1d(dim*3*2)\n",
    "        self.norm_x = BatchNorm1d(node_in)\n",
    "        \n",
    "    def forward(self, data,IsTrain=False):\n",
    "        out = F.rrelu(self.lin_node(self.norm_x(data.x)))\n",
    "        edge_attr = F.rrelu(self.lin_edge_attr(data.edge_attr))\n",
    "        h = out.unsqueeze(0)\n",
    "        # edge_*3 only does not repeat for undirected graph. Hence need to add (j,i) to (i,j) in edges\n",
    "        edge_index3 = torch.cat([data.edge_index3,data.edge_index3[[1,0]]],1)\n",
    "        edge_attr3 = torch.cat([data.edge_attr3,data.edge_attr3],0)\n",
    "        \n",
    "        for conv,gru in zip(self.conv1,self.gru1):\n",
    "            # using bonding as edge\n",
    "            m = F.rrelu(conv(out, data.edge_index, edge_attr))\n",
    "            out, h = gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "        \n",
    "        out = self.lin_covert(out)\n",
    "        h = out.unsqueeze(0)\n",
    "        \n",
    "        for conv,gru in zip(self.conv2,self.gru2):\n",
    "            # using couping as edge\n",
    "            m = F.rrelu(conv(out, edge_index3, edge_attr3))\n",
    "            out, h = gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)     \n",
    "            \n",
    "        temp = out[data.edge_index3] # (2,N,d)\n",
    "        yhat = torch.cat([temp.mean(0),temp[0]*temp[1],(temp[0]-temp[1])**2],1)\n",
    "        yhat = self.norm(yhat)\n",
    "        weight = self.lin_weight(data.edge_attr3)\n",
    "        bias = self.lin_bias(data.edge_attr3)\n",
    "        yhat = torch.sum(yhat * weight,1,keepdim=True) + bias\n",
    "        yhat = yhat.squeeze(1)\n",
    "        \n",
    "        if IsTrain:\n",
    "            k = torch.sum(data.edge_attr3,0)\n",
    "            nonzeroIndex = torch.nonzero(k).squeeze(1)\n",
    "            abs_ = torch.abs(data.y-yhat).unsqueeze(1)\n",
    "            loss = torch.sum(torch.log(torch.sum(abs_ * data.edge_attr3[:,nonzeroIndex],0)/k[nonzeroIndex]))/nonzeroIndex.shape[0]\n",
    "            return loss\n",
    "        else:\n",
    "            return yhat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Net_int_2Edges(dim=dim,edge_dim=edge_dim,node_in=89).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = trainable_parameter(model)\n",
    "opt = Adam(paras,lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:1.1594490586321378, val_loss:0.8086355328559875\n",
      "epoch:1, train_loss:0.7355345898177126, val_loss:0.5043860161279001\n",
      "epoch:2, train_loss:0.42348383854228067, val_loss:0.18682799665018535\n",
      "epoch:3, train_loss:0.2614997324568254, val_loss:0.1339515131762904\n",
      "epoch:4, train_loss:0.19197627482999663, val_loss:0.034456392805864156\n",
      "Training completed in 169.59033465385437s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.13746430281980648, val_loss:-0.002662224894087029\n",
      "epoch:1, train_loss:0.0945514630480313, val_loss:-0.054346995969486035\n",
      "epoch:2, train_loss:0.053573113426252705, val_loss:-0.0722985656804636\n",
      "epoch:3, train_loss:0.01870357307976356, val_loss:-0.10433403677187669\n",
      "epoch:4, train_loss:-0.014622645656172413, val_loss:-0.14452321372894394\n",
      "Training completed in 169.0499165058136s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.0413524946023271, val_loss:-0.1542664094008187\n",
      "epoch:1, train_loss:-0.065125956247332, val_loss:-0.19085314467899564\n",
      "epoch:2, train_loss:-0.08885088705878907, val_loss:-0.21685146457619137\n",
      "epoch:3, train_loss:-0.11049663226763626, val_loss:-0.24656966812590247\n",
      "epoch:4, train_loss:-0.1302875803692405, val_loss:-0.2716748599185903\n",
      "Training completed in 169.9624707698822s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.15095552611988758, val_loss:-0.28606100918518174\n",
      "epoch:1, train_loss:-0.16858280100572257, val_loss:-0.30791402555620057\n",
      "epoch:2, train_loss:-0.1859920694248748, val_loss:-0.300036578049135\n",
      "epoch:3, train_loss:-0.20444482100755318, val_loss:-0.33359170318222964\n",
      "epoch:4, train_loss:-0.22066242348319273, val_loss:-0.33999692607257104\n",
      "Training completed in 169.8011360168457s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.2333877906782419, val_loss:-0.37278015211097193\n",
      "epoch:1, train_loss:-0.24728182025652362, val_loss:-0.3677361056718052\n",
      "epoch:2, train_loss:-0.2631969788219485, val_loss:-0.3892742553009437\n",
      "epoch:3, train_loss:-0.2746893775942776, val_loss:-0.403559311460226\n",
      "epoch:4, train_loss:-0.28780888158965995, val_loss:-0.43840433653985333\n",
      "Training completed in 171.43177342414856s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.29909109896892927, val_loss:-0.4306042927962083\n",
      "epoch:1, train_loss:-0.3090799196048801, val_loss:-0.4241817949546708\n",
      "epoch:2, train_loss:-0.3196433461354266, val_loss:-0.4257182358867592\n",
      "epoch:3, train_loss:-0.3294254029293174, val_loss:-0.47497534204242575\n",
      "epoch:4, train_loss:-0.34052170920055863, val_loss:-0.44629317541153\n",
      "Training completed in 170.09883213043213s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.3496275109396059, val_loss:-0.4916843884011619\n",
      "epoch:1, train_loss:-0.3603542146273383, val_loss:-0.47067818943506634\n",
      "epoch:2, train_loss:-0.3672700528963331, val_loss:-0.5043829705598007\n",
      "epoch:3, train_loss:-0.37823523475418475, val_loss:-0.506532619906287\n",
      "epoch:4, train_loss:-0.3872226643396067, val_loss:-0.5169145995353022\n",
      "Training completed in 169.93472170829773s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.392837257504763, val_loss:-0.5248468982485625\n",
      "epoch:1, train_loss:-0.3999615752831899, val_loss:-0.4886773057982453\n",
      "epoch:2, train_loss:-0.41064669176166224, val_loss:-0.522937386845931\n",
      "epoch:3, train_loss:-0.41749444948343256, val_loss:-0.5377823637209387\n",
      "epoch:4, train_loss:-0.4238019130670921, val_loss:-0.5531528641780218\n",
      "Training completed in 170.36451745033264s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.4318233698906286, val_loss:-0.5468512849930005\n",
      "epoch:1, train_loss:-0.43636129336592594, val_loss:-0.5480451825210172\n",
      "epoch:2, train_loss:-0.4462606151408129, val_loss:-0.5532766004276072\n",
      "epoch:3, train_loss:-0.45291176333403443, val_loss:-0.5944341593063794\n",
      "epoch:4, train_loss:-0.4564936625314293, val_loss:-0.5816825665215142\n",
      "Training completed in 169.9697141647339s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.46464196545896763, val_loss:-0.5822738100830306\n",
      "epoch:1, train_loss:-0.47128466078964715, val_loss:-0.5983556186159452\n",
      "epoch:2, train_loss:-0.47720149661587646, val_loss:-0.6123289756922641\n",
      "epoch:3, train_loss:-0.483131457339916, val_loss:-0.6263906406795877\n",
      "epoch:4, train_loss:-0.4875799993374166, val_loss:-0.6074130747180718\n",
      "Training completed in 169.859388589859s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.49376451935079146, val_loss:-0.6199501685352407\n",
      "epoch:1, train_loss:-0.49977215103188444, val_loss:-0.6315819131385567\n",
      "epoch:2, train_loss:-0.5055402557109664, val_loss:-0.6382716247159191\n",
      "epoch:3, train_loss:-0.5121168648755109, val_loss:-0.6393220304933369\n",
      "epoch:4, train_loss:-0.5165831072176762, val_loss:-0.6165233023910441\n",
      "Training completed in 169.81035041809082s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in opt.param_groups:\n",
    "    para['lr'] = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:-0.5633440023423549, val_loss:-0.7016189865704275\n",
      "epoch:1, train_loss:-0.5675841959906213, val_loss:-0.6992784708610966\n",
      "epoch:2, train_loss:-0.570821271552634, val_loss:-0.7009264338984449\n",
      "epoch:3, train_loss:-0.5735327497594804, val_loss:-0.7016119856992339\n",
      "epoch:4, train_loss:-0.5742597658175227, val_loss:-0.7037720715897715\n",
      "Training completed in 170.8988790512085s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    # training #\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_dl):\n",
    "        data = data.to('cuda:0')\n",
    "        loss = model(data,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(valid_dl):\n",
    "            data = data.to('cuda:0')\n",
    "            loss = model(data,True)\n",
    "            val_loss += loss.item()\n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {}s'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../Model/gnn_int_logloss_2Edges.tar')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            }, '../Model/gnn_int_logloss_2Edges_ACSF2.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "yhat_list = []\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        data = data.to('cuda:0')\n",
    "        yhat_list.append(model(data,False))\n",
    "\n",
    "yhat = torch.cat(yhat_list).cpu().detach().numpy()\n",
    "submission = pd.read_csv('../Data/sample_submission.csv')\n",
    "submission['scalar_coupling_constant'] = yhat\n",
    "submission.to_csv('../Submission/gnn_int_logloss_2Edges_ACSF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
