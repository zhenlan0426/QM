{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from attention import *\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_util import RAdam,trainable_parameter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from pytorch_transformers import AdamW,WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "dim = 512\n",
    "head_d = 8\n",
    "head = SimplyInteraction #SimplyHead\n",
    "encoder_layer,decoder_layer = 4,2\n",
    "epochs = 10\n",
    "clip = 0.1\n",
    "logLoss = True\n",
    "EncoderLayer=TransformerEncoderLayer\n",
    "DecoderLayer=TransformerDecoderLayer\n",
    "\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.05\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/train_data_attention_node.pickle', 'rb') as handle:\n",
    "    train_node = pickle.load(handle)\n",
    "with open('../Data/train_data_attention_edge.pickle', 'rb') as handle:\n",
    "    train_edge = pickle.load(handle)\n",
    "with open('../Data/train_data_attention_edge_y.pickle', 'rb') as handle:\n",
    "    train_y = pickle.load(handle)\n",
    "with open('../Data/train_data_ind.pickle', 'rb') as handle:\n",
    "    ind = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = attentionDataset(train_node[:4000000],train_edge[:4000000],ind[:4000000],train_y[:4000000])\n",
    "train_dl = DataLoader(train_dl,shuffle=True,batch_size=batch_size,collate_fn=collate_fn2,num_workers=4)\n",
    "val_dl = attentionDataset(train_node[4000000:],train_edge[4000000:],ind[4000000:],train_y[4000000:])\n",
    "val_dl = DataLoader(val_dl,shuffle=True,batch_size=batch_size,collate_fn=collate_fn2,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/home/will/anaconda3/envs/pytorch/lib/python3.7/site-packages/amp_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN3c106Device8validateEv')\n"
     ]
    }
   ],
   "source": [
    "model=Attention3(dim,encoder_layer,head_d,head,EncoderLayer,\n",
    "                 dropout=dropout,dim_feedforward=dim_feedforward).to('cuda')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-04)\n",
    "model, opt = amp.initialize(model, opt, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out,mask,edge,y = next(iter(train_dl))\n",
    "# out,mask,edge,y = out.to('cuda:0'),mask.to('cuda:0'),edge.to('cuda:0'),y.to('cuda:0')\n",
    "\n",
    "# opt.zero_grad()\n",
    "# loss,loss_perType = model(out,mask,edge,y,logLoss)\n",
    "# loss.backward()\n",
    "\n",
    "# [(n,p.std()) for n,p in model.named_parameters()]\n",
    "\n",
    "# [(n,p.grad.mean()) for n,p in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +0.407, val_loss: +0.238, \n",
      "train_vector: +1.36|+1.86|+0.23|+0.32|+0.04|+0.15|+0.01|-0.72, \n",
      "val_vector  : +1.31|+1.53|-0.09|+0.07|+0.29|-0.01|-0.36|-0.84\n",
      "\n",
      "epoch:1, train_loss: +0.010, val_loss: -0.071, \n",
      "train_vector: +0.99|+1.21|-0.08|-0.08|-0.44|-0.13|-0.35|-1.05, \n",
      "val_vector  : +1.03|+1.10|-0.20|-0.30|-0.04|-0.30|-0.78|-1.07\n",
      "\n",
      "epoch:2, train_loss: -0.197, val_loss: -0.194, \n",
      "train_vector: +0.86|+0.75|-0.25|-0.27|-0.64|-0.28|-0.53|-1.21, \n",
      "val_vector  : +0.88|+1.11|-0.39|-0.37|-0.32|-0.51|-0.78|-1.16\n",
      "\n",
      "epoch:3, train_loss: -0.324, val_loss: -0.250, \n",
      "train_vector: +0.75|+0.56|-0.37|-0.39|-0.77|-0.40|-0.65|-1.32, \n",
      "val_vector  : +1.01|+0.79|-0.45|-0.36|-0.43|-0.57|-0.84|-1.14\n",
      "\n",
      "epoch:4, train_loss: -0.393, val_loss: -0.209, \n",
      "train_vector: +0.70|+0.48|-0.44|-0.46|-0.84|-0.46|-0.72|-1.39, \n",
      "val_vector  : +1.13|+1.20|-0.53|-0.37|-0.45|-0.58|-0.88|-1.18\n",
      "\n",
      "epoch:5, train_loss: -0.450, val_loss: -0.577, \n",
      "train_vector: +0.65|+0.44|-0.50|-0.52|-0.91|-0.52|-0.78|-1.45, \n",
      "val_vector  : +0.73|+0.43|-0.72|-0.71|-0.98|-0.74|-1.14|-1.48\n",
      "\n",
      "epoch:6, train_loss: -0.498, val_loss: -0.566, \n",
      "train_vector: +0.61|+0.38|-0.55|-0.57|-0.97|-0.56|-0.83|-1.50, \n",
      "val_vector  : +0.94|+0.48|-0.82|-0.73|-0.96|-0.79|-1.08|-1.56\n",
      "\n",
      "epoch:7, train_loss: -0.530, val_loss: -0.636, \n",
      "train_vector: +0.58|+0.35|-0.59|-0.60|-1.01|-0.59|-0.86|-1.53, \n",
      "val_vector  : +0.64|+0.46|-0.87|-0.80|-0.95|-0.83|-1.30|-1.44\n",
      "\n",
      "epoch:8, train_loss: -0.564, val_loss: -0.609, \n",
      "train_vector: +0.55|+0.30|-0.62|-0.63|-1.04|-0.62|-0.89|-1.56, \n",
      "val_vector  : +0.60|+0.84|-0.87|-0.78|-1.11|-0.82|-1.13|-1.60\n",
      "\n",
      "epoch:9, train_loss: -0.567, val_loss: -0.666, \n",
      "train_vector: +0.57|+0.31|-0.62|-0.64|-1.07|-0.62|-0.89|-1.57, \n",
      "val_vector  : +0.81|+0.26|-0.95|-0.89|-0.92|-0.94|-1.12|-1.58\n",
      "\n",
      "Training completed in 4702.834298372269s\n"
     ]
    }
   ],
   "source": [
    "model,bestOpt,bestWeight = train_type2(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                       scheduler=scheduler,logLoss=logLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "epoch:0, train_loss: +0.545, val_loss: +0.128, \n",
      "train_vector: +1.60|+1.86|+0.43|+0.45|+0.10|+0.30|+0.22|-0.60, \n",
      "val_vector  : +1.25|+1.21|+0.05|-0.09|-0.34|+0.01|-0.38|-0.69\n",
      "\n",
      "epoch:1, train_loss: -0.053, val_loss: -0.132, \n",
      "train_vector: +0.89|+0.92|-0.10|-0.01|-0.51|-0.10|-0.40|-1.11, \n",
      "val_vector  : +0.82|+0.75|-0.12|+0.16|-0.45|-0.40|-0.77|-1.05\n",
      "\n",
      "epoch:2, train_loss: -0.262, val_loss: -0.322, \n",
      "train_vector: +0.72|+0.62|-0.32|-0.23|-0.70|-0.29|-0.60|-1.28, \n",
      "val_vector  : +0.65|+0.99|-0.55|-0.44|-0.76|-0.55|-0.85|-1.06\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "epoch:3, train_loss: -0.392, val_loss: -0.452, \n",
      "train_vector: +0.63|+0.49|-0.46|-0.40|-0.84|-0.43|-0.72|-1.40, \n",
      "val_vector  : +0.80|+0.80|-0.56|-0.64|-0.81|-0.74|-1.13|-1.34\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "epoch:4, train_loss: -0.485, val_loss: -0.529, \n",
      "train_vector: +0.54|+0.41|-0.56|-0.51|-0.95|-0.53|-0.80|-1.49, \n",
      "val_vector  : +0.49|+0.81|-0.69|-0.61|-1.00|-0.79|-0.99|-1.46\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "epoch:5, train_loss: -0.560, val_loss: -0.639, \n",
      "train_vector: +0.49|+0.33|-0.64|-0.60|-1.03|-0.60|-0.87|-1.57, \n",
      "val_vector  : +0.45|+0.55|-0.73|-0.82|-0.95|-0.78|-1.21|-1.62\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:6, train_loss: -0.625, val_loss: -0.733, \n",
      "train_vector: +0.44|+0.25|-0.70|-0.67|-1.11|-0.66|-0.92|-1.63, \n",
      "val_vector  : +0.31|+0.53|-0.87|-0.82|-1.16|-0.88|-1.32|-1.66\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:7, train_loss: -0.675, val_loss: -0.657, \n",
      "train_vector: +0.41|+0.20|-0.75|-0.71|-1.17|-0.71|-0.98|-1.68, \n",
      "val_vector  : +0.62|+1.08|-0.94|-0.82|-1.18|-0.96|-1.31|-1.74\n",
      "\n",
      "epoch:8, train_loss: -0.724, val_loss: -0.693, \n",
      "train_vector: +0.36|+0.16|-0.80|-0.77|-1.23|-0.76|-1.03|-1.72, \n",
      "val_vector  : +0.29|+1.26|-0.97|-0.89|-1.23|-1.00|-1.24|-1.74\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:9, train_loss: -0.764, val_loss: -0.674, \n",
      "train_vector: +0.34|+0.12|-0.85|-0.81|-1.28|-0.81|-1.07|-1.76, \n",
      "val_vector  : +0.44|+0.61|-0.90|-1.02|-0.84|-0.83|-1.11|-1.74\n",
      "\n",
      "Training completed in 4817.313369750977s\n"
     ]
    }
   ],
   "source": [
    "# O1\n",
    "model,bestOpt,bestWeight = train_type2(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                       scheduler=scheduler,logLoss=logLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict()\n",
    "            }, '../Model/attention_edge_512_4_10.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../Model/attention_edge_512_4_10.tar')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:0, train_loss: -0.805, val_loss: -0.768, \n",
      "train_vector: +0.31|+0.06|-0.89|-0.86|-1.32|-0.84|-1.10|-1.80, \n",
      "val_vector  : +0.19|+0.46|-1.03|-0.98|-1.11|-0.83|-1.08|-1.77\n",
      "\n",
      "epoch:1, train_loss: -0.837, val_loss: -0.870, \n",
      "train_vector: +0.28|+0.03|-0.92|-0.89|-1.36|-0.87|-1.13|-1.83, \n",
      "val_vector  : +0.54|+0.19|-1.10|-0.99|-1.35|-1.02|-1.47|-1.76\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:2, train_loss: -0.867, val_loss: -0.890, \n",
      "train_vector: +0.26|+0.01|-0.95|-0.93|-1.40|-0.90|-1.17|-1.86, \n",
      "val_vector  : +0.16|+0.55|-1.08|-0.98|-1.16|-1.20|-1.54|-1.86\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "epoch:3, train_loss: -0.897, val_loss: -0.905, \n",
      "train_vector: +0.24|-0.02|-0.98|-0.96|-1.43|-0.93|-1.20|-1.89, \n",
      "val_vector  : +0.17|+0.35|-1.17|-1.11|-1.26|-1.06|-1.29|-1.88\n",
      "\n",
      "epoch:4, train_loss: -0.926, val_loss: -0.879, \n",
      "train_vector: +0.21|-0.06|-1.01|-0.99|-1.46|-0.96|-1.23|-1.92, \n",
      "val_vector  : +0.27|+0.23|-1.15|-1.04|-1.10|-0.99|-1.44|-1.80\n",
      "\n",
      "epoch:5, train_loss: -0.949, val_loss: -0.866, \n",
      "train_vector: +0.19|-0.08|-1.03|-1.02|-1.49|-0.98|-1.25|-1.94, \n",
      "val_vector  : +0.25|+0.86|-1.20|-1.09|-1.36|-1.15|-1.36|-1.88\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:6, train_loss: -0.970, val_loss: -1.008, \n",
      "train_vector: +0.18|-0.11|-1.05|-1.04|-1.51|-1.00|-1.27|-1.96, \n",
      "val_vector  : +0.13|+0.19|-1.20|-1.19|-1.44|-1.14|-1.54|-1.86\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:7, train_loss: -0.997, val_loss: -0.940, \n",
      "train_vector: +0.16|-0.14|-1.07|-1.06|-1.55|-1.02|-1.30|-1.98, \n",
      "val_vector  : +0.39|+0.30|-1.20|-1.24|-1.50|-1.15|-1.26|-1.87\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:8, train_loss: -1.015, val_loss: -0.988, \n",
      "train_vector: +0.14|-0.16|-1.09|-1.08|-1.56|-1.04|-1.32|-2.01, \n",
      "val_vector  : +0.26|+0.19|-1.20|-1.17|-1.55|-1.25|-1.25|-1.92\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "epoch:9, train_loss: -1.038, val_loss: -1.009, \n",
      "train_vector: +0.12|-0.19|-1.12|-1.11|-1.59|-1.06|-1.34|-2.03, \n",
      "val_vector  : +0.27|+0.15|-1.20|-1.05|-1.46|-1.24|-1.55|-1.99\n",
      "\n",
      "Training completed in 4824.697202920914s\n"
     ]
    }
   ],
   "source": [
    "# O1 epoch 10 to 20\n",
    "model,bestOpt,bestWeight = train_type2(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                       scheduler=scheduler,logLoss=logLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict()\n",
    "            }, '../Model/attention_edge_512_4_20.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
