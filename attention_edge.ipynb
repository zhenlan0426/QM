{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from attention import *\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_util import RAdam,trainable_parameter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from pytorch_transformers import AdamW,WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "dim = 512\n",
    "head_d = 8\n",
    "head = SimplyInteraction #SimplyHead\n",
    "encoder_layer,decoder_layer = 2,2\n",
    "epochs = 10\n",
    "clip = 0.1\n",
    "logLoss = False\n",
    "EncoderLayer=TransformerEncoderLayer\n",
    "DecoderLayer=TransformerDecoderLayer\n",
    "\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.05\n",
    "lr = 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/train_data_attention_node.pickle', 'rb') as handle:\n",
    "    train_node = pickle.load(handle)\n",
    "with open('../Data/train_data_attention_edge.pickle', 'rb') as handle:\n",
    "    train_edge = pickle.load(handle)\n",
    "with open('../Data/train_data_attention_edge_y.pickle', 'rb') as handle:\n",
    "    train_y = pickle.load(handle)\n",
    "with open('../Data/train_data_ind.pickle', 'rb') as handle:\n",
    "    ind = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = attentionDataset(train_node[:4000000],train_edge[:4000000],ind[:4000000],train_y[:4000000])\n",
    "train_dl = DataLoader(train_dl,shuffle=True,batch_size=batch_size,collate_fn=collate_fn,num_workers=4)\n",
    "val_dl = attentionDataset(train_node[4000000:],train_edge[4000000:],ind[4000000:],train_y[4000000:])\n",
    "val_dl = DataLoader(val_dl,shuffle=True,batch_size=batch_size,collate_fn=collate_fn,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Attention(dim,encoder_layer,decoder_layer,head_d,head,EncoderLayer,DecoderLayer,\n",
    "                dropout=dropout,dim_feedforward=dim_feedforward).to('cuda')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "#opt = AdamW(paras, lr=lr, correct_bias=False)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-04)\n",
    "#scheduler = WarmupLinearSchedule(opt, warmup_steps=40, t_total=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out,mask,edge,y = next(iter(train_dl))\n",
    "# out,mask,edge,y = out.to('cuda:0'),mask.to('cuda:0'),edge.to('cuda:0'),y.to('cuda:0')\n",
    "\n",
    "# opt.zero_grad()\n",
    "# loss,loss_perType = model(out,mask,edge,y,logLoss)\n",
    "# loss.backward()\n",
    "\n",
    "# [(n,p.std()) for n,p in model.named_parameters()]\n",
    "\n",
    "# [(n,p.grad.mean()) for n,p in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +4.873, val_loss: +1.229, \n",
      "train_vector: +2.59|+2.32|+0.97|+0.99|+0.98|+0.90|+1.07|-0.10, \n",
      "val_vector  : +2.44|+2.26|+0.92|+0.80|+1.33|+0.87|+1.17|+0.05\n",
      "\n",
      "epoch:1, train_loss: +4.529, val_loss: +1.299, \n",
      "train_vector: +2.50|+2.26|+0.97|+0.92|+0.97|+0.89|+1.06|-0.16, \n",
      "val_vector  : +2.67|+2.29|+0.92|+1.10|+1.32|+0.86|+1.18|+0.06\n",
      "\n",
      "epoch:2, train_loss: +4.514, val_loss: +1.239, \n",
      "train_vector: +2.50|+2.26|+0.97|+0.92|+0.97|+0.89|+1.06|-0.16, \n",
      "val_vector  : +2.34|+2.49|+0.92|+0.77|+1.32|+0.84|+1.17|+0.06\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-97bee3c83efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model,bestOpt,bestWeight = train_type(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n\u001b[0;32m----> 2\u001b[0;31m                                        scheduler=scheduler,logLoss=logLoss)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/kaggle/QM/Code/attention.py\u001b[0m in \u001b[0;36mtrain_type\u001b[0;34m(opt, model, epochs, train_dl, val_dl, paras, clip, scheduler, logLoss, weight, patience, saveModelEpoch)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model,bestOpt,bestWeight = train_type(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                       scheduler=scheduler,logLoss=logLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6116, 0.3827, 0.2909, 0.7461, 0.9686],\n",
       "         [0.3556, 0.9787, 0.1412, 0.7747, 0.2204],\n",
       "         [0.4146, 0.6009, 0.1438, 0.2266, 0.5505],\n",
       "         [0.9805, 0.9418, 0.1804, 0.8635, 0.4107]],\n",
       "\n",
       "        [[0.1023, 0.9522, 0.9565, 0.6346, 0.4059],\n",
       "         [0.7043, 0.2908, 0.6731, 0.1355, 0.2348],\n",
       "         [0.6275, 0.3243, 0.2004, 0.3885, 0.8659],\n",
       "         [0.6583, 0.5330, 0.9002, 0.4966, 0.6999]],\n",
       "\n",
       "        [[0.6217, 0.1492, 0.1007, 0.0670, 0.0799],\n",
       "         [0.5098, 0.7673, 0.3892, 0.0952, 0.6599],\n",
       "         [0.4721, 0.4205, 0.3383, 0.5202, 0.7436],\n",
       "         [0.7209, 0.2704, 0.3852, 0.5210, 0.7989]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randint(0,4,(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 0],\n",
       "        [2, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4, 5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[b].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
