{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from attention import *\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_util import RAdam,trainable_parameter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#from pytorch_transformers import AdamW,WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "dim = 512\n",
    "head_d = 8\n",
    "head = SimplyInteraction #SimplyHead\n",
    "encoder_layer,decoder_layer = 4,2\n",
    "epochs = 10\n",
    "clip = 0.1\n",
    "logLoss = True\n",
    "EncoderLayer=TransformerEncoderLayer\n",
    "DecoderLayer=TransformerDecoderLayer\n",
    "\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.05\n",
    "lr = 6e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/train_data_attention_node.pickle', 'rb') as handle:\n",
    "    train_node = pickle.load(handle)\n",
    "with open('../Data/train_data_attention_edge.pickle', 'rb') as handle:\n",
    "    train_edge = pickle.load(handle)\n",
    "with open('../Data/train_data_attention_edge_y.pickle', 'rb') as handle:\n",
    "    train_y = pickle.load(handle)\n",
    "with open('../Data/train_data_ind.pickle', 'rb') as handle:\n",
    "    ind = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = attentionDataset(train_node[:4000000],train_edge[:4000000],ind[:4000000],train_y[:4000000])\n",
    "train_dl = DataLoader(train_dl,shuffle=True,batch_size=batch_size,collate_fn=collate_fn2,num_workers=4)\n",
    "val_dl = attentionDataset(train_node[4000000:],train_edge[4000000:],ind[4000000:],train_y[4000000:])\n",
    "val_dl = DataLoader(val_dl,shuffle=True,batch_size=batch_size,collate_fn=collate_fn2,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "model=Attention3(dim,encoder_layer,head_d,head,EncoderLayer,\n",
    "                 dropout=dropout,dim_feedforward=dim_feedforward).to('cuda')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-04)\n",
    "model, opt = amp.initialize(model, opt, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out,mask,edge,y = next(iter(train_dl))\n",
    "# out,mask,edge,y = out.to('cuda:0'),mask.to('cuda:0'),edge.to('cuda:0'),y.to('cuda:0')\n",
    "\n",
    "# opt.zero_grad()\n",
    "# loss,loss_perType = model(out,mask,edge,y,logLoss)\n",
    "# loss.backward()\n",
    "\n",
    "# [(n,p.std()) for n,p in model.named_parameters()]\n",
    "\n",
    "# [(n,p.grad.mean()) for n,p in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +0.407, val_loss: +0.238, \n",
      "train_vector: +1.36|+1.86|+0.23|+0.32|+0.04|+0.15|+0.01|-0.72, \n",
      "val_vector  : +1.31|+1.53|-0.09|+0.07|+0.29|-0.01|-0.36|-0.84\n",
      "\n",
      "epoch:1, train_loss: +0.010, val_loss: -0.071, \n",
      "train_vector: +0.99|+1.21|-0.08|-0.08|-0.44|-0.13|-0.35|-1.05, \n",
      "val_vector  : +1.03|+1.10|-0.20|-0.30|-0.04|-0.30|-0.78|-1.07\n",
      "\n",
      "epoch:2, train_loss: -0.197, val_loss: -0.194, \n",
      "train_vector: +0.86|+0.75|-0.25|-0.27|-0.64|-0.28|-0.53|-1.21, \n",
      "val_vector  : +0.88|+1.11|-0.39|-0.37|-0.32|-0.51|-0.78|-1.16\n",
      "\n",
      "epoch:3, train_loss: -0.324, val_loss: -0.250, \n",
      "train_vector: +0.75|+0.56|-0.37|-0.39|-0.77|-0.40|-0.65|-1.32, \n",
      "val_vector  : +1.01|+0.79|-0.45|-0.36|-0.43|-0.57|-0.84|-1.14\n",
      "\n",
      "epoch:4, train_loss: -0.393, val_loss: -0.209, \n",
      "train_vector: +0.70|+0.48|-0.44|-0.46|-0.84|-0.46|-0.72|-1.39, \n",
      "val_vector  : +1.13|+1.20|-0.53|-0.37|-0.45|-0.58|-0.88|-1.18\n",
      "\n",
      "epoch:5, train_loss: -0.450, val_loss: -0.577, \n",
      "train_vector: +0.65|+0.44|-0.50|-0.52|-0.91|-0.52|-0.78|-1.45, \n",
      "val_vector  : +0.73|+0.43|-0.72|-0.71|-0.98|-0.74|-1.14|-1.48\n",
      "\n",
      "epoch:6, train_loss: -0.498, val_loss: -0.566, \n",
      "train_vector: +0.61|+0.38|-0.55|-0.57|-0.97|-0.56|-0.83|-1.50, \n",
      "val_vector  : +0.94|+0.48|-0.82|-0.73|-0.96|-0.79|-1.08|-1.56\n",
      "\n",
      "epoch:7, train_loss: -0.530, val_loss: -0.636, \n",
      "train_vector: +0.58|+0.35|-0.59|-0.60|-1.01|-0.59|-0.86|-1.53, \n",
      "val_vector  : +0.64|+0.46|-0.87|-0.80|-0.95|-0.83|-1.30|-1.44\n",
      "\n",
      "epoch:8, train_loss: -0.564, val_loss: -0.609, \n",
      "train_vector: +0.55|+0.30|-0.62|-0.63|-1.04|-0.62|-0.89|-1.56, \n",
      "val_vector  : +0.60|+0.84|-0.87|-0.78|-1.11|-0.82|-1.13|-1.60\n",
      "\n",
      "epoch:9, train_loss: -0.567, val_loss: -0.666, \n",
      "train_vector: +0.57|+0.31|-0.62|-0.64|-1.07|-0.62|-0.89|-1.57, \n",
      "val_vector  : +0.81|+0.26|-0.95|-0.89|-0.92|-0.94|-1.12|-1.58\n",
      "\n",
      "Training completed in 4702.834298372269s\n"
     ]
    }
   ],
   "source": [
    "model,bestOpt,bestWeight = train_type2(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                       scheduler=scheduler,logLoss=logLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:0, train_loss: +0.392, val_loss: +0.190, \n",
      "train_vector: +1.37|+1.76|+0.18|+0.34|+0.02|+0.16|+0.00|-0.70, \n",
      "val_vector  : +0.95|+1.71|-0.16|+0.18|+0.22|-0.12|-0.44|-0.82\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "epoch:1, train_loss: -0.018, val_loss: -0.034, \n",
      "train_vector: +0.98|+1.05|-0.14|-0.04|-0.46|-0.11|-0.40|-1.04, \n",
      "val_vector  : +1.02|+1.24|-0.28|-0.15|-0.40|-0.20|-0.57|-0.93\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "epoch:2, train_loss: -0.190, val_loss: -0.176, \n",
      "train_vector: +0.83|+0.79|-0.27|-0.25|-0.65|-0.24|-0.54|-1.19, \n",
      "val_vector  : +0.93|+1.03|-0.40|-0.38|-0.24|-0.45|-0.83|-1.07\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "epoch:3, train_loss: -0.295, val_loss: -0.349, \n",
      "train_vector: +0.74|+0.70|-0.37|-0.37|-0.78|-0.35|-0.65|-1.29, \n",
      "val_vector  : +1.12|+0.75|-0.58|-0.47|-0.70|-0.57|-1.14|-1.21\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "epoch:4, train_loss: -0.375, val_loss: -0.447, \n",
      "train_vector: +0.68|+0.59|-0.44|-0.45|-0.86|-0.43|-0.72|-1.37, \n",
      "val_vector  : +0.87|+0.41|-0.66|-0.59|-0.74|-0.56|-0.87|-1.42\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "epoch:5, train_loss: -0.428, val_loss: -0.421, \n",
      "train_vector: +0.63|+0.52|-0.49|-0.52|-0.92|-0.47|-0.77|-1.42, \n",
      "val_vector  : +0.71|+0.86|-0.69|-0.60|-0.90|-0.48|-0.97|-1.32\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "epoch:6, train_loss: -0.473, val_loss: -0.433, \n",
      "train_vector: +0.60|+0.47|-0.53|-0.57|-0.98|-0.51|-0.81|-1.46, \n",
      "val_vector  : +1.03|+0.52|-0.75|-0.62|-0.89|-0.60|-0.73|-1.41\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "epoch:7, train_loss: -0.508, val_loss: -0.559, \n",
      "train_vector: +0.57|+0.42|-0.57|-0.61|-1.02|-0.53|-0.84|-1.50, \n",
      "val_vector  : +0.70|+0.65|-0.85|-0.66|-0.90|-0.82|-1.06|-1.54\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "epoch:8, train_loss: -0.544, val_loss: -0.518, \n",
      "train_vector: +0.54|+0.38|-0.60|-0.64|-1.06|-0.57|-0.88|-1.53, \n",
      "val_vector  : +0.78|+0.89|-0.80|-0.76|-0.76|-0.72|-1.18|-1.59\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "epoch:9, train_loss: -0.573, val_loss: -0.583, \n",
      "train_vector: +0.52|+0.35|-0.63|-0.67|-1.09|-0.60|-0.90|-1.56, \n",
      "val_vector  : +0.63|+0.65|-0.79|-0.46|-1.07|-0.80|-1.16|-1.67\n",
      "\n",
      "Training completed in 3000.3757128715515s\n"
     ]
    }
   ],
   "source": [
    "# O1\n",
    "model,bestOpt,bestWeight = train_type2(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                       scheduler=scheduler,logLoss=logLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
