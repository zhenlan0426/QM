{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "from functions_refactor import *\n",
    "from pytorch_util import *\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "reuse = False\n",
    "block = MEGNet_block\n",
    "head = feedforwardHead_Update\n",
    "data = '../Data/{}_data_ACSF_expand_PCA.pickle'\n",
    "batch_size = 32\n",
    "dim = 128\n",
    "epochs = 50\n",
    "clip = 0.4\n",
    "layer1 = 3\n",
    "layer2 = 3\n",
    "factor = 2\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +1.845, val_loss: -0.428, \n",
      "train_vector: +1.17|+0.60|+0.17|-0.36|-0.40|+0.22|-0.39|-0.74, \n",
      "val_vector  : +0.53|+0.06|-0.24|-0.83|-0.88|-0.12|-0.87|-1.07\n",
      "\n",
      "epoch:1, train_loss: +0.819, val_loss: -0.479, \n",
      "train_vector: +0.71|+0.09|-0.27|-0.90|-0.91|-0.16|-0.87|-1.10, \n",
      "val_vector  : +0.56|+0.14|-0.46|-0.47|-1.11|-0.29|-0.97|-1.23\n",
      "\n",
      "epoch:2, train_loss: +0.687, val_loss: -0.706, \n",
      "train_vector: +0.48|-0.05|-0.42|-1.07|-1.09|-0.29|-0.99|-1.26, \n",
      "val_vector  : +0.27|-0.26|-0.59|-0.94|-1.27|-0.39|-1.12|-1.35\n",
      "\n",
      "epoch:3, train_loss: +0.600, val_loss: -0.757, \n",
      "train_vector: +0.33|-0.19|-0.54|-1.23|-1.23|-0.40|-1.10|-1.36, \n",
      "val_vector  : +0.21|-0.08|-0.67|-0.96|-1.33|-0.49|-1.24|-1.48\n",
      "\n",
      "epoch:4, train_loss: +0.549, val_loss: -0.791, \n",
      "train_vector: +0.23|-0.27|-0.62|-1.32|-1.31|-0.47|-1.18|-1.44, \n",
      "val_vector  : +0.37|+0.29|-0.73|-1.53|-1.43|-0.54|-1.25|-1.50\n",
      "\n",
      "epoch:5, train_loss: +0.512, val_loss: -0.996, \n",
      "train_vector: +0.16|-0.34|-0.69|-1.39|-1.38|-0.54|-1.24|-1.50, \n",
      "val_vector  : -0.16|-0.46|-0.80|-1.55|-1.51|-0.60|-1.29|-1.60\n",
      "\n",
      "epoch:6, train_loss: +0.483, val_loss: -1.032, \n",
      "train_vector: +0.10|-0.41|-0.75|-1.43|-1.45|-0.59|-1.29|-1.56, \n",
      "val_vector  : -0.21|-0.43|-0.84|-1.58|-1.48|-0.68|-1.42|-1.61\n",
      "\n",
      "epoch:7, train_loss: +0.461, val_loss: -1.049, \n",
      "train_vector: +0.05|-0.45|-0.79|-1.48|-1.50|-0.64|-1.34|-1.61, \n",
      "val_vector  : +0.07|-0.64|-0.88|-1.56|-1.59|-0.69|-1.47|-1.65\n",
      "\n",
      "epoch:8, train_loss: +0.437, val_loss: -1.121, \n",
      "train_vector: -0.01|-0.51|-0.84|-1.54|-1.55|-0.68|-1.38|-1.66, \n",
      "val_vector  : -0.15|-0.63|-0.93|-1.71|-1.60|-0.75|-1.51|-1.69\n",
      "\n",
      "epoch:9, train_loss: +0.421, val_loss: -1.051, \n",
      "train_vector: -0.04|-0.55|-0.88|-1.57|-1.59|-0.72|-1.42|-1.70, \n",
      "val_vector  : +0.11|-0.34|-0.94|-1.65|-1.62|-0.78|-1.44|-1.75\n",
      "\n",
      "epoch:10, train_loss: +0.407, val_loss: -1.129, \n",
      "train_vector: -0.08|-0.58|-0.91|-1.61|-1.63|-0.74|-1.44|-1.72, \n",
      "val_vector  : -0.07|-0.85|-1.01|-1.22|-1.75|-0.81|-1.55|-1.78\n",
      "\n",
      "epoch:11, train_loss: +0.394, val_loss: -1.035, \n",
      "train_vector: -0.11|-0.62|-0.94|-1.64|-1.66|-0.77|-1.47|-1.76, \n",
      "val_vector  : +0.02|-0.55|-1.02|-0.91|-1.71|-0.82|-1.50|-1.80\n",
      "\n",
      "epoch:12, train_loss: +0.382, val_loss: -1.228, \n",
      "train_vector: -0.13|-0.66|-0.97|-1.66|-1.69|-0.80|-1.50|-1.79, \n",
      "val_vector  : -0.35|-0.72|-1.03|-1.76|-1.68|-0.87|-1.60|-1.83\n",
      "\n",
      "epoch:13, train_loss: +0.374, val_loss: -1.264, \n",
      "train_vector: -0.15|-0.68|-1.00|-1.69|-1.72|-0.83|-1.53|-1.82, \n",
      "val_vector  : -0.09|-0.99|-1.07|-1.81|-1.75|-0.90|-1.65|-1.85\n",
      "\n",
      "epoch:14, train_loss: +0.366, val_loss: -1.258, \n",
      "train_vector: -0.17|-0.70|-1.03|-1.72|-1.75|-0.85|-1.55|-1.85, \n",
      "val_vector  : -0.34|-0.73|-1.07|-1.74|-1.76|-0.90|-1.66|-1.86\n",
      "\n",
      "epoch:15, train_loss: +0.356, val_loss: -1.142, \n",
      "train_vector: -0.21|-0.73|-1.05|-1.73|-1.77|-0.87|-1.57|-1.87, \n",
      "val_vector  : -0.13|-0.67|-1.13|-1.13|-1.80|-0.90|-1.53|-1.84\n",
      "\n",
      "epoch:16, train_loss: +0.349, val_loss: -1.209, \n",
      "train_vector: -0.21|-0.75|-1.07|-1.75|-1.79|-0.89|-1.59|-1.89, \n",
      "val_vector  : -0.21|-0.24|-1.14|-1.82|-1.82|-0.94|-1.60|-1.91\n",
      "\n",
      "epoch:17, train_loss: +0.342, val_loss: -1.254, \n",
      "train_vector: -0.24|-0.76|-1.09|-1.79|-1.82|-0.91|-1.61|-1.91, \n",
      "val_vector  : +0.13|-0.79|-1.13|-1.81|-1.87|-0.96|-1.71|-1.88\n",
      "\n",
      "epoch:18, train_loss: +0.334, val_loss: -1.353, \n",
      "train_vector: -0.26|-0.80|-1.11|-1.80|-1.84|-0.93|-1.63|-1.93, \n",
      "val_vector  : -0.12|-1.18|-1.17|-1.87|-1.84|-0.99|-1.71|-1.95\n",
      "\n",
      "epoch:19, train_loss: +0.332, val_loss: -1.325, \n",
      "train_vector: -0.27|-0.78|-1.12|-1.80|-1.86|-0.95|-1.65|-1.95, \n",
      "val_vector  : -0.31|-1.01|-1.20|-1.55|-1.88|-0.99|-1.68|-1.97\n",
      "\n",
      "epoch:20, train_loss: +0.329, val_loss: -1.360, \n",
      "train_vector: -0.27|-0.79|-1.14|-1.83|-1.87|-0.96|-1.66|-1.97, \n",
      "val_vector  : -0.05|-1.04|-1.21|-2.01|-1.85|-1.01|-1.77|-1.94\n",
      "\n",
      "epoch:21, train_loss: +0.321, val_loss: -1.239, \n",
      "train_vector: -0.29|-0.84|-1.16|-1.84|-1.89|-0.98|-1.68|-1.99, \n",
      "val_vector  : -0.10|-0.52|-1.22|-1.41|-1.89|-1.01|-1.79|-1.97\n",
      "\n",
      "epoch:22, train_loss: +0.314, val_loss: -1.417, \n",
      "train_vector: -0.32|-0.86|-1.17|-1.86|-1.91|-0.99|-1.69|-2.01, \n",
      "val_vector  : -0.55|-0.95|-1.22|-1.93|-1.88|-1.01|-1.77|-2.01\n",
      "\n",
      "epoch:23, train_loss: +0.307, val_loss: -1.404, \n",
      "train_vector: -0.35|-0.88|-1.19|-1.87|-1.93|-1.01|-1.71|-2.03, \n",
      "val_vector  : -0.48|-0.73|-1.25|-1.93|-1.92|-1.06|-1.82|-2.03\n",
      "\n",
      "epoch:24, train_loss: +0.310, val_loss: -1.322, \n",
      "train_vector: -0.32|-0.88|-1.20|-1.88|-1.93|-1.02|-1.72|-2.04, \n",
      "val_vector  : -0.10|-0.51|-1.27|-1.87|-1.95|-1.05|-1.83|-2.00\n",
      "\n",
      "epoch:25, train_loss: +0.297, val_loss: -1.491, \n",
      "train_vector: -0.37|-0.93|-1.21|-1.91|-1.96|-1.04|-1.74|-2.06, \n",
      "val_vector  : -0.59|-1.09|-1.28|-2.12|-1.88|-1.05|-1.87|-2.05\n",
      "\n",
      "epoch:26, train_loss: +0.294, val_loss: -1.441, \n",
      "train_vector: -0.39|-0.93|-1.23|-1.93|-1.97|-1.05|-1.75|-2.07, \n",
      "val_vector  : -0.45|-1.03|-1.29|-1.77|-1.96|-1.08|-1.87|-2.08\n",
      "\n",
      "epoch:27, train_loss: +0.293, val_loss: -1.424, \n",
      "train_vector: -0.39|-0.93|-1.24|-1.94|-1.97|-1.06|-1.76|-2.08, \n",
      "val_vector  : -0.41|-0.69|-1.29|-1.96|-2.00|-1.10|-1.86|-2.08\n",
      "\n",
      "epoch:28, train_loss: +0.291, val_loss: -1.362, \n",
      "train_vector: -0.39|-0.95|-1.26|-1.96|-1.99|-1.07|-1.78|-2.09, \n",
      "val_vector  : +0.03|-0.64|-1.33|-1.96|-1.94|-1.11|-1.89|-2.06\n",
      "\n",
      "epoch:29, train_loss: +0.288, val_loss: -1.424, \n",
      "train_vector: -0.40|-0.94|-1.27|-1.96|-2.01|-1.08|-1.79|-2.11, \n",
      "val_vector  : -0.59|-0.79|-1.32|-1.72|-1.89|-1.12|-1.88|-2.07\n",
      "\n",
      "epoch:30, train_loss: +0.282, val_loss: -1.491, \n",
      "train_vector: -0.43|-0.98|-1.27|-1.97|-2.02|-1.09|-1.80|-2.12, \n",
      "val_vector  : -0.42|-0.93|-1.33|-2.11|-2.01|-1.13|-1.89|-2.11\n",
      "\n",
      "epoch:31, train_loss: +0.280, val_loss: -1.336, \n",
      "train_vector: -0.43|-0.98|-1.29|-1.97|-2.03|-1.11|-1.81|-2.14, \n",
      "val_vector  : -0.09|-0.43|-1.33|-1.65|-2.01|-1.15|-1.91|-2.12\n",
      "\n",
      "epoch:32, train_loss: +0.277, val_loss: -1.531, \n",
      "train_vector: -0.44|-0.99|-1.30|-1.99|-2.05|-1.12|-1.82|-2.15, \n",
      "val_vector  : -0.59|-1.08|-1.36|-1.98|-2.06|-1.15|-1.91|-2.11\n",
      "\n",
      "epoch:33, train_loss: +0.272, val_loss: -1.432, \n",
      "train_vector: -0.46|-1.03|-1.31|-1.99|-2.06|-1.12|-1.83|-2.16, \n",
      "val_vector  : -0.57|-0.99|-1.34|-1.33|-2.04|-1.14|-1.93|-2.11\n",
      "\n",
      "epoch:34, train_loss: +0.273, val_loss: -1.486, \n",
      "train_vector: -0.44|-1.01|-1.32|-2.00|-2.07|-1.14|-1.84|-2.17, \n",
      "val_vector  : -0.10|-1.05|-1.37|-2.01|-2.09|-1.18|-1.92|-2.17\n",
      "\n",
      "epoch:35, train_loss: +0.266, val_loss: -1.461, \n",
      "train_vector: -0.47|-1.04|-1.33|-2.02|-2.09|-1.15|-1.86|-2.18, \n",
      "val_vector  : -0.57|-1.01|-1.38|-1.44|-2.08|-1.16|-1.95|-2.11\n",
      "\n",
      "epoch:36, train_loss: +0.267, val_loss: -1.385, \n",
      "train_vector: -0.48|-1.03|-1.33|-2.03|-2.09|-1.15|-1.85|-2.19, \n",
      "val_vector  : -0.16|-0.48|-1.39|-1.83|-2.03|-1.19|-1.87|-2.14\n",
      "\n",
      "epoch:37, train_loss: +0.268, val_loss: -1.540, \n",
      "train_vector: -0.46|-1.02|-1.35|-2.03|-2.09|-1.16|-1.86|-2.19, \n",
      "val_vector  : -0.67|-1.06|-1.38|-1.92|-2.08|-1.19|-1.92|-2.11\n",
      "\n",
      "epoch:38, train_loss: +0.262, val_loss: -1.572, \n",
      "train_vector: -0.49|-1.05|-1.36|-2.05|-2.11|-1.17|-1.88|-2.21, \n",
      "val_vector  : -0.60|-1.24|-1.39|-2.00|-2.03|-1.21|-1.96|-2.15\n",
      "\n",
      "epoch:39, train_loss: +0.259, val_loss: -1.565, \n",
      "train_vector: -0.51|-1.06|-1.36|-2.06|-2.12|-1.18|-1.89|-2.22, \n",
      "val_vector  : -0.70|-0.82|-1.41|-2.04|-2.11|-1.22|-1.99|-2.20\n",
      "\n",
      "epoch:40, train_loss: +0.255, val_loss: -1.559, \n",
      "train_vector: -0.51|-1.10|-1.38|-2.07|-2.13|-1.19|-1.90|-2.24, \n",
      "val_vector  : -0.61|-1.32|-1.41|-1.70|-2.10|-1.20|-1.95|-2.18\n",
      "\n",
      "epoch:41, train_loss: +0.257, val_loss: -1.570, \n",
      "train_vector: -0.50|-1.07|-1.38|-2.07|-2.14|-1.20|-1.90|-2.24, \n",
      "val_vector  : -0.64|-0.96|-1.41|-2.12|-2.04|-1.22|-1.96|-2.22\n",
      "\n",
      "epoch:42, train_loss: +0.252, val_loss: -1.456, \n",
      "train_vector: -0.52|-1.09|-1.39|-2.08|-2.15|-1.21|-1.92|-2.25, \n",
      "val_vector  : -0.42|-0.49|-1.42|-1.89|-2.07|-1.22|-1.95|-2.19\n",
      "\n",
      "epoch:43, train_loss: +0.251, val_loss: -1.526, \n",
      "train_vector: -0.53|-1.10|-1.40|-2.09|-2.16|-1.22|-1.92|-2.26, \n",
      "val_vector  : -0.36|-0.75|-1.39|-2.14|-2.15|-1.25|-1.96|-2.21\n",
      "\n",
      "epoch:44, train_loss: +0.248, val_loss: -1.637, \n",
      "train_vector: -0.54|-1.11|-1.41|-2.09|-2.16|-1.22|-1.93|-2.27, \n",
      "val_vector  : -0.70|-1.21|-1.44|-2.20|-2.12|-1.24|-1.97|-2.22\n",
      "\n",
      "epoch:45, train_loss: +0.245, val_loss: -1.504, \n",
      "train_vector: -0.55|-1.14|-1.41|-2.09|-2.17|-1.23|-1.94|-2.28, \n",
      "val_vector  : -0.18|-1.04|-1.45|-1.76|-2.15|-1.23|-1.99|-2.23\n",
      "\n",
      "epoch:46, train_loss: +0.242, val_loss: -1.325, \n",
      "train_vector: -0.57|-1.15|-1.42|-2.12|-2.18|-1.24|-1.95|-2.29, \n",
      "val_vector  : +0.49|-0.32|-1.46|-2.16|-1.97|-1.22|-1.79|-2.17\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47, train_loss: +0.242, val_loss: -1.589, \n",
      "train_vector: -0.57|-1.15|-1.43|-2.11|-2.19|-1.24|-1.95|-2.30, \n",
      "val_vector  : -0.70|-1.19|-1.46|-1.77|-2.12|-1.25|-2.02|-2.20\n",
      "\n",
      "epoch:48, train_loss: +0.241, val_loss: -1.648, \n",
      "train_vector: -0.57|-1.15|-1.43|-2.12|-2.19|-1.25|-1.95|-2.30, \n",
      "val_vector  : -0.76|-1.16|-1.46|-2.13|-2.16|-1.29|-2.00|-2.23\n",
      "\n",
      "epoch:49, train_loss: +0.239, val_loss: -1.647, \n",
      "train_vector: -0.58|-1.17|-1.44|-2.11|-2.20|-1.26|-1.96|-2.31, \n",
      "val_vector  : -0.64|-1.25|-1.48|-2.13|-2.13|-1.28|-2.03|-2.25\n",
      "\n",
      "Training completed in 4211.929825544357s\n"
     ]
    }
   ],
   "source": [
    "train_dl,val_dl = get_data(data,batch_size)\n",
    "\n",
    "model = GNN_edgeUpdate(reuse,block,head,dim,layer1,layer2,factor,**data_dict[data]).to('cuda:0')\n",
    "\n",
    "paras = trainable_parameter(model)\n",
    "opt = Adam(paras,lr=lr)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5)\n",
    "\n",
    "model,train_loss_list,val_loss_list,bestWeight = train_type(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                                            logLoss=False,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(train_loss_list,val_loss_list,reuse,block,\\\n",
    "             head,data,batch_size,dim,clip,layer1,layer2,factor,epochs,postStr='MAE')\n",
    "save_model_type(bestWeight,opt,reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs,postStr='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs,postStr='base'):\n",
    "    # set up\n",
    "    model = GNN_edgeUpdate(reuse,block,head,dim,layer1,layer2,factor,**data_dict[data]).to('cuda:0')\n",
    "    submission = pd.read_csv('../Data/sample_submission.csv')\n",
    "    \n",
    "    for i in range(8):\n",
    "        # load test data and type_id\n",
    "        with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(i)+'.pickle', 'rb') as handle:\n",
    "            test_data = pickle.load(handle)\n",
    "        test_list = [Data(**d) for d in test_data]\n",
    "        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n",
    "        with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(i)+'.pickle', 'rb') as handle:\n",
    "            test_id = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "        # load model\n",
    "        checkpoint = torch.load('../Model/{}.tar'.format('_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n",
    "                                            for i in [reuse,block,head,data,batch_size,dim,clip,\\\n",
    "                                                  layer1,layer2,factor,epochs,'type_'+str(i)+postStr]])))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    \n",
    "        # predict\n",
    "        model.eval()\n",
    "        yhat_list = []\n",
    "        with torch.no_grad():\n",
    "            for data_torch in test_dl:\n",
    "                data_torch = data_torch.to('cuda:0')\n",
    "                yhat_list.append(model(data_torch,False,True))\n",
    "        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "        \n",
    "        # join\n",
    "        submit_ = dict(zip(test_id,yhat))\n",
    "        submission['type_'+str(i)] = submission.id.map(submit_)\n",
    "    \n",
    "    # save types results    \n",
    "    submission.to_csv('../Submission/{}.csv'.format('_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n",
    "                                        for i in [reuse,block,head,data,batch_size,dim,clip,\\\n",
    "                                              layer1,layer2,factor,epochs,'all_types'+postStr]])),\\\n",
    "                      index=False)\n",
    "    \n",
    "    # save final results for submission\n",
    "    submission['scalar_coupling_constant'] = submission.iloc[:,2:].mean(1)\n",
    "    submission = submission[['id','scalar_coupling_constant']]\n",
    "    \n",
    "    submission.to_csv('../Submission/{}.csv'.format('_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n",
    "                                        for i in [reuse,block,head,data,batch_size,dim,clip,\\\n",
    "                                              layer1,layer2,factor,epochs,'final'+postStr]])),\\\n",
    "                      index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs,postStr='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
