{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "from functions_refactor import *\n",
    "from pytorch_util import *\n",
    "#from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed parameters\n",
    "block = MEGNet_block\n",
    "head_mol,head_atom,head_edge = head_mol,head_atom,head_edge\n",
    "clip = 0.5\n",
    "batch_size = 64\n",
    "threshold = 1e3\n",
    "reuse = False\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# changing parameters\n",
    "head = SimplyInteraction\n",
    "data = '../Data/{}_data_stacking_0815_wNodeInfo.pickle'\n",
    "dim = 22\n",
    "logLoss = False\n",
    "layer = 4\n",
    "factor = 3\n",
    "epochs = 19\n",
    "edge_in4 = 22\n",
    "node_in = 32\n",
    "aggr = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,val_dl = get_data(data,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_multiHead_interleave_stacking(torch.nn.Module):\n",
    "    def __init__(self,reuse,block,head,dim,layer,factor,\\\n",
    "                 edge_in4,node_in,edge_in3=8,aggr='mean'):\n",
    "        # block,head are nn.Module\n",
    "        # node_in,edge_in are dim for bonding and edge_in4,edge_in3 for coupling\n",
    "        super(GNN_multiHead_interleave_stacking, self).__init__()\n",
    "        \n",
    "        self.lin_node = Sequential(BatchNorm1d(node_in),Linear(node_in, dim*factor),LeakyReLU(), \\\n",
    "                                   BatchNorm1d(dim*factor),Linear(dim*factor, dim),LeakyReLU())       \n",
    "        if reuse:\n",
    "            self.conv = block(dim=dim,aggr=aggr)\n",
    "        else:\n",
    "            self.conv = nn.ModuleList([block(dim=dim,aggr=aggr) for _ in range(layer)])        \n",
    "        self.head = head(dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, data,IsTrain=False,typeTrain=False,logLoss=True,weight=None):\n",
    "        out = self.lin_node(data.x)\n",
    "        # edge_*3 only does not repeat for undirected graph. Hence need to add (j,i) to (i,j) in edges\n",
    "        edge_index3 = torch.cat([data.edge_index3,data.edge_index3[[1,0]]],1)\n",
    "        n = data.edge_attr3.shape[0]\n",
    "        edge_attr3 = torch.cat([data.edge_attr4,data.edge_attr4],0)\n",
    "          \n",
    "        for conv in self.conv:\n",
    "            out,edge_attr3 = conv(out,edge_index3,edge_attr3)\n",
    "\n",
    "        \n",
    "        edge_attr3 = edge_attr3[:n]\n",
    "\n",
    "        if typeTrain:\n",
    "            if IsTrain:\n",
    "                y = data.y[data.type_attr]\n",
    "            edge_attr3 = edge_attr3[data.type_attr]\n",
    "            edge_index3 = data.edge_index3[:,data.type_attr]\n",
    "            edge_attr3_old = data.edge_attr3[data.type_attr]\n",
    "        else:\n",
    "            if IsTrain:\n",
    "                y = data.y\n",
    "            edge_index3 = data.edge_index3\n",
    "            edge_attr3_old = data.edge_attr3\n",
    "            \n",
    "        yhat = self.head(out,edge_index3,edge_attr3,edge_attr3_old)\n",
    "        \n",
    "        if IsTrain:\n",
    "            k = torch.sum(edge_attr3_old,0)\n",
    "            nonzeroIndex = torch.nonzero(k).squeeze(1)\n",
    "            abs_ = torch.abs(y-yhat).unsqueeze(1)\n",
    "            loss_perType = torch.zeros(8,device='cuda:0')\n",
    "            if logLoss:\n",
    "                loss_perType[nonzeroIndex] = torch.log(torch.sum(abs_ * edge_attr3_old[:,nonzeroIndex],0)/k[nonzeroIndex])\n",
    "                loss = torch.sum(loss_perType)/nonzeroIndex.shape[0]\n",
    "                return loss,loss_perType         \n",
    "            else:\n",
    "                loss_perType[nonzeroIndex] = torch.sum(abs_ * edge_attr3_old[:,nonzeroIndex],0)/k[nonzeroIndex]\n",
    "                loss = torch.sum(loss_perType)/nonzeroIndex.shape[0]\n",
    "                loss_perType[nonzeroIndex] = torch.log(loss_perType[nonzeroIndex])\n",
    "                return loss,loss_perType\n",
    "        else:\n",
    "            return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +4.882, val_loss: -2.450, \n",
      "train_vector: +0.76|+0.17|-1.16|-1.34|-1.36|-1.14|-1.26|-1.85, \n",
      "val_vector  : -1.50|-1.96|-2.41|-3.18|-2.70|-2.19|-2.79|-2.88\n",
      "\n",
      "epoch:1, train_loss: +0.104, val_loss: -2.530, \n",
      "train_vector: -1.38|-1.88|-2.44|-3.21|-2.71|-2.24|-2.86|-2.95, \n",
      "val_vector  : -1.48|-1.96|-2.48|-3.29|-2.86|-2.30|-2.86|-3.01\n",
      "\n",
      "epoch:2, train_loss: +0.098, val_loss: -2.558, \n",
      "train_vector: -1.41|-1.93|-2.49|-3.26|-2.87|-2.30|-2.95|-3.09, \n",
      "val_vector  : -1.30|-1.85|-2.52|-3.33|-2.94|-2.34|-3.04|-3.16\n",
      "\n",
      "epoch:3, train_loss: +0.096, val_loss: -2.591, \n",
      "train_vector: -1.41|-1.95|-2.52|-3.28|-2.92|-2.34|-3.01|-3.15, \n",
      "val_vector  : -1.51|-1.87|-2.53|-3.34|-2.88|-2.37|-3.05|-3.17\n",
      "\n",
      "epoch:4, train_loss: +0.095, val_loss: -2.577, \n",
      "train_vector: -1.40|-1.95|-2.54|-3.29|-2.96|-2.37|-3.04|-3.17, \n",
      "val_vector  : -1.27|-1.75|-2.56|-3.35|-3.00|-2.38|-3.10|-3.20\n",
      "\n",
      "epoch:5, train_loss: +0.095, val_loss: -2.646, \n",
      "train_vector: -1.39|-1.95|-2.55|-3.30|-2.98|-2.38|-3.07|-3.18, \n",
      "val_vector  : -1.52|-2.02|-2.57|-3.35|-2.98|-2.39|-3.12|-3.22\n",
      "\n",
      "epoch:6, train_loss: +0.093, val_loss: -2.604, \n",
      "train_vector: -1.42|-1.97|-2.56|-3.30|-3.00|-2.39|-3.08|-3.20, \n",
      "val_vector  : -1.45|-1.76|-2.58|-3.27|-3.04|-2.40|-3.11|-3.22\n",
      "\n",
      "epoch:7, train_loss: +0.093, val_loss: -2.655, \n",
      "train_vector: -1.41|-1.94|-2.57|-3.31|-3.01|-2.40|-3.09|-3.21, \n",
      "val_vector  : -1.52|-2.03|-2.59|-3.35|-3.04|-2.41|-3.07|-3.23\n",
      "\n",
      "epoch:8, train_loss: +0.092, val_loss: -2.655, \n",
      "train_vector: -1.42|-1.99|-2.57|-3.32|-3.01|-2.40|-3.10|-3.20, \n",
      "val_vector  : -1.39|-2.05|-2.59|-3.37|-3.05|-2.42|-3.13|-3.24\n",
      "\n",
      "epoch:9, train_loss: +0.091, val_loss: -2.647, \n",
      "train_vector: -1.42|-1.99|-2.58|-3.32|-3.02|-2.40|-3.12|-3.21, \n",
      "val_vector  : -1.40|-2.01|-2.59|-3.33|-3.05|-2.41|-3.14|-3.24\n",
      "\n",
      "epoch:10, train_loss: +0.091, val_loss: -2.642, \n",
      "train_vector: -1.44|-1.99|-2.58|-3.33|-3.03|-2.41|-3.13|-3.21, \n",
      "val_vector  : -1.50|-2.01|-2.59|-3.33|-3.00|-2.42|-3.10|-3.19\n",
      "\n",
      "epoch:11, train_loss: +0.091, val_loss: -2.669, \n",
      "train_vector: -1.43|-1.99|-2.58|-3.33|-3.02|-2.41|-3.13|-3.22, \n",
      "val_vector  : -1.52|-2.03|-2.60|-3.38|-3.06|-2.42|-3.10|-3.25\n",
      "\n",
      "epoch:12, train_loss: +0.090, val_loss: -2.664, \n",
      "train_vector: -1.45|-2.01|-2.58|-3.33|-3.03|-2.41|-3.13|-3.22, \n",
      "val_vector  : -1.55|-1.96|-2.60|-3.31|-3.06|-2.42|-3.16|-3.25\n",
      "\n",
      "epoch:13, train_loss: +0.090, val_loss: -2.643, \n",
      "train_vector: -1.45|-2.00|-2.58|-3.33|-3.03|-2.41|-3.14|-3.22, \n",
      "val_vector  : -1.26|-2.01|-2.60|-3.37|-3.06|-2.42|-3.17|-3.25\n",
      "\n",
      "epoch:14, train_loss: +0.089, val_loss: -2.645, \n",
      "train_vector: -1.48|-2.00|-2.59|-3.34|-3.03|-2.41|-3.15|-3.22, \n",
      "val_vector  : -1.47|-1.83|-2.60|-3.36|-3.06|-2.42|-3.17|-3.25\n",
      "\n",
      "epoch:15, train_loss: +0.088, val_loss: -2.687, \n",
      "train_vector: -1.49|-2.01|-2.59|-3.33|-3.03|-2.41|-3.15|-3.22, \n",
      "val_vector  : -1.58|-2.02|-2.60|-3.37|-3.05|-2.42|-3.19|-3.26\n",
      "\n",
      "epoch:16, train_loss: +0.088, val_loss: -2.681, \n",
      "train_vector: -1.50|-2.00|-2.59|-3.33|-3.03|-2.41|-3.15|-3.23, \n",
      "val_vector  : -1.54|-2.03|-2.60|-3.39|-3.04|-2.43|-3.18|-3.24\n",
      "\n",
      "epoch:17, train_loss: +0.088, val_loss: -2.681, \n",
      "train_vector: -1.49|-2.01|-2.59|-3.34|-3.03|-2.41|-3.17|-3.23, \n",
      "val_vector  : -1.54|-2.05|-2.59|-3.36|-3.04|-2.43|-3.19|-3.25\n",
      "\n",
      "epoch:18, train_loss: +0.087, val_loss: -2.691, \n",
      "train_vector: -1.52|-2.01|-2.59|-3.34|-3.04|-2.41|-3.17|-3.23, \n",
      "val_vector  : -1.59|-2.04|-2.60|-3.39|-3.04|-2.43|-3.21|-3.24\n",
      "\n",
      "Training completed in 708.0715703964233s\n"
     ]
    }
   ],
   "source": [
    "# layer = 4\n",
    "model = GNN_multiHead_interleave_stacking(reuse,block,head,dim,layer,factor,edge_in4,node_in).to('cuda:0')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n",
    "\n",
    "model,train_loss_perType,val_loss_perType,bestWeight = train_type_earlyStop(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                                                scheduler=scheduler,logLoss=logLoss,threshold=threshold,typeTrain=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Model/GNN_stacking_0815.pickle', 'wb') as handle:\n",
    "    pickle.dump(bestWeight, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Model/GNN_stacking_0815.pickle', 'rb') as handle:\n",
    "    bestWeight = pickle.load(handle)\n",
    "\n",
    "model = GNN_multiHead_interleave_stacking(reuse,block,head,dim,layer,factor,edge_in4).to('cuda:0')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test\n",
    "for type_i in range(8):\n",
    "    # load val data and type_id\n",
    "    with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "        test_data = pickle.load(handle)\n",
    "    test_list = [Data(**d) for d in test_data]\n",
    "    test_dl = DataLoader(test_list,batch_size,shuffle=False)\n",
    "\n",
    "    with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "        test_id = pickle.load(handle)\n",
    "\n",
    "    # load model\n",
    "    model.load_state_dict(bestWeight[type_i])\n",
    "\n",
    "    # predict\n",
    "    model.eval()\n",
    "    yhat_list = []\n",
    "    with torch.no_grad():\n",
    "        for data_torch in test_dl:\n",
    "            data_torch = data_torch.to('cuda:0')\n",
    "            yhat_list.append(model(data_torch,False,typeTrain=True))\n",
    "    yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "\n",
    "    # join\n",
    "    assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n",
    "    submit_ = dict(zip(test_id,yhat))\n",
    "    test_df['type'+str(type_i)] = test_df.id.map(submit_)\n",
    "    #test_df['fold'+str(i)+'_type'+str(type_i)] = test_df.id.map(submit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['scalar_coupling_constant'] = np.nanmean(test_df.iloc[:,5:],1)\n",
    "#test = test[['id','yhat']]\n",
    "test_df[['id','scalar_coupling_constant']].to_csv('../Submission/GNN_stacking_0815',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
