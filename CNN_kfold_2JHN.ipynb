{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from functions_refactor import RAdam\n",
    "from functions_cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "type_ = '2JHN'\n",
    "num_workers = 4\n",
    "batch_size = 512\n",
    "clip = 2\n",
    "n_epochs = 50\n",
    "model_struct = CNN2\n",
    "root_dir =\"../Data/full-images-2jhn/Image_2JHN/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/Desktop/kaggle/QM/Code/functions_cnn.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  train_ids['file_name'] = train_ids.molecule_name.str.cat(train_ids.id.astype(str),sep='_')\n",
      "/home/will/Desktop/kaggle/QM/Code/functions_cnn.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test_ids['file_name'] = test_ids.molecule_name.str.cat(test_ids.id.astype(str),sep='_')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../Data/train.csv')\n",
    "test_df = pd.read_csv('../Data/test.csv')\n",
    "train_ids,test_ids = process_data(train_df,test_df,type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start fold: 0\n",
      "Epoch: 1 \tTraining Loss: 1.641698 \tValidation Loss: 0.434549\n",
      "Epoch: 2 \tTraining Loss: 0.315032 \tValidation Loss: 0.291733\n",
      "Epoch: 3 \tTraining Loss: 0.207307 \tValidation Loss: 0.175524\n",
      "Epoch: 4 \tTraining Loss: 0.149404 \tValidation Loss: 0.141235\n",
      "Epoch: 5 \tTraining Loss: 0.111764 \tValidation Loss: 0.118738\n",
      "Epoch: 6 \tTraining Loss: 0.094020 \tValidation Loss: 0.083164\n",
      "Epoch: 7 \tTraining Loss: 0.078700 \tValidation Loss: 0.126344\n",
      "Epoch: 8 \tTraining Loss: 0.066964 \tValidation Loss: 0.070978\n",
      "Epoch: 9 \tTraining Loss: 0.060208 \tValidation Loss: 0.059814\n",
      "Epoch: 10 \tTraining Loss: 0.054800 \tValidation Loss: 0.061182\n",
      "Epoch: 11 \tTraining Loss: 0.052374 \tValidation Loss: 0.056253\n",
      "Epoch: 12 \tTraining Loss: 0.047273 \tValidation Loss: 0.067774\n",
      "Epoch: 13 \tTraining Loss: 0.042804 \tValidation Loss: 0.044480\n",
      "Epoch: 14 \tTraining Loss: 0.037071 \tValidation Loss: 0.046309\n",
      "Epoch: 15 \tTraining Loss: 0.037755 \tValidation Loss: 0.061434\n",
      "Epoch: 16 \tTraining Loss: 0.030628 \tValidation Loss: 0.040354\n",
      "Epoch: 17 \tTraining Loss: 0.027723 \tValidation Loss: 0.060496\n",
      "Epoch: 18 \tTraining Loss: 0.032873 \tValidation Loss: 0.041879\n",
      "Epoch: 19 \tTraining Loss: 0.027089 \tValidation Loss: 0.044180\n",
      "Epoch: 20 \tTraining Loss: 0.026221 \tValidation Loss: 0.044524\n",
      "Epoch: 21 \tTraining Loss: 0.022263 \tValidation Loss: 0.039665\n",
      "Epoch: 22 \tTraining Loss: 0.020956 \tValidation Loss: 0.038644\n",
      "Epoch: 23 \tTraining Loss: 0.020565 \tValidation Loss: 0.034327\n",
      "Epoch: 24 \tTraining Loss: 0.018819 \tValidation Loss: 0.040058\n",
      "Epoch: 25 \tTraining Loss: 0.019010 \tValidation Loss: 0.053219\n",
      "Epoch: 26 \tTraining Loss: 0.017175 \tValidation Loss: 0.040082\n",
      "Epoch: 27 \tTraining Loss: 0.018863 \tValidation Loss: 0.034186\n",
      "Epoch: 28 \tTraining Loss: 0.014591 \tValidation Loss: 0.031372\n",
      "Epoch: 29 \tTraining Loss: 0.012087 \tValidation Loss: 0.032787\n",
      "Epoch: 30 \tTraining Loss: 0.014572 \tValidation Loss: 0.038311\n",
      "Epoch: 31 \tTraining Loss: 0.015555 \tValidation Loss: 0.031586\n",
      "Epoch: 32 \tTraining Loss: 0.013895 \tValidation Loss: 0.036981\n",
      "Epoch: 33 \tTraining Loss: 0.016293 \tValidation Loss: 0.035112\n",
      "Epoch: 34 \tTraining Loss: 0.012104 \tValidation Loss: 0.030046\n",
      "Epoch: 35 \tTraining Loss: 0.010994 \tValidation Loss: 0.030986\n",
      "Epoch: 36 \tTraining Loss: 0.011521 \tValidation Loss: 0.030809\n",
      "Epoch: 37 \tTraining Loss: 0.011636 \tValidation Loss: 0.028649\n",
      "Epoch: 38 \tTraining Loss: 0.010870 \tValidation Loss: 0.040992\n",
      "Epoch: 39 \tTraining Loss: 0.010881 \tValidation Loss: 0.034166\n",
      "Epoch: 40 \tTraining Loss: 0.010944 \tValidation Loss: 0.032671\n",
      "Epoch: 41 \tTraining Loss: 0.013162 \tValidation Loss: 0.033819\n",
      "Epoch: 42 \tTraining Loss: 0.012504 \tValidation Loss: 0.030038\n",
      "Epoch: 43 \tTraining Loss: 0.010539 \tValidation Loss: 0.029343\n",
      "Epoch: 44 \tTraining Loss: 0.005092 \tValidation Loss: 0.024806\n",
      "Epoch: 45 \tTraining Loss: 0.004629 \tValidation Loss: 0.024901\n",
      "Epoch: 46 \tTraining Loss: 0.003677 \tValidation Loss: 0.026200\n",
      "Epoch: 47 \tTraining Loss: 0.003819 \tValidation Loss: 0.024980\n",
      "Epoch: 48 \tTraining Loss: 0.003499 \tValidation Loss: 0.024723\n",
      "Epoch: 49 \tTraining Loss: 0.003171 \tValidation Loss: 0.025633\n",
      "Epoch: 50 \tTraining Loss: 0.004536 \tValidation Loss: 0.024869\n",
      "Training completed in 1353.0945734977722s\n"
     ]
    }
   ],
   "source": [
    "group_kfold = GroupKFold(n_splits=5)\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(group_kfold.split(train_ids,groups=train_ids['molecule_name'])):\n",
    "    print('\\nstart fold: {}'.format(n_fold))\n",
    "    \n",
    "    # set-up data\n",
    "    train_dl = train_ids.iloc[train_idx]\n",
    "    valid_dl = train_ids.iloc[valid_idx]\n",
    "    train_dl = CustomImageDataset(train_dl, root_dir, transform=None, IsTrain=True)\n",
    "    valid_dl = CustomImageDataset(valid_dl, root_dir, transform=None, IsTrain=True)\n",
    "    test_dl = CustomImageDataset(test_ids, root_dir, transform=None, IsTrain=False)\n",
    "    train_dl = torch.utils.data.DataLoader(train_dl,batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_dl,batch_size=batch_size,shuffle=False,num_workers=num_workers)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dl,batch_size=batch_size,shuffle=False,num_workers=num_workers)\n",
    "    \n",
    "    # train model\n",
    "    model = model_struct().to('cuda')\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = RAdam(model.parameters(),lr=0.0001,weight_decay=1e-2)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=5)\n",
    "    model = train_cnn(model,optimizer,train_dl,valid_dl,n_epochs,clip,scheduler)\n",
    "    \n",
    "    # predict oof\n",
    "    model.eval()\n",
    "    yhat_list = []\n",
    "    with torch.no_grad():\n",
    "        for x_torch,_ in valid_dl:\n",
    "            x_torch = x_torch.to('cuda:0')\n",
    "            yhat_list.append(model(x_torch).squeeze(1))\n",
    "    yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "    \n",
    "    assert yhat.shape[0]==train_ids.iloc[valid_idx].shape[0],'yhat and test_id should have same shape'\n",
    "    submit_ = dict(zip(train_ids.iloc[valid_idx]['id'].values,yhat))\n",
    "    train_df['fold'+str(n_fold)+'_'+type_] = train_df.id.map(submit_)\n",
    "    \n",
    "    # predict test\n",
    "    model.eval()\n",
    "    yhat_list = []\n",
    "    with torch.no_grad():\n",
    "        for data_torch in test_dl:\n",
    "            data_torch = data_torch.to('cuda:0')\n",
    "            yhat_list.append(model(data_torch).squeeze(1))\n",
    "    yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "\n",
    "    # join\n",
    "    assert yhat.shape[0]==test_ids.shape[0],'yhat and test_id should have same shape'\n",
    "    submit_ = dict(zip(test_ids['id'].values,yhat))\n",
    "    test_df['fold'+str(n_fold)+'_'+type_] = test_df.id.map(submit_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start fold: 0\n",
      "\n",
      "start fold: 1\n",
      "Epoch: 1 \tTraining Loss: 1.762554 \tValidation Loss: 0.428096\n",
      "Epoch: 2 \tTraining Loss: 0.288497 \tValidation Loss: 0.221795\n",
      "Epoch: 3 \tTraining Loss: 0.190965 \tValidation Loss: 0.194087\n",
      "Epoch: 4 \tTraining Loss: 0.136508 \tValidation Loss: 0.123796\n",
      "Epoch: 5 \tTraining Loss: 0.109118 \tValidation Loss: 0.114164\n",
      "Epoch: 6 \tTraining Loss: 0.099961 \tValidation Loss: 0.092920\n",
      "Epoch: 7 \tTraining Loss: 0.083005 \tValidation Loss: 0.161890\n",
      "Epoch: 8 \tTraining Loss: 0.071006 \tValidation Loss: 0.099755\n",
      "Epoch: 9 \tTraining Loss: 0.062941 \tValidation Loss: 0.062690\n",
      "Epoch: 10 \tTraining Loss: 0.057434 \tValidation Loss: 0.061134\n",
      "Epoch: 11 \tTraining Loss: 0.052759 \tValidation Loss: 0.079954\n",
      "Epoch: 12 \tTraining Loss: 0.050826 \tValidation Loss: 0.052246\n",
      "Epoch: 13 \tTraining Loss: 0.043867 \tValidation Loss: 0.055242\n",
      "Epoch: 14 \tTraining Loss: 0.036940 \tValidation Loss: 0.080921\n",
      "Epoch: 15 \tTraining Loss: 0.038753 \tValidation Loss: 0.044290\n",
      "Epoch: 16 \tTraining Loss: 0.031092 \tValidation Loss: 0.072736\n",
      "Epoch: 17 \tTraining Loss: 0.034349 \tValidation Loss: 0.044023\n",
      "Epoch: 18 \tTraining Loss: 0.030626 \tValidation Loss: 0.037716\n",
      "Epoch: 19 \tTraining Loss: 0.029560 \tValidation Loss: 0.039320\n",
      "Epoch: 20 \tTraining Loss: 0.026642 \tValidation Loss: 0.040636\n",
      "Epoch: 21 \tTraining Loss: 0.022464 \tValidation Loss: 0.057926\n",
      "Epoch: 22 \tTraining Loss: 0.025892 \tValidation Loss: 0.042914\n",
      "Epoch: 23 \tTraining Loss: 0.022715 \tValidation Loss: 0.037401\n",
      "Epoch: 24 \tTraining Loss: 0.018909 \tValidation Loss: 0.037992\n",
      "Epoch: 25 \tTraining Loss: 0.018161 \tValidation Loss: 0.032903\n",
      "Epoch: 26 \tTraining Loss: 0.015956 \tValidation Loss: 0.035606\n",
      "Epoch: 27 \tTraining Loss: 0.016289 \tValidation Loss: 0.044109\n",
      "Epoch: 28 \tTraining Loss: 0.016745 \tValidation Loss: 0.035781\n",
      "Epoch: 29 \tTraining Loss: 0.015892 \tValidation Loss: 0.033327\n",
      "Epoch: 30 \tTraining Loss: 0.012645 \tValidation Loss: 0.038525\n",
      "Epoch: 31 \tTraining Loss: 0.013681 \tValidation Loss: 0.033241\n",
      "Epoch: 32 \tTraining Loss: 0.008709 \tValidation Loss: 0.028848\n",
      "Epoch: 33 \tTraining Loss: 0.007183 \tValidation Loss: 0.030009\n",
      "Epoch: 34 \tTraining Loss: 0.006805 \tValidation Loss: 0.030619\n",
      "Epoch: 35 \tTraining Loss: 0.006355 \tValidation Loss: 0.028054\n",
      "Epoch: 36 \tTraining Loss: 0.007318 \tValidation Loss: 0.031355\n",
      "Epoch: 37 \tTraining Loss: 0.006037 \tValidation Loss: 0.027502\n",
      "Epoch: 38 \tTraining Loss: 0.006658 \tValidation Loss: 0.027959\n",
      "Epoch: 39 \tTraining Loss: 0.006557 \tValidation Loss: 0.028280\n",
      "Epoch: 40 \tTraining Loss: 0.005672 \tValidation Loss: 0.032492\n",
      "Epoch: 41 \tTraining Loss: 0.006182 \tValidation Loss: 0.028270\n",
      "Epoch: 42 \tTraining Loss: 0.007071 \tValidation Loss: 0.033321\n",
      "Epoch: 43 \tTraining Loss: 0.005888 \tValidation Loss: 0.035777\n",
      "Epoch: 44 \tTraining Loss: 0.003667 \tValidation Loss: 0.026849\n",
      "Epoch: 45 \tTraining Loss: 0.003140 \tValidation Loss: 0.026632\n",
      "Epoch: 46 \tTraining Loss: 0.002974 \tValidation Loss: 0.027093\n",
      "Epoch: 47 \tTraining Loss: 0.002821 \tValidation Loss: 0.025950\n",
      "Epoch: 48 \tTraining Loss: 0.003272 \tValidation Loss: 0.026743\n",
      "Epoch: 49 \tTraining Loss: 0.003553 \tValidation Loss: 0.027652\n",
      "Epoch: 50 \tTraining Loss: 0.003409 \tValidation Loss: 0.026024\n",
      "Training completed in 1346.5524580478668s\n",
      "\n",
      "start fold: 2\n",
      "Epoch: 1 \tTraining Loss: 1.618354 \tValidation Loss: 0.606095\n",
      "Epoch: 2 \tTraining Loss: 0.338398 \tValidation Loss: 0.351972\n",
      "Epoch: 3 \tTraining Loss: 0.214792 \tValidation Loss: 0.158681\n",
      "Epoch: 4 \tTraining Loss: 0.147702 \tValidation Loss: 0.129059\n",
      "Epoch: 5 \tTraining Loss: 0.122632 \tValidation Loss: 0.119723\n",
      "Epoch: 6 \tTraining Loss: 0.098821 \tValidation Loss: 0.101963\n",
      "Epoch: 7 \tTraining Loss: 0.088114 \tValidation Loss: 0.103164\n",
      "Epoch: 8 \tTraining Loss: 0.082345 \tValidation Loss: 0.078532\n",
      "Epoch: 9 \tTraining Loss: 0.070094 \tValidation Loss: 0.084779\n",
      "Epoch: 10 \tTraining Loss: 0.060057 \tValidation Loss: 0.063308\n",
      "Epoch: 11 \tTraining Loss: 0.051607 \tValidation Loss: 0.057469\n",
      "Epoch: 12 \tTraining Loss: 0.047134 \tValidation Loss: 0.063998\n",
      "Epoch: 13 \tTraining Loss: 0.041888 \tValidation Loss: 0.106228\n",
      "Epoch: 14 \tTraining Loss: 0.042666 \tValidation Loss: 0.078869\n",
      "Epoch: 15 \tTraining Loss: 0.036686 \tValidation Loss: 0.062488\n",
      "Epoch: 16 \tTraining Loss: 0.031833 \tValidation Loss: 0.045588\n",
      "Epoch: 17 \tTraining Loss: 0.027552 \tValidation Loss: 0.041312\n",
      "Epoch: 18 \tTraining Loss: 0.030260 \tValidation Loss: 0.047103\n",
      "Epoch: 19 \tTraining Loss: 0.027046 \tValidation Loss: 0.044608\n",
      "Epoch: 20 \tTraining Loss: 0.028645 \tValidation Loss: 0.043221\n",
      "Epoch: 21 \tTraining Loss: 0.020025 \tValidation Loss: 0.041548\n",
      "Epoch: 22 \tTraining Loss: 0.021360 \tValidation Loss: 0.039469\n",
      "Epoch: 23 \tTraining Loss: 0.019675 \tValidation Loss: 0.043101\n",
      "Epoch: 24 \tTraining Loss: 0.022669 \tValidation Loss: 0.041601\n",
      "Epoch: 25 \tTraining Loss: 0.017221 \tValidation Loss: 0.050366\n",
      "Epoch: 26 \tTraining Loss: 0.018116 \tValidation Loss: 0.038755\n",
      "Epoch: 27 \tTraining Loss: 0.018269 \tValidation Loss: 0.038575\n",
      "Epoch: 28 \tTraining Loss: 0.014045 \tValidation Loss: 0.042832\n",
      "Epoch: 29 \tTraining Loss: 0.014381 \tValidation Loss: 0.033375\n",
      "Epoch: 30 \tTraining Loss: 0.016804 \tValidation Loss: 0.034751\n",
      "Epoch: 31 \tTraining Loss: 0.014355 \tValidation Loss: 0.035287\n",
      "Epoch: 32 \tTraining Loss: 0.012541 \tValidation Loss: 0.036150\n",
      "Epoch: 33 \tTraining Loss: 0.013015 \tValidation Loss: 0.032570\n",
      "Epoch: 34 \tTraining Loss: 0.012806 \tValidation Loss: 0.036031\n",
      "Epoch: 35 \tTraining Loss: 0.011888 \tValidation Loss: 0.032923\n",
      "Epoch: 36 \tTraining Loss: 0.011273 \tValidation Loss: 0.033854\n",
      "Epoch: 37 \tTraining Loss: 0.012686 \tValidation Loss: 0.033162\n",
      "Epoch: 38 \tTraining Loss: 0.012108 \tValidation Loss: 0.031676\n",
      "Epoch: 39 \tTraining Loss: 0.009898 \tValidation Loss: 0.036157\n",
      "Epoch: 40 \tTraining Loss: 0.012423 \tValidation Loss: 0.031147\n",
      "Epoch: 41 \tTraining Loss: 0.012601 \tValidation Loss: 0.043907\n",
      "Epoch: 42 \tTraining Loss: 0.012598 \tValidation Loss: 0.030794\n",
      "Epoch: 43 \tTraining Loss: 0.010710 \tValidation Loss: 0.030186\n",
      "Epoch: 44 \tTraining Loss: 0.009330 \tValidation Loss: 0.029634\n",
      "Epoch: 45 \tTraining Loss: 0.008307 \tValidation Loss: 0.029481\n",
      "Epoch: 46 \tTraining Loss: 0.008829 \tValidation Loss: 0.034101\n",
      "Epoch: 47 \tTraining Loss: 0.010500 \tValidation Loss: 0.031401\n",
      "Epoch: 48 \tTraining Loss: 0.008607 \tValidation Loss: 0.030016\n",
      "Epoch: 49 \tTraining Loss: 0.009413 \tValidation Loss: 0.030503\n",
      "Epoch: 50 \tTraining Loss: 0.008711 \tValidation Loss: 0.031230\n",
      "Training completed in 1351.5732765197754s\n",
      "\n",
      "start fold: 3\n",
      "Epoch: 1 \tTraining Loss: 1.732078 \tValidation Loss: 0.427963\n",
      "Epoch: 2 \tTraining Loss: 0.324242 \tValidation Loss: 0.255218\n",
      "Epoch: 3 \tTraining Loss: 0.201527 \tValidation Loss: 0.189527\n",
      "Epoch: 4 \tTraining Loss: 0.152003 \tValidation Loss: 0.141970\n",
      "Epoch: 5 \tTraining Loss: 0.111968 \tValidation Loss: 0.117091\n",
      "Epoch: 6 \tTraining Loss: 0.100523 \tValidation Loss: 0.088612\n",
      "Epoch: 7 \tTraining Loss: 0.084292 \tValidation Loss: 0.083006\n",
      "Epoch: 8 \tTraining Loss: 0.073374 \tValidation Loss: 0.063472\n",
      "Epoch: 9 \tTraining Loss: 0.066026 \tValidation Loss: 0.071682\n",
      "Epoch: 10 \tTraining Loss: 0.057542 \tValidation Loss: 0.070471\n",
      "Epoch: 11 \tTraining Loss: 0.052567 \tValidation Loss: 0.052492\n",
      "Epoch: 12 \tTraining Loss: 0.050460 \tValidation Loss: 0.050616\n",
      "Epoch: 13 \tTraining Loss: 0.047086 \tValidation Loss: 0.050351\n",
      "Epoch: 14 \tTraining Loss: 0.042564 \tValidation Loss: 0.044825\n",
      "Epoch: 15 \tTraining Loss: 0.039393 \tValidation Loss: 0.050860\n",
      "Epoch: 16 \tTraining Loss: 0.030937 \tValidation Loss: 0.058456\n",
      "Epoch: 17 \tTraining Loss: 0.030189 \tValidation Loss: 0.056162\n",
      "Epoch: 18 \tTraining Loss: 0.027726 \tValidation Loss: 0.046994\n",
      "Epoch: 19 \tTraining Loss: 0.027827 \tValidation Loss: 0.041543\n",
      "Epoch: 20 \tTraining Loss: 0.027485 \tValidation Loss: 0.048614\n",
      "Epoch: 21 \tTraining Loss: 0.021612 \tValidation Loss: 0.046604\n",
      "Epoch: 22 \tTraining Loss: 0.022891 \tValidation Loss: 0.042141\n",
      "Epoch: 23 \tTraining Loss: 0.021228 \tValidation Loss: 0.034903\n",
      "Epoch: 24 \tTraining Loss: 0.018404 \tValidation Loss: 0.052786\n",
      "Epoch: 25 \tTraining Loss: 0.019229 \tValidation Loss: 0.034617\n",
      "Epoch: 26 \tTraining Loss: 0.014989 \tValidation Loss: 0.036588\n",
      "Epoch: 27 \tTraining Loss: 0.015263 \tValidation Loss: 0.032798\n",
      "Epoch: 28 \tTraining Loss: 0.016371 \tValidation Loss: 0.051210\n",
      "Epoch: 29 \tTraining Loss: 0.017087 \tValidation Loss: 0.033177\n",
      "Epoch: 30 \tTraining Loss: 0.015945 \tValidation Loss: 0.032468\n",
      "Epoch: 31 \tTraining Loss: 0.018721 \tValidation Loss: 0.035136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 \tTraining Loss: 0.014614 \tValidation Loss: 0.030639\n",
      "Epoch: 33 \tTraining Loss: 0.014970 \tValidation Loss: 0.030902\n",
      "Epoch: 34 \tTraining Loss: 0.012469 \tValidation Loss: 0.031075\n",
      "Epoch: 35 \tTraining Loss: 0.011373 \tValidation Loss: 0.034365\n",
      "Epoch: 36 \tTraining Loss: 0.009821 \tValidation Loss: 0.028747\n",
      "Epoch: 37 \tTraining Loss: 0.011507 \tValidation Loss: 0.032274\n",
      "Epoch: 38 \tTraining Loss: 0.011638 \tValidation Loss: 0.033440\n",
      "Epoch: 39 \tTraining Loss: 0.012474 \tValidation Loss: 0.029723\n",
      "Epoch: 40 \tTraining Loss: 0.010087 \tValidation Loss: 0.030158\n",
      "Epoch: 41 \tTraining Loss: 0.009575 \tValidation Loss: 0.032488\n",
      "Epoch: 42 \tTraining Loss: 0.010853 \tValidation Loss: 0.039610\n",
      "Epoch: 43 \tTraining Loss: 0.006540 \tValidation Loss: 0.026055\n",
      "Epoch: 44 \tTraining Loss: 0.004509 \tValidation Loss: 0.025361\n",
      "Epoch: 45 \tTraining Loss: 0.003549 \tValidation Loss: 0.025784\n",
      "Epoch: 46 \tTraining Loss: 0.003941 \tValidation Loss: 0.025807\n",
      "Epoch: 47 \tTraining Loss: 0.004034 \tValidation Loss: 0.027689\n",
      "Epoch: 48 \tTraining Loss: 0.003861 \tValidation Loss: 0.026110\n",
      "Epoch: 49 \tTraining Loss: 0.004017 \tValidation Loss: 0.027398\n",
      "Epoch: 50 \tTraining Loss: 0.004164 \tValidation Loss: 0.025778\n",
      "Training completed in 1348.480954170227s\n",
      "\n",
      "start fold: 4\n",
      "Epoch: 1 \tTraining Loss: 1.756280 \tValidation Loss: 0.764234\n",
      "Epoch: 2 \tTraining Loss: 0.380594 \tValidation Loss: 0.269119\n",
      "Epoch: 3 \tTraining Loss: 0.231640 \tValidation Loss: 0.220767\n",
      "Epoch: 4 \tTraining Loss: 0.177020 \tValidation Loss: 0.149201\n",
      "Epoch: 5 \tTraining Loss: 0.130949 \tValidation Loss: 0.138596\n",
      "Epoch: 6 \tTraining Loss: 0.112898 \tValidation Loss: 0.104385\n",
      "Epoch: 7 \tTraining Loss: 0.094139 \tValidation Loss: 0.144682\n",
      "Epoch: 8 \tTraining Loss: 0.085082 \tValidation Loss: 0.087913\n",
      "Epoch: 9 \tTraining Loss: 0.076781 \tValidation Loss: 0.073327\n",
      "Epoch: 10 \tTraining Loss: 0.073483 \tValidation Loss: 0.063977\n",
      "Epoch: 11 \tTraining Loss: 0.060009 \tValidation Loss: 0.087649\n",
      "Epoch: 12 \tTraining Loss: 0.061006 \tValidation Loss: 0.054406\n",
      "Epoch: 13 \tTraining Loss: 0.049078 \tValidation Loss: 0.056839\n",
      "Epoch: 14 \tTraining Loss: 0.048238 \tValidation Loss: 0.062263\n",
      "Epoch: 15 \tTraining Loss: 0.039846 \tValidation Loss: 0.051727\n",
      "Epoch: 16 \tTraining Loss: 0.039204 \tValidation Loss: 0.054418\n",
      "Epoch: 17 \tTraining Loss: 0.038417 \tValidation Loss: 0.049100\n",
      "Epoch: 18 \tTraining Loss: 0.037669 \tValidation Loss: 0.049267\n",
      "Epoch: 19 \tTraining Loss: 0.032461 \tValidation Loss: 0.040280\n",
      "Epoch: 20 \tTraining Loss: 0.027545 \tValidation Loss: 0.042629\n",
      "Epoch: 21 \tTraining Loss: 0.028135 \tValidation Loss: 0.044137\n",
      "Epoch: 22 \tTraining Loss: 0.024848 \tValidation Loss: 0.036365\n",
      "Epoch: 23 \tTraining Loss: 0.025069 \tValidation Loss: 0.041842\n",
      "Epoch: 24 \tTraining Loss: 0.023076 \tValidation Loss: 0.038582\n",
      "Epoch: 25 \tTraining Loss: 0.023493 \tValidation Loss: 0.043649\n",
      "Epoch: 26 \tTraining Loss: 0.022428 \tValidation Loss: 0.034818\n",
      "Epoch: 27 \tTraining Loss: 0.018512 \tValidation Loss: 0.034800\n",
      "Epoch: 28 \tTraining Loss: 0.019034 \tValidation Loss: 0.035628\n",
      "Epoch: 29 \tTraining Loss: 0.016269 \tValidation Loss: 0.035930\n",
      "Epoch: 30 \tTraining Loss: 0.016228 \tValidation Loss: 0.032049\n",
      "Epoch: 31 \tTraining Loss: 0.016521 \tValidation Loss: 0.037377\n",
      "Epoch: 32 \tTraining Loss: 0.018384 \tValidation Loss: 0.041625\n",
      "Epoch: 33 \tTraining Loss: 0.016658 \tValidation Loss: 0.041112\n",
      "Epoch: 34 \tTraining Loss: 0.015679 \tValidation Loss: 0.036946\n",
      "Epoch: 35 \tTraining Loss: 0.014030 \tValidation Loss: 0.030849\n",
      "Epoch: 36 \tTraining Loss: 0.012047 \tValidation Loss: 0.030102\n",
      "Epoch: 37 \tTraining Loss: 0.011561 \tValidation Loss: 0.033724\n",
      "Epoch: 38 \tTraining Loss: 0.011877 \tValidation Loss: 0.031212\n",
      "Epoch: 39 \tTraining Loss: 0.011082 \tValidation Loss: 0.030139\n",
      "Epoch: 40 \tTraining Loss: 0.012944 \tValidation Loss: 0.032916\n",
      "Epoch: 41 \tTraining Loss: 0.012361 \tValidation Loss: 0.028980\n",
      "Epoch: 42 \tTraining Loss: 0.010922 \tValidation Loss: 0.030296\n",
      "Epoch: 43 \tTraining Loss: 0.009797 \tValidation Loss: 0.031694\n",
      "Epoch: 44 \tTraining Loss: 0.009673 \tValidation Loss: 0.027543\n",
      "Epoch: 45 \tTraining Loss: 0.009123 \tValidation Loss: 0.029121\n",
      "Epoch: 46 \tTraining Loss: 0.012705 \tValidation Loss: 0.030450\n",
      "Epoch: 47 \tTraining Loss: 0.009926 \tValidation Loss: 0.031309\n",
      "Epoch: 48 \tTraining Loss: 0.009268 \tValidation Loss: 0.027720\n",
      "Epoch: 49 \tTraining Loss: 0.008790 \tValidation Loss: 0.027340\n",
      "Epoch: 50 \tTraining Loss: 0.010885 \tValidation Loss: 0.030835\n",
      "Training completed in 1352.2257719039917s\n"
     ]
    }
   ],
   "source": [
    "group_kfold = GroupKFold(n_splits=5)\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(group_kfold.split(train_ids,groups=train_ids['molecule_name'])):\n",
    "    print('\\nstart fold: {}'.format(n_fold))\n",
    "    if n_fold==0:continue\n",
    "    # set-up data\n",
    "    train_dl = train_ids.iloc[train_idx]\n",
    "    valid_dl = train_ids.iloc[valid_idx]\n",
    "    train_dl = CustomImageDataset(train_dl, root_dir, transform=None, IsTrain=True)\n",
    "    valid_dl = CustomImageDataset(valid_dl, root_dir, transform=None, IsTrain=True)\n",
    "    test_dl = CustomImageDataset(test_ids, root_dir, transform=None, IsTrain=False)\n",
    "    train_dl = torch.utils.data.DataLoader(train_dl,batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_dl,batch_size=batch_size,shuffle=False,num_workers=num_workers)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dl,batch_size=batch_size,shuffle=False,num_workers=num_workers)\n",
    "    \n",
    "    # train model\n",
    "    model = model_struct().to('cuda')\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = RAdam(model.parameters(),lr=0.0001,weight_decay=1e-2)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=5)\n",
    "    model = train_cnn(model,optimizer,train_dl,valid_dl,n_epochs,clip,scheduler)\n",
    "    \n",
    "    # predict oof\n",
    "    model.eval()\n",
    "    yhat_list = []\n",
    "    with torch.no_grad():\n",
    "        for x_torch,_ in valid_dl:\n",
    "            x_torch = x_torch.to('cuda:0')\n",
    "            yhat_list.append(model(x_torch).squeeze(1))\n",
    "    yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "    \n",
    "    assert yhat.shape[0]==train_ids.iloc[valid_idx].shape[0],'yhat and test_id should have same shape'\n",
    "    submit_ = dict(zip(train_ids.iloc[valid_idx]['id'].values,yhat))\n",
    "    train_df['fold'+str(n_fold)+'_'+type_] = train_df.id.map(submit_)\n",
    "    \n",
    "    # predict test\n",
    "    model.eval()\n",
    "    yhat_list = []\n",
    "    with torch.no_grad():\n",
    "        for data_torch in test_dl:\n",
    "            data_torch = data_torch.to('cuda:0')\n",
    "            yhat_list.append(model(data_torch).squeeze(1))\n",
    "    yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "\n",
    "    # join\n",
    "    assert yhat.shape[0]==test_ids.shape[0],'yhat and test_id should have same shape'\n",
    "    submit_ = dict(zip(test_ids['id'].values,yhat))\n",
    "    test_df['fold'+str(n_fold)+'_'+type_] = test_df.id.map(submit_)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert set(test.iloc[:,5:].isnull().sum(1)) == set([7*5])\n",
    "test_df['yhat'] = np.nanmean(test_df.iloc[:,5:],1)\n",
    "#test = test[['id','yhat']]\n",
    "test_df.to_csv('../Data/test_oof_'+type_,index=False)\n",
    "\n",
    "#assert set(train.iloc[:,6:].isnull().sum(1)) == set([train.iloc[:,6:].shape[1]-1])\n",
    "train_df['yhat'] = np.nanmean(train_df.iloc[:,6:],1)\n",
    "#train = train[['id','yhat']]\n",
    "train_df.to_csv('../Data/train_oof_'+type_,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: Mean of empty slice\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_df['yhat'] = np.nanmean(train_df.iloc[:,6:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.loc[~train_df.yhat.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119253, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8672321232564857"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.mean(np.abs(train_df2.scalar_coupling_constant-train_df2.yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15455084804097682"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(train_df2.scalar_coupling_constant-train_df2.yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>fold0_2JHN</th>\n",
       "      <th>fold1_2JHN</th>\n",
       "      <th>fold2_2JHN</th>\n",
       "      <th>fold3_2JHN</th>\n",
       "      <th>fold4_2JHN</th>\n",
       "      <th>yhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4658152</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4658153</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4658154</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4658155</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4658156</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4658157</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4658158</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4658159</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4658160</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4658161</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4658162</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4658163</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4658164</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4658165</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4658166</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4658167</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4658168</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4658169</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4658170</td>\n",
       "      <td>dsgdb9nsd_000016</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4658171</td>\n",
       "      <td>dsgdb9nsd_000016</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4658172</td>\n",
       "      <td>dsgdb9nsd_000016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4658173</td>\n",
       "      <td>dsgdb9nsd_000016</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4658174</td>\n",
       "      <td>dsgdb9nsd_000016</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4658175</td>\n",
       "      <td>dsgdb9nsd_000016</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4658176</td>\n",
       "      <td>dsgdb9nsd_000016</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505512</th>\n",
       "      <td>7163659</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505513</th>\n",
       "      <td>7163660</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505514</th>\n",
       "      <td>7163661</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505515</th>\n",
       "      <td>7163662</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505516</th>\n",
       "      <td>7163663</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505517</th>\n",
       "      <td>7163664</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505518</th>\n",
       "      <td>7163665</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505519</th>\n",
       "      <td>7163666</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505520</th>\n",
       "      <td>7163667</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505521</th>\n",
       "      <td>7163668</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505522</th>\n",
       "      <td>7163669</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505523</th>\n",
       "      <td>7163670</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505524</th>\n",
       "      <td>7163671</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505525</th>\n",
       "      <td>7163672</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505526</th>\n",
       "      <td>7163673</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505527</th>\n",
       "      <td>7163674</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHN</td>\n",
       "      <td>2.99997</td>\n",
       "      <td>3.302536</td>\n",
       "      <td>2.527809</td>\n",
       "      <td>3.222677</td>\n",
       "      <td>2.90786</td>\n",
       "      <td>2.992171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505528</th>\n",
       "      <td>7163675</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505529</th>\n",
       "      <td>7163676</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505530</th>\n",
       "      <td>7163677</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505531</th>\n",
       "      <td>7163678</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505532</th>\n",
       "      <td>7163679</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505533</th>\n",
       "      <td>7163680</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505534</th>\n",
       "      <td>7163681</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505535</th>\n",
       "      <td>7163682</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3JHN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505536</th>\n",
       "      <td>7163683</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505537</th>\n",
       "      <td>7163684</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505538</th>\n",
       "      <td>7163685</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505539</th>\n",
       "      <td>7163686</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>3JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505540</th>\n",
       "      <td>7163687</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505541</th>\n",
       "      <td>7163688</td>\n",
       "      <td>dsgdb9nsd_133885</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2505542 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     molecule_name  atom_index_0  atom_index_1  type  \\\n",
       "0        4658147  dsgdb9nsd_000004             2             0  2JHC   \n",
       "1        4658148  dsgdb9nsd_000004             2             1  1JHC   \n",
       "2        4658149  dsgdb9nsd_000004             2             3  3JHH   \n",
       "3        4658150  dsgdb9nsd_000004             3             0  1JHC   \n",
       "4        4658151  dsgdb9nsd_000004             3             1  2JHC   \n",
       "5        4658152  dsgdb9nsd_000015             3             0  1JHC   \n",
       "6        4658153  dsgdb9nsd_000015             3             2  3JHC   \n",
       "7        4658154  dsgdb9nsd_000015             3             4  2JHH   \n",
       "8        4658155  dsgdb9nsd_000015             3             5  2JHH   \n",
       "9        4658156  dsgdb9nsd_000015             4             0  1JHC   \n",
       "10       4658157  dsgdb9nsd_000015             4             2  3JHC   \n",
       "11       4658158  dsgdb9nsd_000015             4             5  2JHH   \n",
       "12       4658159  dsgdb9nsd_000015             5             0  1JHC   \n",
       "13       4658160  dsgdb9nsd_000015             5             2  3JHC   \n",
       "14       4658161  dsgdb9nsd_000015             6             0  3JHC   \n",
       "15       4658162  dsgdb9nsd_000015             6             2  1JHC   \n",
       "16       4658163  dsgdb9nsd_000015             6             7  2JHH   \n",
       "17       4658164  dsgdb9nsd_000015             6             8  2JHH   \n",
       "18       4658165  dsgdb9nsd_000015             7             0  3JHC   \n",
       "19       4658166  dsgdb9nsd_000015             7             2  1JHC   \n",
       "20       4658167  dsgdb9nsd_000015             7             8  2JHH   \n",
       "21       4658168  dsgdb9nsd_000015             8             0  3JHC   \n",
       "22       4658169  dsgdb9nsd_000015             8             2  1JHC   \n",
       "23       4658170  dsgdb9nsd_000016             3             0  1JHC   \n",
       "24       4658171  dsgdb9nsd_000016             3             1  2JHC   \n",
       "25       4658172  dsgdb9nsd_000016             3             2  2JHC   \n",
       "26       4658173  dsgdb9nsd_000016             3             4  2JHH   \n",
       "27       4658174  dsgdb9nsd_000016             3             5  3JHH   \n",
       "28       4658175  dsgdb9nsd_000016             3             6  3JHH   \n",
       "29       4658176  dsgdb9nsd_000016             3             7  3JHH   \n",
       "...          ...               ...           ...           ...   ...   \n",
       "2505512  7163659  dsgdb9nsd_133885            12             3  1JHC   \n",
       "2505513  7163660  dsgdb9nsd_133885            12             4  2JHC   \n",
       "2505514  7163661  dsgdb9nsd_133885            12             6  3JHC   \n",
       "2505515  7163662  dsgdb9nsd_133885            12             7  3JHC   \n",
       "2505516  7163663  dsgdb9nsd_133885            12             8  2JHC   \n",
       "2505517  7163664  dsgdb9nsd_133885            12            13  3JHH   \n",
       "2505518  7163665  dsgdb9nsd_133885            12            15  3JHH   \n",
       "2505519  7163666  dsgdb9nsd_133885            13             2  3JHC   \n",
       "2505520  7163667  dsgdb9nsd_133885            13             3  2JHC   \n",
       "2505521  7163668  dsgdb9nsd_133885            13             4  1JHC   \n",
       "2505522  7163669  dsgdb9nsd_133885            13             6  3JHC   \n",
       "2505523  7163670  dsgdb9nsd_133885            13             7  3JHC   \n",
       "2505524  7163671  dsgdb9nsd_133885            13             8  2JHC   \n",
       "2505525  7163672  dsgdb9nsd_133885            13            15  3JHH   \n",
       "2505526  7163673  dsgdb9nsd_133885            14             0  3JHC   \n",
       "2505527  7163674  dsgdb9nsd_133885            14             1  2JHN   \n",
       "2505528  7163675  dsgdb9nsd_133885            14             2  3JHC   \n",
       "2505529  7163676  dsgdb9nsd_133885            14             3  3JHC   \n",
       "2505530  7163677  dsgdb9nsd_133885            14             4  3JHC   \n",
       "2505531  7163678  dsgdb9nsd_133885            14             6  2JHC   \n",
       "2505532  7163679  dsgdb9nsd_133885            14             7  1JHC   \n",
       "2505533  7163680  dsgdb9nsd_133885            14             8  2JHC   \n",
       "2505534  7163681  dsgdb9nsd_133885            14            15  3JHH   \n",
       "2505535  7163682  dsgdb9nsd_133885            15             1  3JHN   \n",
       "2505536  7163683  dsgdb9nsd_133885            15             2  3JHC   \n",
       "2505537  7163684  dsgdb9nsd_133885            15             3  2JHC   \n",
       "2505538  7163685  dsgdb9nsd_133885            15             4  2JHC   \n",
       "2505539  7163686  dsgdb9nsd_133885            15             6  3JHC   \n",
       "2505540  7163687  dsgdb9nsd_133885            15             7  2JHC   \n",
       "2505541  7163688  dsgdb9nsd_133885            15             8  1JHC   \n",
       "\n",
       "         fold0_2JHN  fold1_2JHN  fold2_2JHN  fold3_2JHN  fold4_2JHN      yhat  \n",
       "0               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "1               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "3               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "4               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "5               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "6               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "7               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "8               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "9               NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "10              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "11              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "12              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "13              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "14              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "15              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "16              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "17              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "18              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "19              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "20              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "21              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "22              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "23              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "24              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "25              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "26              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "27              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "28              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "29              NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "...             ...         ...         ...         ...         ...       ...  \n",
       "2505512         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505513         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505514         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505515         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505516         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505517         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505518         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505519         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505520         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505521         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505522         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505523         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505524         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505525         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505526         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505527     2.99997    3.302536    2.527809    3.222677     2.90786  2.992171  \n",
       "2505528         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505529         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505530         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505531         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505532         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505533         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505534         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505535         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505536         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505537         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505538         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505539         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505540         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "2505541         NaN         NaN         NaN         NaN         NaN       NaN  \n",
       "\n",
       "[2505542 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ = test_df.type==type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../Data/test_model2_bigger_0823_-2.596.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.loc[index_,'scalar_coupling_constant'] = test_df.loc[index_,'yhat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../Submission/CNN_kfold_2JHN.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
