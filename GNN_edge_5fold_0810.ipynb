{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GNN_edge_5fold_0810.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"zj1OZEMe8yv2","colab":{}},"source":["import pickle\n","import torch\n","from torch_geometric.data import Data,DataLoader\n","from functions_refactor import *\n","from pytorch_util import *\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B4DHlNoJ8yv6","colab":{}},"source":["# fixed parameters\n","clip = 2\n","batch_size = 48\n","lr = 1e-4\n","threshold = -1.3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t57aEB9W8yv8","scrolled":true,"colab":{}},"source":["# changing parameters\n","head = feedforwardHead_Update\n","data = '../Data/{}_data_ACSF_SOAP_atomInfo_otherInfo.pickle'\n","dim = 512\n","logLoss = True\n","weight = 0.6\n","layer1 = 3\n","layer2 = 3\n","factor = 2\n","epochs = 150\n","BatchNorm = True\n","useMax = False\n","interleave = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZWY5cdxw8yv_","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2R_CGPnI8ywB","colab":{}},"source":["prefix = '_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n","                                        for i in [head,data,dim,logLoss,weight,layer1,layer2,factor,\\\n","                                                  BatchNorm,useMax,interleave]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MijEZgrd8ywD","colab":{}},"source":["train_df = pd.read_csv('../Data/train.csv')\n","test_df = pd.read_csv('../Data/test.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P1EThOHi8ywE","colab":{}},"source":["folds = []\n","for f in range(5):\n","    with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(f)+'.pickle', 'rb') as handle:\n","        folds.append(pickle.load(handle))\n","folds = [[Data(**d) for d in fold] for fold in folds]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M-TFr-yT8ywG","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A0kjHO6t8ywH","scrolled":false,"colab":{}},"source":["for i in range(5):\n","    print('\\nstart fold '+str(i))\n","    # parpare data\n","    train_list = []\n","    val_list = []\n","    for j in range(5):\n","        if i == j:\n","            val_list.extend(folds[j])\n","        else:\n","            train_list.extend(folds[j])\n","    \n","    train_dl = DataLoader(train_list,batch_size,shuffle=True)\n","    val_dl = DataLoader(val_list,batch_size,shuffle=False)\n","    \n","    # train model\n","    model = GNN_MataLayer(head,head_mol2,head_atom,head_edge,\\\n","                          dim,layer1,layer2,factor,**data_dict[data],\\\n","                          BatchNorm=BatchNorm,useMax=useMax,interleave=interleave).to('cuda:0')\n","    paras = trainable_parameter(model)\n","    opt = Adam(paras,lr=lr)\n","    scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n","    \n","    model,train_loss_perType,val_loss_perType,bestWeight = train_type_earlyStop(opt,model,epochs_type,train_dl,val_dl,paras,clip,\\\n","                                                                scheduler=scheduler,logLoss=logLoss,weight=weight,threshold=threshold)    \n","    torch.save({'model_state_dict_type_'+str(j_):w for j_,w in enumerate(bestWeight)},\\\n","                '../Model/'+prefix+'_fold'+str(i)+'.tar')\n","    # predict oof for each type\n","    for type_i in range(8):\n","        # load val data and type_id\n","        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n","            test_data = pickle.load(handle)\n","        test_list = [Data(**d) for d in test_data]\n","        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n","        \n","        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'_id.pickle', 'rb') as handle:\n","            test_id = pickle.load(handle)\n","    \n","        # load model\n","        model.load_state_dict(bestWeight[type_i])\n","    \n","        # predict\n","        model.eval()\n","        yhat_list = []\n","        with torch.no_grad():\n","            for data_torch in test_dl:\n","                data_torch = data_torch.to('cuda:0')\n","                yhat_list.append(model(data_torch,False,True))\n","        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n","    \n","        # join\n","        assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n","        submit_ = dict(zip(test_id,yhat))\n","        train_df['fold'+str(i)+'_type'+str(type_i)] = train_df.id.map(submit_)\n","    \n","    # predict test\n","    for type_i in range(8):\n","        # load val data and type_id\n","        with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n","            test_data = pickle.load(handle)\n","        test_list = [Data(**d) for d in test_data]\n","        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n","        \n","        with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(type_i)+'.pickle', 'rb') as handle:\n","            test_id = pickle.load(handle)\n","    \n","        # load model\n","        model.load_state_dict(bestWeight[type_i])\n","    \n","        # predict\n","        model.eval()\n","        yhat_list = []\n","        with torch.no_grad():\n","            for data_torch in test_dl:\n","                data_torch = data_torch.to('cuda:0')\n","                yhat_list.append(model(data_torch,False,True))\n","        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n","    \n","        # join\n","        assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n","        submit_ = dict(zip(test_id,yhat))\n","        test_df['fold'+str(i)+'_type'+str(type_i)] = test_df.id.map(submit_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r2gkclcB8ywK","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PDHoGUIQ8ywM","colab":{}},"source":["#assert set(test.iloc[:,5:].isnull().sum(1)) == set([7*5])\n","test_df['yhat'] = np.nanmean(test_df.iloc[:,5:],1)\n","#test = test[['id','yhat']]\n","test_df.to_csv('../Data/test_oof_'+prefix,index=False)\n","\n","#assert set(train.iloc[:,6:].isnull().sum(1)) == set([train.iloc[:,6:].shape[1]-1])\n","train_df['yhat'] = np.nanmean(train_df.iloc[:,6:],1)\n","#train = train[['id','yhat']]\n","train_df.to_csv('../Data/train_oof_'+prefix,index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S_P9q9ji8ywN","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UKVCVd9W8ywP","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}