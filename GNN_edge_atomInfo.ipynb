{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "from functions_refactor import *\n",
    "from pytorch_util import *\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "reuse = False\n",
    "block = MEGNet_block\n",
    "head = feedforwardHead_Update\n",
    "data = '../Data/{}_data_atomInfo.pickle'\n",
    "batch_size = 32\n",
    "dim = 128\n",
    "epochs = 50\n",
    "clip = 0.4\n",
    "layer1 = 3\n",
    "layer2 = 3\n",
    "factor = 2\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +0.328, val_loss: -0.065, \n",
      "train_vector: +4.22|+0.99|+0.00|-0.50|-0.59|+0.08|-0.67|-0.91, \n",
      "val_vector  : +3.94|+0.45|-0.35|-1.13|-0.93|-0.21|-1.07|-1.21\n",
      "\n",
      "epoch:1, train_loss: -0.268, val_loss: -0.648, \n",
      "train_vector: +2.59|+0.32|-0.37|-1.01|-1.10|-0.25|-1.06|-1.27, \n",
      "val_vector  : +0.95|+0.06|-0.48|-1.37|-1.31|-0.37|-1.25|-1.41\n",
      "\n",
      "epoch:2, train_loss: -0.632, val_loss: -0.735, \n",
      "train_vector: +0.88|+0.13|-0.52|-1.25|-1.28|-0.38|-1.21|-1.44, \n",
      "val_vector  : +0.73|+0.07|-0.66|-1.17|-1.41|-0.48|-1.41|-1.55\n",
      "\n",
      "epoch:3, train_loss: -0.747, val_loss: -0.901, \n",
      "train_vector: +0.79|-0.04|-0.61|-1.39|-1.40|-0.46|-1.31|-1.56, \n",
      "val_vector  : +0.58|-0.35|-0.72|-1.74|-1.48|-0.54|-1.35|-1.60\n",
      "\n",
      "epoch:4, train_loss: -0.819, val_loss: -0.963, \n",
      "train_vector: +0.76|-0.13|-0.68|-1.49|-1.48|-0.52|-1.38|-1.63, \n",
      "val_vector  : +0.53|-0.36|-0.81|-1.67|-1.59|-0.60|-1.47|-1.73\n",
      "\n",
      "epoch:5, train_loss: -0.871, val_loss: -0.977, \n",
      "train_vector: +0.72|-0.17|-0.74|-1.54|-1.54|-0.56|-1.43|-1.70, \n",
      "val_vector  : +0.42|-0.51|-0.83|-1.37|-1.58|-0.64|-1.55|-1.76\n",
      "\n",
      "epoch:6, train_loss: -0.925, val_loss: -1.057, \n",
      "train_vector: +0.71|-0.26|-0.78|-1.63|-1.60|-0.61|-1.48|-1.76, \n",
      "val_vector  : +0.23|-0.36|-0.89|-1.62|-1.67|-0.69|-1.61|-1.82\n",
      "\n",
      "epoch:7, train_loss: -0.962, val_loss: -1.053, \n",
      "train_vector: +0.66|-0.29|-0.82|-1.65|-1.64|-0.64|-1.52|-1.81, \n",
      "val_vector  : +0.07|-0.14|-0.91|-1.57|-1.78|-0.72|-1.55|-1.82\n",
      "\n",
      "epoch:8, train_loss: -0.994, val_loss: -1.099, \n",
      "train_vector: +0.67|-0.32|-0.86|-1.68|-1.68|-0.67|-1.56|-1.85, \n",
      "val_vector  : -0.02|+0.03|-0.95|-1.86|-1.66|-0.75|-1.69|-1.89\n",
      "\n",
      "epoch:9, train_loss: -1.032, val_loss: -1.142, \n",
      "train_vector: +0.66|-0.38|-0.89|-1.75|-1.72|-0.70|-1.59|-1.88, \n",
      "val_vector  : +0.06|-0.24|-0.97|-1.75|-1.80|-0.79|-1.73|-1.92\n",
      "\n",
      "epoch:10, train_loss: -1.065, val_loss: -1.232, \n",
      "train_vector: +0.60|-0.42|-0.92|-1.76|-1.75|-0.73|-1.62|-1.92, \n",
      "val_vector  : -0.08|-0.62|-1.00|-1.85|-1.79|-0.80|-1.77|-1.93\n",
      "\n",
      "epoch:11, train_loss: -1.096, val_loss: -1.060, \n",
      "train_vector: +0.58|-0.45|-0.95|-1.81|-1.79|-0.75|-1.65|-1.95, \n",
      "val_vector  : +0.47|+0.29|-1.03|-2.01|-1.88|-0.82|-1.54|-1.97\n",
      "\n",
      "epoch:12, train_loss: -1.113, val_loss: -1.216, \n",
      "train_vector: +0.54|-0.43|-0.97|-1.83|-1.80|-0.77|-1.66|-1.98, \n",
      "val_vector  : +0.33|-0.77|-1.06|-1.79|-1.89|-0.85|-1.69|-2.01\n",
      "\n",
      "epoch:13, train_loss: -1.153, val_loss: -1.222, \n",
      "train_vector: +0.44|-0.49|-0.99|-1.87|-1.84|-0.79|-1.69|-2.00, \n",
      "val_vector  : -0.02|-0.24|-1.07|-2.09|-1.79|-0.84|-1.78|-1.95\n",
      "\n",
      "epoch:14, train_loss: -1.173, val_loss: -1.229, \n",
      "train_vector: +0.41|-0.50|-1.01|-1.89|-1.86|-0.81|-1.70|-2.03, \n",
      "val_vector  : +0.08|-0.39|-1.09|-1.89|-1.89|-0.88|-1.73|-2.05\n",
      "\n",
      "epoch:15, train_loss: -1.195, val_loss: -1.298, \n",
      "train_vector: +0.37|-0.51|-1.03|-1.90|-1.88|-0.83|-1.73|-2.06, \n",
      "val_vector  : +0.03|-0.59|-1.10|-2.14|-1.92|-0.84|-1.82|-2.01\n",
      "\n",
      "epoch:16, train_loss: -1.229, val_loss: -1.326, \n",
      "train_vector: +0.30|-0.55|-1.05|-1.94|-1.91|-0.85|-1.75|-2.08, \n",
      "val_vector  : -0.17|-0.71|-1.12|-1.77|-1.96|-0.91|-1.89|-2.08\n",
      "\n",
      "epoch:17, train_loss: -1.247, val_loss: -1.323, \n",
      "train_vector: +0.26|-0.58|-1.07|-1.96|-1.92|-0.86|-1.76|-2.09, \n",
      "val_vector  : +0.31|-0.86|-1.14|-1.99|-1.94|-0.93|-1.91|-2.11\n",
      "\n",
      "epoch:18, train_loss: -1.268, val_loss: -1.242, \n",
      "train_vector: +0.23|-0.59|-1.08|-1.98|-1.94|-0.88|-1.78|-2.12, \n",
      "val_vector  : +0.65|-0.63|-1.17|-2.05|-1.82|-0.93|-1.86|-2.12\n",
      "\n",
      "epoch:19, train_loss: -1.287, val_loss: -1.440, \n",
      "train_vector: +0.18|-0.61|-1.10|-1.97|-1.96|-0.89|-1.80|-2.15, \n",
      "val_vector  : -0.24|-0.98|-1.17|-2.17|-1.94|-0.95|-1.93|-2.14\n",
      "\n",
      "epoch:20, train_loss: -1.307, val_loss: -1.331, \n",
      "train_vector: +0.15|-0.62|-1.12|-2.01|-1.98|-0.90|-1.82|-2.16, \n",
      "val_vector  : -0.05|-0.60|-1.16|-1.98|-1.96|-0.95|-1.82|-2.13\n",
      "\n",
      "epoch:21, train_loss: -1.324, val_loss: -1.352, \n",
      "train_vector: +0.12|-0.64|-1.13|-2.00|-2.00|-0.92|-1.83|-2.18, \n",
      "val_vector  : -0.13|-0.49|-1.18|-2.08|-1.97|-0.97|-1.87|-2.13\n",
      "\n",
      "epoch:22, train_loss: -1.344, val_loss: -1.408, \n",
      "train_vector: +0.09|-0.66|-1.14|-2.05|-2.01|-0.93|-1.85|-2.20, \n",
      "val_vector  : -0.22|-0.87|-1.21|-2.10|-1.99|-0.96|-1.79|-2.13\n",
      "\n",
      "epoch:23, train_loss: -1.353, val_loss: -1.277, \n",
      "train_vector: +0.08|-0.67|-1.16|-2.04|-2.03|-0.94|-1.86|-2.22, \n",
      "val_vector  : +0.85|-0.80|-1.18|-1.94|-2.05|-0.98|-1.96|-2.17\n",
      "\n",
      "epoch:24, train_loss: -1.366, val_loss: -1.452, \n",
      "train_vector: +0.08|-0.68|-1.17|-2.07|-2.04|-0.95|-1.87|-2.23, \n",
      "val_vector  : +0.01|-0.98|-1.23|-2.22|-2.07|-0.98|-1.99|-2.17\n",
      "\n",
      "epoch:25, train_loss: -1.383, val_loss: -1.402, \n",
      "train_vector: +0.05|-0.72|-1.18|-2.07|-2.05|-0.96|-1.89|-2.24, \n",
      "val_vector  : +0.17|-0.92|-1.23|-2.21|-2.06|-0.99|-1.80|-2.18\n",
      "\n",
      "epoch:26, train_loss: -1.396, val_loss: -1.361, \n",
      "train_vector: +0.04|-0.74|-1.19|-2.07|-2.07|-0.97|-1.90|-2.26, \n",
      "val_vector  : +0.02|-0.87|-1.24|-2.01|-1.89|-1.01|-1.81|-2.06\n",
      "\n",
      "epoch:27, train_loss: -1.416, val_loss: -1.482, \n",
      "train_vector: -0.01|-0.76|-1.20|-2.10|-2.09|-0.98|-1.91|-2.27, \n",
      "val_vector  : +0.14|-1.08|-1.25|-2.32|-2.06|-1.03|-2.02|-2.21\n",
      "\n",
      "epoch:28, train_loss: -1.434, val_loss: -1.326, \n",
      "train_vector: -0.04|-0.78|-1.21|-2.12|-2.10|-0.99|-1.93|-2.29, \n",
      "val_vector  : +0.14|-0.51|-1.26|-1.71|-2.08|-1.02|-1.94|-2.23\n",
      "\n",
      "epoch:29, train_loss: -1.439, val_loss: -1.336, \n",
      "train_vector: -0.04|-0.77|-1.22|-2.12|-2.11|-1.00|-1.94|-2.30, \n",
      "val_vector  : +0.86|-0.64|-1.29|-2.28|-2.12|-1.05|-1.95|-2.22\n",
      "\n",
      "epoch:30, train_loss: -1.456, val_loss: -1.401, \n",
      "train_vector: -0.07|-0.78|-1.24|-2.14|-2.13|-1.01|-1.95|-2.32, \n",
      "val_vector  : -0.35|-0.11|-1.28|-2.02|-2.12|-1.05|-2.03|-2.26\n",
      "\n",
      "epoch:31, train_loss: -1.465, val_loss: -1.534, \n",
      "train_vector: -0.09|-0.80|-1.24|-2.14|-2.14|-1.02|-1.96|-2.33, \n",
      "val_vector  : -0.12|-1.09|-1.26|-2.33|-2.11|-1.04|-2.06|-2.26\n",
      "\n",
      "epoch:32, train_loss: -1.474, val_loss: -1.553, \n",
      "train_vector: -0.10|-0.80|-1.25|-2.15|-2.15|-1.03|-1.97|-2.34, \n",
      "val_vector  : -0.40|-0.92|-1.26|-2.35|-2.11|-1.07|-2.06|-2.26\n",
      "\n",
      "epoch:33, train_loss: -1.485, val_loss: -1.328, \n",
      "train_vector: -0.10|-0.82|-1.26|-2.16|-2.17|-1.04|-1.99|-2.35, \n",
      "val_vector  : +0.21|-0.05|-1.28|-2.27|-2.05|-1.06|-1.88|-2.26\n",
      "\n",
      "epoch:34, train_loss: -1.495, val_loss: -1.562, \n",
      "train_vector: -0.13|-0.83|-1.27|-2.15|-2.17|-1.04|-1.99|-2.36, \n",
      "val_vector  : -0.33|-1.12|-1.30|-2.19|-2.12|-1.08|-2.06|-2.30\n",
      "\n",
      "epoch:35, train_loss: -1.507, val_loss: -1.540, \n",
      "train_vector: -0.13|-0.85|-1.28|-2.19|-2.18|-1.05|-2.00|-2.38, \n",
      "val_vector  : -0.18|-1.09|-1.33|-2.16|-2.13|-1.09|-2.04|-2.30\n",
      "\n",
      "epoch:36, train_loss: -1.515, val_loss: -1.421, \n",
      "train_vector: -0.14|-0.84|-1.29|-2.19|-2.20|-1.06|-2.01|-2.39, \n",
      "val_vector  : +0.51|-1.05|-1.30|-1.95|-2.13|-1.08|-2.07|-2.30\n",
      "\n",
      "epoch:37, train_loss: -1.520, val_loss: -1.478, \n",
      "train_vector: -0.14|-0.85|-1.29|-2.20|-2.20|-1.06|-2.01|-2.40, \n",
      "val_vector  : -0.25|-0.56|-1.33|-2.10|-2.17|-1.07|-2.03|-2.32\n",
      "\n",
      "epoch:38, train_loss: -1.535, val_loss: -1.522, \n",
      "train_vector: -0.17|-0.87|-1.30|-2.22|-2.21|-1.07|-2.02|-2.41, \n",
      "val_vector  : +0.02|-0.78|-1.32|-2.35|-2.17|-1.11|-2.13|-2.33\n",
      "\n",
      "epoch:39, train_loss: -1.541, val_loss: -1.490, \n",
      "train_vector: -0.17|-0.89|-1.31|-2.21|-2.22|-1.08|-2.03|-2.42, \n",
      "val_vector  : -0.18|-0.33|-1.37|-2.40|-2.07|-1.12|-2.13|-2.33\n",
      "\n",
      "epoch:40, train_loss: -1.547, val_loss: -1.506, \n",
      "train_vector: -0.17|-0.89|-1.32|-2.21|-2.23|-1.09|-2.04|-2.43, \n",
      "val_vector  : +0.28|-0.90|-1.36|-2.32|-2.17|-1.12|-2.12|-2.35\n",
      "\n",
      "epoch:41, train_loss: -1.696, val_loss: -1.678, \n",
      "train_vector: -0.34|-1.15|-1.39|-2.40|-2.41|-1.14|-2.16|-2.59, \n",
      "val_vector  : -0.50|-0.79|-1.43|-2.54|-2.32|-1.18|-2.20|-2.47\n",
      "\n",
      "epoch:42, train_loss: -1.722, val_loss: -1.635, \n",
      "train_vector: -0.35|-1.19|-1.41|-2.42|-2.45|-1.15|-2.18|-2.63, \n",
      "val_vector  : -0.11|-1.34|-1.42|-2.08|-2.34|-1.17|-2.19|-2.43\n",
      "\n",
      "epoch:43, train_loss: -1.737, val_loss: -1.679, \n",
      "train_vector: -0.38|-1.20|-1.42|-2.44|-2.47|-1.16|-2.19|-2.64, \n",
      "val_vector  : -0.31|-0.97|-1.45|-2.45|-2.35|-1.19|-2.24|-2.46\n",
      "\n",
      "epoch:44, train_loss: -1.743, val_loss: -1.576, \n",
      "train_vector: -0.37|-1.20|-1.42|-2.45|-2.47|-1.17|-2.20|-2.66, \n",
      "val_vector  : +0.01|-0.35|-1.45|-2.57|-2.39|-1.18|-2.19|-2.49\n",
      "\n",
      "epoch:45, train_loss: -1.745, val_loss: -1.556, \n",
      "train_vector: -0.38|-1.20|-1.43|-2.44|-2.48|-1.17|-2.21|-2.66, \n",
      "val_vector  : +0.36|-0.61|-1.44|-2.58|-2.34|-1.19|-2.18|-2.47\n",
      "\n",
      "epoch:46, train_loss: -1.759, val_loss: -1.652, \n",
      "train_vector: -0.39|-1.23|-1.43|-2.46|-2.49|-1.17|-2.22|-2.67, \n",
      "val_vector  : -0.06|-0.95|-1.46|-2.46|-2.37|-1.20|-2.24|-2.48\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47, train_loss: -1.769, val_loss: -1.684, \n",
      "train_vector: -0.40|-1.25|-1.44|-2.47|-2.51|-1.18|-2.22|-2.68, \n",
      "val_vector  : -0.21|-0.92|-1.46|-2.53|-2.37|-1.20|-2.27|-2.50\n",
      "\n",
      "epoch:48, train_loss: -1.765, val_loss: -1.724, \n",
      "train_vector: -0.39|-1.22|-1.44|-2.47|-2.51|-1.18|-2.22|-2.69, \n",
      "val_vector  : -0.29|-1.14|-1.46|-2.58|-2.38|-1.20|-2.25|-2.49\n",
      "\n",
      "epoch:49, train_loss: -1.776, val_loss: -1.767, \n",
      "train_vector: -0.40|-1.25|-1.45|-2.48|-2.52|-1.19|-2.23|-2.70, \n",
      "val_vector  : -0.43|-1.34|-1.46|-2.54|-2.40|-1.20|-2.26|-2.50\n",
      "\n",
      "Training completed in 4186.085424900055s\n"
     ]
    }
   ],
   "source": [
    "train_dl,val_dl = get_data(data,batch_size)\n",
    "\n",
    "model = GNN_edgeUpdate(reuse,block,head,dim,layer1,layer2,factor,**data_dict[data]).to('cuda:0')\n",
    "\n",
    "paras = trainable_parameter(model)\n",
    "opt = Adam(paras,lr=lr)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5)\n",
    "\n",
    "model,train_loss_list,val_loss_list,bestWeight = train_type(opt,model,epochs,train_dl,val_dl,paras,clip,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(train_loss_list,val_loss_list,reuse,block,\\\n",
    "             head,data,batch_size,dim,clip,layer1,layer2,factor,epochs)\n",
    "save_model_type(bestWeight,opt,reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs,postStr='base'):\n",
    "    # set up\n",
    "    model = GNN_edgeUpdate(reuse,block,head,dim,layer1,layer2,factor,**data_dict[data]).to('cuda:0')\n",
    "    submission = pd.read_csv('../Data/sample_submission.csv')\n",
    "    \n",
    "    for i in range(8):\n",
    "        # load test data and type_id\n",
    "        with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(i)+'.pickle', 'rb') as handle:\n",
    "            test_data = pickle.load(handle)\n",
    "        test_list = [Data(**d) for d in test_data]\n",
    "        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n",
    "        with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(i)+'.pickle', 'rb') as handle:\n",
    "            test_id = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "        # load model\n",
    "        checkpoint = torch.load('../Model/{}.tar'.format('_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n",
    "                                            for i in [reuse,block,head,data,batch_size,dim,clip,\\\n",
    "                                                  layer1,layer2,factor,epochs,'type_'+str(i)+postStr]])))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    \n",
    "        # predict\n",
    "        model.eval()\n",
    "        yhat_list = []\n",
    "        with torch.no_grad():\n",
    "            for data_torch in test_dl:\n",
    "                data_torch = data_torch.to('cuda:0')\n",
    "                yhat_list.append(model(data_torch,False,True))\n",
    "        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "        \n",
    "        # join\n",
    "        submit_ = dict(zip(test_id,yhat))\n",
    "        submission['type_'+str(i)] = submission.id.map(submit_)\n",
    "    \n",
    "    # save types results    \n",
    "    submission.to_csv('../Submission/{}.csv'.format('_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n",
    "                                        for i in [reuse,block,head,data,batch_size,dim,clip,\\\n",
    "                                              layer1,layer2,factor,epochs,'all_types'+postStr]])),\\\n",
    "                      index=False)\n",
    "    \n",
    "    # save final results for submission\n",
    "    submission['scalar_coupling_constant'] = submission.iloc[:,2:].mean(1)\n",
    "    submission = submission[['id','scalar_coupling_constant']]\n",
    "    \n",
    "    submission.to_csv('../Submission/{}.csv'.format('_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n",
    "                                        for i in [reuse,block,head,data,batch_size,dim,clip,\\\n",
    "                                              layer1,layer2,factor,epochs,'final'+postStr]])),\\\n",
    "                      index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(reuse,block,head,data,batch_size,dim,clip,layer1,layer2,factor,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
