{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pickle\nimport torch\nfrom torch_geometric.data import Data,DataLoader\nfrom functions_refactor import *\nfrom pytorch_util import *\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fixed parameters\nhead_mol,head_atom,head_edge = head_mol,head_atom,head_edge2\nclip = 2\nbatch_size = 32\nthreshold = -1.3\nreuse = False\nlr = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# changing parameters\nblock = NNConv_block\nhead = SimplyInteraction_noEdge\ndata = '../Data/{}_data_ACSF_SOAP_atomInfo_otherInfo.pickle'\ndim = 512\nlogLoss = True\nweight = 0.6\nlayer1 = 3\nlayer2 = 3\nfactor = 2\nepochs = 120\naggr = 'max'\ninterleave = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"prefix = '_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n                                        for i in [block,head,data,dim,logLoss,weight,layer1,layer2,factor,epochs,aggr,interleave]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = pd.read_csv('../Data/train.csv')\ntest_df = pd.read_csv('../Data/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"folds = []\nfor f in range(5):\n    with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(f)+'.pickle', 'rb') as handle:\n        folds.append(pickle.load(handle))\nfolds = [[Data(**d) for d in fold] for fold in folds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"for i in range(5):\n    print('\\nstart fold '+str(i))\n    # parpare data\n    train_list = []\n    val_list = []\n    for j in range(5):\n        if i == j:\n            val_list.extend(folds[j])\n        else:\n            train_list.extend(folds[j])\n    \n    train_dl = DataLoader(train_list,batch_size,shuffle=True)\n    val_dl = DataLoader(val_list,batch_size,shuffle=False)\n    \n    # train model\n    model = GNN_multiHead_noEdge(reuse,block,head,head_mol,head_atom,head_edge,\\\n                          dim,layer1,layer2,factor,**data_dict[data],aggr=aggr,interleave=interleave).to('cuda:0')\n    paras = trainable_parameter(model)\n    opt = Adam(paras,lr=lr)\n    scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n    \n    model,train_loss_perType,val_loss_perType,bestWeight = train_type_earlyStop_5fold(opt,model,epochs_type,train_dl,val_dl,paras,clip,\\\n                                                                    scheduler=scheduler,logLoss=logLoss,weight=weight,patience=8)\n    torch.save({'model_state_dict_type_'+str(j_):w for j_,w in enumerate(bestWeight)},\\\n                '../Model/'+prefix+'_fold'+str(i)+'.tar')\n    # predict oof for each type\n    for type_i in range(8):\n        # load val data and type_id\n        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n            test_data = pickle.load(handle)\n        test_list = [Data(**d) for d in test_data]\n        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n        \n        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'_id.pickle', 'rb') as handle:\n            test_id = pickle.load(handle)\n    \n        # load model\n        model.load_state_dict(bestWeight[type_i])\n    \n        # predict\n        model.eval()\n        yhat_list = []\n        with torch.no_grad():\n            for data_torch in test_dl:\n                data_torch = data_torch.to('cuda:0')\n                yhat_list.append(model(data_torch,False,True))\n        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n    \n        # join\n        assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n        submit_ = dict(zip(test_id,yhat))\n        train_df['fold'+str(i)+'_type'+str(type_i)] = train_df.id.map(submit_)\n    \n    # predict test\n    for type_i in range(8):\n        # load val data and type_id\n        with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n            test_data = pickle.load(handle)\n        test_list = [Data(**d) for d in test_data]\n        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n        \n        with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(type_i)+'.pickle', 'rb') as handle:\n            test_id = pickle.load(handle)\n    \n        # load model\n        model.load_state_dict(bestWeight[type_i])\n    \n        # predict\n        model.eval()\n        yhat_list = []\n        with torch.no_grad():\n            for data_torch in test_dl:\n                data_torch = data_torch.to('cuda:0')\n                yhat_list.append(model(data_torch,False,True))\n        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n    \n        # join\n        assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n        submit_ = dict(zip(test_id,yhat))\n        test_df['fold'+str(i)+'_type'+str(type_i)] = test_df.id.map(submit_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#assert set(test.iloc[:,5:].isnull().sum(1)) == set([7*5])\ntest_df['yhat'] = np.nanmean(test_df.iloc[:,5:],1)\n#test = test[['id','yhat']]\ntest_df.to_csv('../Data/test_oof_'+prefix,index=False)\n\n#assert set(train.iloc[:,6:].isnull().sum(1)) == set([train.iloc[:,6:].shape[1]-1])\ntrain_df['yhat'] = np.nanmean(train_df.iloc[:,6:],1)\n#train = train[['id','yhat']]\ntrain_df.to_csv('../Data/train_oof_'+prefix,index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}