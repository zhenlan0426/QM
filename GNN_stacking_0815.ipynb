{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "from functions_refactor import *\n",
    "from pytorch_util import *\n",
    "#from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed parameters\n",
    "block = MEGNet_block\n",
    "head_mol,head_atom,head_edge = head_mol,head_atom,head_edge\n",
    "clip = 0.5\n",
    "batch_size = 64\n",
    "threshold = 1e3\n",
    "reuse = False\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# changing parameters\n",
    "head = SimplyInteraction\n",
    "data = '../Data/{}_data_stacking_0815.pickle'\n",
    "dim = 22\n",
    "logLoss = False\n",
    "layer = 6\n",
    "factor = 3\n",
    "epochs = 19\n",
    "edge_in4 = 22\n",
    "aggr = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,val_dl = get_data(data,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +1.327, val_loss: +0.097, \n",
      "train_vector: -0.79|-0.87|-2.05|-2.25|-2.32|-1.95|-2.19|-2.54, \n",
      "val_vector  : -1.55|-1.91|-2.43|-3.01|-2.94|-2.27|-2.91|-3.12\n",
      "\n",
      "epoch:1, train_loss: +0.099, val_loss: +0.091, \n",
      "train_vector: -1.48|-1.79|-2.47|-3.10|-2.97|-2.31|-2.97|-3.15, \n",
      "val_vector  : -1.57|-2.00|-2.49|-3.26|-3.00|-2.31|-3.05|-3.11\n",
      "\n",
      "epoch:2, train_loss: +0.095, val_loss: +0.096, \n",
      "train_vector: -1.49|-1.82|-2.52|-3.18|-3.00|-2.36|-3.07|-3.19, \n",
      "val_vector  : -1.56|-1.71|-2.53|-3.04|-3.04|-2.36|-3.04|-3.19\n",
      "\n",
      "epoch:3, train_loss: +0.094, val_loss: +0.094, \n",
      "train_vector: -1.49|-1.81|-2.54|-3.22|-3.01|-2.38|-3.11|-3.21, \n",
      "val_vector  : -1.42|-1.99|-2.55|-3.01|-3.04|-2.39|-3.16|-3.21\n",
      "\n",
      "epoch:4, train_loss: +0.091, val_loss: +0.092, \n",
      "train_vector: -1.51|-1.88|-2.56|-3.26|-3.02|-2.39|-3.14|-3.22, \n",
      "val_vector  : -1.58|-1.87|-2.49|-3.08|-3.03|-2.38|-3.13|-3.10\n",
      "\n",
      "epoch:5, train_loss: +0.093, val_loss: +0.099, \n",
      "train_vector: -1.48|-1.82|-2.56|-3.28|-3.03|-2.40|-3.15|-3.22, \n",
      "val_vector  : -1.59|-1.87|-2.48|-2.39|-2.98|-2.37|-3.04|-3.15\n",
      "\n",
      "epoch:6, train_loss: +0.092, val_loss: +0.089, \n",
      "train_vector: -1.50|-1.85|-2.57|-3.29|-3.03|-2.40|-3.16|-3.23, \n",
      "val_vector  : -1.54|-2.00|-2.55|-3.08|-3.07|-2.40|-3.11|-3.24\n",
      "\n",
      "epoch:7, train_loss: +0.090, val_loss: +0.106, \n",
      "train_vector: -1.52|-1.89|-2.57|-3.30|-3.05|-2.40|-3.16|-3.23, \n",
      "val_vector  : -1.55|-1.35|-2.56|-2.97|-2.98|-2.39|-3.21|-3.20\n",
      "\n",
      "epoch:8, train_loss: +0.090, val_loss: +0.093, \n",
      "train_vector: -1.52|-1.88|-2.58|-3.31|-3.04|-2.41|-3.17|-3.23, \n",
      "val_vector  : -1.37|-2.04|-2.58|-3.06|-3.05|-2.40|-3.19|-3.23\n",
      "\n",
      "epoch:9, train_loss: +0.089, val_loss: +0.109, \n",
      "train_vector: -1.51|-1.94|-2.58|-3.30|-3.04|-2.41|-3.17|-3.23, \n",
      "val_vector  : -1.22|-1.57|-2.56|-3.33|-3.01|-2.39|-3.09|-3.10\n",
      "\n",
      "epoch:10, train_loss: +0.090, val_loss: +0.089, \n",
      "train_vector: -1.50|-1.90|-2.58|-3.31|-3.05|-2.41|-3.17|-3.24, \n",
      "val_vector  : -1.47|-2.04|-2.57|-3.33|-3.07|-2.41|-3.18|-3.24\n",
      "\n",
      "epoch:11, train_loss: +0.089, val_loss: +0.085, \n",
      "train_vector: -1.50|-1.93|-2.58|-3.31|-3.05|-2.41|-3.18|-3.24, \n",
      "val_vector  : -1.60|-2.07|-2.58|-3.34|-3.05|-2.40|-3.21|-3.24\n",
      "\n",
      "epoch:12, train_loss: +0.090, val_loss: +0.094, \n",
      "train_vector: -1.49|-1.93|-2.58|-3.33|-3.05|-2.41|-3.18|-3.24, \n",
      "val_vector  : -1.44|-2.01|-2.53|-3.01|-3.06|-2.36|-3.00|-3.15\n",
      "\n",
      "epoch:13, train_loss: +0.090, val_loss: +0.096, \n",
      "train_vector: -1.49|-1.92|-2.59|-3.33|-3.06|-2.41|-3.18|-3.24, \n",
      "val_vector  : -1.34|-2.05|-2.54|-2.96|-3.02|-2.39|-3.12|-3.17\n",
      "\n",
      "epoch:14, train_loss: +0.089, val_loss: +0.099, \n",
      "train_vector: -1.50|-1.93|-2.58|-3.34|-3.06|-2.41|-3.18|-3.24, \n",
      "val_vector  : -1.62|-1.48|-2.57|-3.24|-3.06|-2.40|-3.04|-3.17\n",
      "\n",
      "epoch:15, train_loss: +0.089, val_loss: +0.091, \n",
      "train_vector: -1.50|-1.95|-2.59|-3.33|-3.06|-2.41|-3.19|-3.24, \n",
      "val_vector  : -1.47|-1.95|-2.57|-3.23|-3.06|-2.41|-3.20|-3.23\n",
      "\n",
      "epoch:16, train_loss: +0.089, val_loss: +0.086, \n",
      "train_vector: -1.45|-1.98|-2.59|-3.35|-3.07|-2.41|-3.19|-3.25, \n",
      "val_vector  : -1.55|-2.12|-2.54|-3.26|-3.08|-2.41|-3.18|-3.22\n",
      "\n",
      "epoch:17, train_loss: +0.088, val_loss: +0.087, \n",
      "train_vector: -1.51|-1.96|-2.59|-3.35|-3.07|-2.41|-3.19|-3.24, \n",
      "val_vector  : -1.62|-2.05|-2.57|-3.08|-3.01|-2.41|-3.16|-3.17\n",
      "\n",
      "epoch:18, train_loss: +0.085, val_loss: +0.084, \n",
      "train_vector: -1.56|-2.03|-2.59|-3.38|-3.08|-2.42|-3.20|-3.25, \n",
      "val_vector  : -1.57|-2.11|-2.58|-3.37|-3.08|-2.41|-3.21|-3.26\n",
      "\n",
      "epoch:19, train_loss: +0.085, val_loss: +0.088, \n",
      "train_vector: -1.56|-2.05|-2.59|-3.38|-3.08|-2.42|-3.20|-3.25, \n",
      "val_vector  : -1.47|-2.06|-2.57|-3.39|-3.08|-2.41|-3.15|-3.22\n",
      "\n",
      "-----stop due to poor performance-----\n"
     ]
    }
   ],
   "source": [
    "    model = GNN_multiHead_interleave_stacking(reuse,block,head,dim,layer,factor,edge_in4).to('cuda:0')\n",
    "    paras = trainable_parameter(model)\n",
    "    opt = Adam(paras,lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n",
    "    \n",
    "    model,train_loss_perType,val_loss_perType,bestWeight = train_type_earlyStop(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                                                    scheduler=scheduler,logLoss=logLoss,threshold=threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +3.744, val_loss: +0.105, \n",
      "train_vector: +0.33|-0.10|-0.71|-1.61|-1.42|-1.04|-0.70|-1.72, \n",
      "val_vector  : -1.46|-1.83|-2.30|-3.05|-2.85|-2.20|-2.82|-2.99\n",
      "\n",
      "epoch:1, train_loss: +0.100, val_loss: +0.099, \n",
      "train_vector: -1.47|-1.89|-2.37|-3.08|-2.95|-2.24|-2.91|-3.05, \n",
      "val_vector  : -1.40|-1.86|-2.46|-3.28|-2.99|-2.31|-3.01|-3.15\n",
      "\n",
      "epoch:2, train_loss: +0.095, val_loss: +0.095, \n",
      "train_vector: -1.48|-1.90|-2.46|-3.23|-3.00|-2.32|-3.00|-3.13, \n",
      "val_vector  : -1.52|-1.91|-2.48|-3.06|-3.01|-2.37|-3.00|-3.15\n",
      "\n",
      "epoch:3, train_loss: +0.092, val_loss: +0.091, \n",
      "train_vector: -1.50|-1.93|-2.50|-3.28|-3.02|-2.36|-3.05|-3.17, \n",
      "val_vector  : -1.51|-2.09|-2.45|-3.11|-2.95|-2.40|-3.07|-3.22\n",
      "\n",
      "epoch:4, train_loss: +0.090, val_loss: +0.096, \n",
      "train_vector: -1.51|-1.96|-2.52|-3.28|-3.03|-2.38|-3.09|-3.18, \n",
      "val_vector  : -1.35|-1.94|-2.55|-3.22|-3.00|-2.40|-3.16|-3.21\n",
      "\n",
      "epoch:5, train_loss: +0.091, val_loss: +0.089, \n",
      "train_vector: -1.48|-1.96|-2.54|-3.30|-3.04|-2.39|-3.11|-3.20, \n",
      "val_vector  : -1.51|-2.07|-2.55|-3.32|-2.94|-2.42|-3.09|-3.19\n",
      "\n",
      "epoch:6, train_loss: +0.090, val_loss: +0.089, \n",
      "train_vector: -1.48|-1.97|-2.55|-3.32|-3.05|-2.40|-3.13|-3.20, \n",
      "val_vector  : -1.53|-1.97|-2.56|-3.40|-2.99|-2.41|-3.14|-3.24\n",
      "\n",
      "epoch:7, train_loss: +0.089, val_loss: +0.097, \n",
      "train_vector: -1.51|-1.98|-2.56|-3.33|-3.06|-2.40|-3.14|-3.21, \n",
      "val_vector  : -1.38|-1.96|-2.46|-3.38|-2.83|-2.41|-2.91|-3.21\n",
      "\n",
      "epoch:8, train_loss: +0.089, val_loss: +0.093, \n",
      "train_vector: -1.50|-1.99|-2.56|-3.33|-3.07|-2.41|-3.14|-3.22, \n",
      "val_vector  : -1.30|-2.12|-2.57|-3.38|-3.05|-2.42|-3.20|-3.26\n",
      "\n",
      "epoch:9, train_loss: +0.088, val_loss: +0.085, \n",
      "train_vector: -1.49|-2.01|-2.57|-3.34|-3.06|-2.41|-3.16|-3.22, \n",
      "val_vector  : -1.60|-2.12|-2.57|-3.34|-3.06|-2.41|-3.14|-3.07\n",
      "\n",
      "epoch:10, train_loss: +0.088, val_loss: +0.089, \n",
      "train_vector: -1.52|-1.99|-2.57|-3.34|-3.07|-2.41|-3.15|-3.23, \n",
      "val_vector  : -1.47|-2.12|-2.55|-3.25|-2.97|-2.42|-3.09|-3.22\n",
      "\n",
      "epoch:11, train_loss: +0.088, val_loss: +0.108, \n",
      "train_vector: -1.51|-2.01|-2.57|-3.35|-3.07|-2.41|-3.16|-3.22, \n",
      "val_vector  : -1.15|-1.74|-2.42|-3.35|-2.90|-2.41|-3.04|-3.25\n",
      "\n",
      "epoch:12, train_loss: +0.087, val_loss: +0.087, \n",
      "train_vector: -1.52|-2.01|-2.58|-3.35|-3.07|-2.41|-3.17|-3.23, \n",
      "val_vector  : -1.48|-2.06|-2.60|-3.41|-3.02|-2.43|-3.22|-3.26\n",
      "\n",
      "epoch:13, train_loss: +0.086, val_loss: +0.089, \n",
      "train_vector: -1.54|-2.02|-2.58|-3.35|-3.08|-2.41|-3.17|-3.23, \n",
      "val_vector  : -1.59|-1.95|-2.59|-3.25|-3.04|-2.42|-3.20|-2.98\n",
      "\n",
      "epoch:14, train_loss: +0.087, val_loss: +0.092, \n",
      "train_vector: -1.51|-2.01|-2.58|-3.36|-3.07|-2.41|-3.17|-3.23, \n",
      "val_vector  : -1.43|-1.97|-2.57|-3.10|-3.09|-2.43|-3.21|-3.19\n",
      "\n",
      "epoch:15, train_loss: +0.088, val_loss: +0.086, \n",
      "train_vector: -1.50|-2.00|-2.58|-3.35|-3.08|-2.41|-3.17|-3.24, \n",
      "val_vector  : -1.51|-2.11|-2.60|-3.39|-3.08|-2.43|-3.08|-3.26\n",
      "\n",
      "epoch:16, train_loss: +0.085, val_loss: +0.085, \n",
      "train_vector: -1.56|-2.05|-2.59|-3.39|-3.09|-2.42|-3.19|-3.25, \n",
      "val_vector  : -1.54|-2.13|-2.57|-3.45|-3.06|-2.43|-3.18|-3.27\n",
      "\n",
      "epoch:17, train_loss: +0.084, val_loss: +0.094, \n",
      "train_vector: -1.57|-2.05|-2.59|-3.39|-3.09|-2.42|-3.19|-3.25, \n",
      "val_vector  : -1.61|-2.06|-2.29|-2.79|-2.97|-2.37|-2.98|-3.18\n",
      "\n",
      "epoch:18, train_loss: +0.084, val_loss: +0.093, \n",
      "train_vector: -1.56|-2.06|-2.59|-3.39|-3.09|-2.42|-3.18|-3.24, \n",
      "val_vector  : -1.32|-2.08|-2.55|-3.43|-3.06|-2.43|-3.16|-3.21\n",
      "\n",
      "Training completed in 670.399570941925s\n"
     ]
    }
   ],
   "source": [
    "# layer = 4\n",
    "    model = GNN_multiHead_interleave_stacking(reuse,block,head,dim,layer,factor,edge_in4).to('cuda:0')\n",
    "    paras = trainable_parameter(model)\n",
    "    opt = RAdam(paras,lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n",
    "    \n",
    "    model,train_loss_perType,val_loss_perType,bestWeight = train_type_earlyStop(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                                                    scheduler=scheduler,logLoss=logLoss,threshold=threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'type_attr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0f956b6b3a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model,train_loss_perType,val_loss_perType,bestWeight = train_type_earlyStop(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n\u001b[0;32m----> 8\u001b[0;31m                                                                 scheduler=scheduler,logLoss=logLoss,threshold=threshold)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/kaggle/QM/Code/functions_refactor.py\u001b[0m in \u001b[0;36mtrain_type_earlyStop\u001b[0;34m(opt, model, epochs, train_dl, val_dl, paras, clip, typeTrain, train_loss_list, val_loss_list, scheduler, logLoss, weight, threshold)\u001b[0m\n\u001b[1;32m   2250\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_perType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtypeTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 \u001b[0mval_loss_perType\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_perType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/QM/Code/functions_refactor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, IsTrain, logLoss, typeTrain, weight)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtypeTrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mIsTrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m             \u001b[0medge_attr3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_attr3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0medge_index3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'type_attr'"
     ]
    }
   ],
   "source": [
    "# layer = 6\n",
    "model = GNN_multiHead_interleave_stacking(reuse,block,head,dim,layer,factor,edge_in4).to('cuda:0')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n",
    "\n",
    "model,train_loss_perType,val_loss_perType,bestWeight = train_type_earlyStop(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                                                scheduler=scheduler,logLoss=logLoss,threshold=threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Model/GNN_stacking_0815.pickle', 'wb') as handle:\n",
    "    pickle.dump(bestWeight, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Model/GNN_stacking_0815.pickle', 'rb') as handle:\n",
    "    bestWeight = pickle.load(handle)\n",
    "\n",
    "model = GNN_multiHead_interleave_stacking(reuse,block,head,dim,layer,factor,edge_in4).to('cuda:0')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test\n",
    "for type_i in range(8):\n",
    "    # load val data and type_id\n",
    "    with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "        test_data = pickle.load(handle)\n",
    "    test_list = [Data(**d) for d in test_data]\n",
    "    test_dl = DataLoader(test_list,batch_size,shuffle=False)\n",
    "\n",
    "    with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "        test_id = pickle.load(handle)\n",
    "\n",
    "    # load model\n",
    "    model.load_state_dict(bestWeight[type_i])\n",
    "\n",
    "    # predict\n",
    "    model.eval()\n",
    "    yhat_list = []\n",
    "    with torch.no_grad():\n",
    "        for data_torch in test_dl:\n",
    "            data_torch = data_torch.to('cuda:0')\n",
    "            yhat_list.append(model(data_torch,False,typeTrain=True))\n",
    "    yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "\n",
    "    # join\n",
    "    assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n",
    "    submit_ = dict(zip(test_id,yhat))\n",
    "    test_df['type'+str(type_i)] = test_df.id.map(submit_)\n",
    "    #test_df['fold'+str(i)+'_type'+str(type_i)] = test_df.id.map(submit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type0</th>\n",
       "      <th>type1</th>\n",
       "      <th>type2</th>\n",
       "      <th>type3</th>\n",
       "      <th>type4</th>\n",
       "      <th>type5</th>\n",
       "      <th>type6</th>\n",
       "      <th>type7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.856739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181.097107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.559926</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181.094376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.856319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90.784004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.330306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.494313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.638249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90.790771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.335852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.641841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>82.853027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.335028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.331028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>90.785568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.494311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.638478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.335409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>90.789902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.641539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.335388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>82.852303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>106.220139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.132621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.134823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.766340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.927660</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.595562</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.591513</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505512</th>\n",
       "      <td>119.862213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505513</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.968998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505514</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.916530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505515</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.952584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505516</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.925524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505517</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.162330</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505518</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.439510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505519</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.474028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505520</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.763641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505521</th>\n",
       "      <td>127.367676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505522</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.383390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505523</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.512477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505524</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.749565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505525</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.156626</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505526</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.587318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505527</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.804598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505528</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.032138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505529</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.302214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505530</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.174740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505531</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.406243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505532</th>\n",
       "      <td>105.917137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505533</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.540180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505534</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716071</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505535</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.015088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505536</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.963654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505537</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.919523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505538</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.959100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505539</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.919174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505540</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.200829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505541</th>\n",
       "      <td>119.884903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2505542 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              type0  type1      type2     type3     type4      type5  \\\n",
       "0               NaN    NaN  18.856739       NaN       NaN        NaN   \n",
       "1        181.097107    NaN        NaN       NaN       NaN        NaN   \n",
       "2               NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "3        181.094376    NaN        NaN       NaN       NaN        NaN   \n",
       "4               NaN    NaN  18.856319       NaN       NaN        NaN   \n",
       "5         90.784004    NaN        NaN       NaN       NaN        NaN   \n",
       "6               NaN    NaN        NaN       NaN       NaN   2.330306   \n",
       "7               NaN    NaN        NaN -7.494313       NaN        NaN   \n",
       "8               NaN    NaN        NaN -9.638249       NaN        NaN   \n",
       "9         90.790771    NaN        NaN       NaN       NaN        NaN   \n",
       "10              NaN    NaN        NaN       NaN       NaN   2.335852   \n",
       "11              NaN    NaN        NaN -9.641841       NaN        NaN   \n",
       "12        82.853027    NaN        NaN       NaN       NaN        NaN   \n",
       "13              NaN    NaN        NaN       NaN       NaN  11.335028   \n",
       "14              NaN    NaN        NaN       NaN       NaN   2.331028   \n",
       "15        90.785568    NaN        NaN       NaN       NaN        NaN   \n",
       "16              NaN    NaN        NaN -7.494311       NaN        NaN   \n",
       "17              NaN    NaN        NaN -9.638478       NaN        NaN   \n",
       "18              NaN    NaN        NaN       NaN       NaN   2.335409   \n",
       "19        90.789902    NaN        NaN       NaN       NaN        NaN   \n",
       "20              NaN    NaN        NaN -9.641539       NaN        NaN   \n",
       "21              NaN    NaN        NaN       NaN       NaN  11.335388   \n",
       "22        82.852303    NaN        NaN       NaN       NaN        NaN   \n",
       "23       106.220139    NaN        NaN       NaN       NaN        NaN   \n",
       "24              NaN    NaN  -1.132621       NaN       NaN        NaN   \n",
       "25              NaN    NaN  -1.134823       NaN       NaN        NaN   \n",
       "26              NaN    NaN        NaN -3.766340       NaN        NaN   \n",
       "27              NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "28              NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "29              NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "...             ...    ...        ...       ...       ...        ...   \n",
       "2505512  119.862213    NaN        NaN       NaN       NaN        NaN   \n",
       "2505513         NaN    NaN   3.968998       NaN       NaN        NaN   \n",
       "2505514         NaN    NaN        NaN       NaN       NaN   1.916530   \n",
       "2505515         NaN    NaN        NaN       NaN       NaN   2.952584   \n",
       "2505516         NaN    NaN   0.925524       NaN       NaN        NaN   \n",
       "2505517         NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "2505518         NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "2505519         NaN    NaN        NaN       NaN       NaN   1.474028   \n",
       "2505520         NaN    NaN   3.763641       NaN       NaN        NaN   \n",
       "2505521  127.367676    NaN        NaN       NaN       NaN        NaN   \n",
       "2505522         NaN    NaN        NaN       NaN       NaN   9.383390   \n",
       "2505523         NaN    NaN        NaN       NaN       NaN   1.512477   \n",
       "2505524         NaN    NaN   3.749565       NaN       NaN        NaN   \n",
       "2505525         NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "2505526         NaN    NaN        NaN       NaN       NaN   4.587318   \n",
       "2505527         NaN    NaN        NaN       NaN  3.804598        NaN   \n",
       "2505528         NaN    NaN        NaN       NaN       NaN   9.032138   \n",
       "2505529         NaN    NaN        NaN       NaN       NaN   3.302214   \n",
       "2505530         NaN    NaN        NaN       NaN       NaN  -0.174740   \n",
       "2505531         NaN    NaN  -0.406243       NaN       NaN        NaN   \n",
       "2505532  105.917137    NaN        NaN       NaN       NaN        NaN   \n",
       "2505533         NaN    NaN   9.540180       NaN       NaN        NaN   \n",
       "2505534         NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "2505535         NaN    NaN        NaN       NaN       NaN        NaN   \n",
       "2505536         NaN    NaN        NaN       NaN       NaN   2.963654   \n",
       "2505537         NaN    NaN   0.919523       NaN       NaN        NaN   \n",
       "2505538         NaN    NaN   3.959100       NaN       NaN        NaN   \n",
       "2505539         NaN    NaN        NaN       NaN       NaN   1.919174   \n",
       "2505540         NaN    NaN   4.200829       NaN       NaN        NaN   \n",
       "2505541  119.884903    NaN        NaN       NaN       NaN        NaN   \n",
       "\n",
       "            type6     type7  \n",
       "0             NaN       NaN  \n",
       "1             NaN       NaN  \n",
       "2        3.559926       NaN  \n",
       "3             NaN       NaN  \n",
       "4             NaN       NaN  \n",
       "5             NaN       NaN  \n",
       "6             NaN       NaN  \n",
       "7             NaN       NaN  \n",
       "8             NaN       NaN  \n",
       "9             NaN       NaN  \n",
       "10            NaN       NaN  \n",
       "11            NaN       NaN  \n",
       "12            NaN       NaN  \n",
       "13            NaN       NaN  \n",
       "14            NaN       NaN  \n",
       "15            NaN       NaN  \n",
       "16            NaN       NaN  \n",
       "17            NaN       NaN  \n",
       "18            NaN       NaN  \n",
       "19            NaN       NaN  \n",
       "20            NaN       NaN  \n",
       "21            NaN       NaN  \n",
       "22            NaN       NaN  \n",
       "23            NaN       NaN  \n",
       "24            NaN       NaN  \n",
       "25            NaN       NaN  \n",
       "26            NaN       NaN  \n",
       "27       3.927660       NaN  \n",
       "28       7.595562       NaN  \n",
       "29       7.591513       NaN  \n",
       "...           ...       ...  \n",
       "2505512       NaN       NaN  \n",
       "2505513       NaN       NaN  \n",
       "2505514       NaN       NaN  \n",
       "2505515       NaN       NaN  \n",
       "2505516       NaN       NaN  \n",
       "2505517  2.162330       NaN  \n",
       "2505518  4.439510       NaN  \n",
       "2505519       NaN       NaN  \n",
       "2505520       NaN       NaN  \n",
       "2505521       NaN       NaN  \n",
       "2505522       NaN       NaN  \n",
       "2505523       NaN       NaN  \n",
       "2505524       NaN       NaN  \n",
       "2505525  2.156626       NaN  \n",
       "2505526       NaN       NaN  \n",
       "2505527       NaN       NaN  \n",
       "2505528       NaN       NaN  \n",
       "2505529       NaN       NaN  \n",
       "2505530       NaN       NaN  \n",
       "2505531       NaN       NaN  \n",
       "2505532       NaN       NaN  \n",
       "2505533       NaN       NaN  \n",
       "2505534  0.716071       NaN  \n",
       "2505535       NaN -0.015088  \n",
       "2505536       NaN       NaN  \n",
       "2505537       NaN       NaN  \n",
       "2505538       NaN       NaN  \n",
       "2505539       NaN       NaN  \n",
       "2505540       NaN       NaN  \n",
       "2505541       NaN       NaN  \n",
       "\n",
       "[2505542 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['scalar_coupling_constant'] = np.nanmean(test_df.iloc[:,5:],1)\n",
    "#test = test[['id','yhat']]\n",
    "test_df[['id','scalar_coupling_constant']].to_csv('../Submission/GNN_stacking_0815',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
