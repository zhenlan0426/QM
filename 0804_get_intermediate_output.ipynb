{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "from functions_refactor import *\n",
    "from pytorch_util import *\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed parameters\n",
    "block = MEGNet_block\n",
    "head_mol,head_atom,head_edge = head_mol,head_atom,head_edge\n",
    "clip = 2\n",
    "batch_size = 32\n",
    "threshold = -1.2\n",
    "reuse = False\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# changing parameters\n",
    "head = SimplyInteraction\n",
    "data = '../Data/{}_data_ACSF_SOAP_atomInfo_otherInfo.pickle'\n",
    "dim = 512\n",
    "logLoss = True\n",
    "weight = 0.6\n",
    "layer1 = 4\n",
    "layer2 = 3\n",
    "factor = 2\n",
    "epochs = 100\n",
    "aggr = 'max'\n",
    "interleave = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n",
    "                                        for i in [head,data,dim,logLoss,weight,layer1,layer2,factor,epochs,aggr,interleave]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Data/train.csv')\n",
    "test_df = pd.read_csv('../Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "for f in range(5):\n",
    "    with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(f)+'.pickle', 'rb') as handle:\n",
    "        folds.append(pickle.load(handle))\n",
    "folds = [[Data(**d) for d in fold] for fold in folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediate_output(model,dl) :\n",
    "    # for GNN_multiHead_interleave\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dl:\n",
    "            data = data.to('cuda:0')\n",
    "            out = model.lin_node(data.x)\n",
    "            edge_attr = model.edge1(data.edge_attr)\n",
    "            out,_ = model.conv1[0](out,data.edge_index,edge_attr)\n",
    "            temp = out[data.edge_index3[:,data.type_attr]] # (2,N,d)\n",
    "            out = torch.cat([temp[0],temp[1]],1)\n",
    "            outputs.append(out.cpu().detach().numpy())\n",
    "    return np.concatenate(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oof_features = []\n",
    "for i in range(5):\n",
    "    print('\\nstart fold '+str(i))\n",
    "    # parpare data\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    for j in range(5):\n",
    "        if i == j:\n",
    "            val_list.extend(folds[j])\n",
    "        else:\n",
    "            train_list.extend(folds[j])\n",
    "    \n",
    "    train_dl = DataLoader(train_list,batch_size,shuffle=True)\n",
    "    val_dl = DataLoader(val_list,batch_size,shuffle=False)\n",
    "    \n",
    "    # train model\n",
    "    model = GNN_multiHead_interleave(reuse,block,head,head_mol,head_atom,head_edge,\\\n",
    "                          dim,layer1,layer2,factor,**data_dict[data],aggr=aggr,interleave=interleave).to('cuda:0')\n",
    "    \n",
    "    checkpoint = torch.load('../Model/'+prefix+'_fold'+str(i)+'.tar')\n",
    "\n",
    "    # predict oof for each type\n",
    "    for type_i in range(8):\n",
    "        \n",
    "        # load val data and type_id\n",
    "        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "            test_data = pickle.load(handle)\n",
    "        test_list = [Data(**d) for d in test_data]\n",
    "        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n",
    "        \n",
    "        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'_id.pickle', 'rb') as handle:\n",
    "            test_id = pickle.load(handle)\n",
    "    \n",
    "        # load model\n",
    "        model.load_state_dict(checkpoint['model_state_dict_type_'+str(type_i)])\n",
    "    \n",
    "        # predict\n",
    "        yhat = get_intermediate_output(model,test_dl)    \n",
    "    \n",
    "        # join\n",
    "        assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n",
    "        oof_features.append(pd.DataFrame(np.concatenate([test_id.reshape(-1,1),yhat],1),columns=['id']+['feature_'+str(feature_i) for feature_i in range(yhat.shape[1])]))\n",
    "    \n",
    "    test_features = []\n",
    "    # predict test\n",
    "    for type_i in range(8):\n",
    "        # load val data and type_id\n",
    "        with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "            test_data = pickle.load(handle)\n",
    "        test_list = [Data(**d) for d in test_data]\n",
    "        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n",
    "        \n",
    "        with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "            test_id = pickle.load(handle)\n",
    "    \n",
    "        # load model\n",
    "        model.load_state_dict(checkpoint['model_state_dict_type_'+str(type_i)])\n",
    "    \n",
    "        # predict\n",
    "        yhat = get_intermediate_output(model,test_dl)          \n",
    "    \n",
    "        # join\n",
    "        assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n",
    "        test_features.append(pd.DataFrame(np.concatenate([test_id.reshape(-1,1),yhat],1),columns=['id']+['feature_'+str(feature_i) for feature_i in range(yhat.shape[1])]))\n",
    "    \n",
    "    if i == 0:\n",
    "        test_oof_df = pd.concat(test_features, ignore_index=True)\n",
    "    else:\n",
    "        temp_df = pd.concat(test_features, ignore_index=True)\n",
    "        test_oof_df.loc[:,['feature_'+str(feature_i) for feature_i in range(yhat.shape[1])]] = test_oof_df.loc[:,['feature_'+str(feature_i) for feature_i in range(yhat.shape[1])]].values \\\n",
    "                                                                                            + temp_df.loc[:,['feature_'+str(feature_i) for feature_i in range(yhat.shape[1])]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df = pd.concat(oof_features, ignore_index=True)\n",
    "n_ = train_df.shape[0]\n",
    "train_df = pd.merge(train_df,oof_df,how='inner',on='id')\n",
    "m_ = train_df.shape[0]\n",
    "assert n_==m_,\"oof id should match train id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_oof_df.loc[:,['feature_'+str(feature_i) for feature_i in range(yhat.shape[1])]] = test_oof_df.loc[:,['feature_'+str(feature_i) for feature_i in range(yhat.shape[1])]]/5\n",
    "n_ = test_df.shape[0]\n",
    "test_df = pd.merge(test_df,test_oof_df,how='inner',on='id')\n",
    "m_ = test_df.shape[0]\n",
    "assert n_==m_,\"oof id should match train id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../Data/test_oof_features_'+prefix,index=False)\n",
    "train_df.to_csv('../Data/train_oof_features_'+prefix,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
