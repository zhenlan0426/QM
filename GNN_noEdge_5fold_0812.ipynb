{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"GNN_noEdge_5fold_0812.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"4p0m7GVfGi_D","colab_type":"code","colab":{}},"source":["import pickle\n","import torch\n","from torch_geometric.data import Data,DataLoader\n","from functions_refactor import *\n","from pytorch_util import *\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rWJawH60Gi_G","colab_type":"code","colab":{}},"source":["# fixed parameters\n","head_mol,head_atom,head_edge = head_mol,head_atom,head_edge2\n","clip = 2\n","batch_size = 32\n","threshold = -1.3\n","reuse = False\n","lr = 1e-4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NhCd9l6AGi_H","colab_type":"code","colab":{}},"source":["# changing parameters\n","block = schnet_block_Dense\n","head = cat3HeadInteraction_noEdge\n","data = '../Data/{}_data_ACSF_SOAP_atomInfo_otherInfo.pickle'\n","dim = 32\n","logLoss = False\n","weight = 0.3\n","factor = 2\n","epochs = 150\n","aggr = 'max'\n","interleave = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFP0GKKzGi_J","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkMgBU7EGi_K","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qfA2JKZ5Gi_L","colab_type":"code","colab":{}},"source":["prefix = '_'.join([str(i).split('}')[1] if '}' in str(i) else str(i) \\\n","                                        for i in [block,head,data,dim,logLoss,weight,factor,epochs,aggr,interleave]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EB6heqTjGi_N","colab_type":"code","colab":{}},"source":["train_df = pd.read_csv('../Data/train.csv')\n","test_df = pd.read_csv('../Data/test.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFEYoLRVGi_O","colab_type":"code","colab":{}},"source":["folds = []\n","for f in range(5):\n","    with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(f)+'.pickle', 'rb') as handle:\n","        folds.append(pickle.load(handle))\n","folds = [[Data(**d) for d in fold] for fold in folds]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pC5wrUf1Gi_P","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"TDpyKJ0cGi_R","colab_type":"code","colab":{}},"source":["for i in range(5):\n","    print('\\nstart fold '+str(i))\n","    # parpare data\n","    train_list = []\n","    val_list = []\n","    for j in range(5):\n","        if i == j:\n","            val_list.extend(folds[j])\n","        else:\n","            train_list.extend(folds[j])\n","    \n","    train_dl = DataLoader(train_list,batch_size,shuffle=True)\n","    val_dl = DataLoader(val_list,batch_size,shuffle=False)\n","    \n","    # train model\n","    model = GNN_multiHead_noEdge_Dense(block,head,head_mol,head_atom,head_edge,\\\n","                          dim,factor,**data_dict[data],aggr=aggr,interleave=interleave).to('cuda:0')                          \n","    paras = trainable_parameter(model)\n","    opt = Adam(paras,lr=lr)\n","    scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n","    model,train_loss_perType,val_loss_perType,bestWeight = train_type_earlyStop(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n","                                                                    scheduler=scheduler,logLoss=logLoss,weight=weight,threshold=threshold)\n","    \n","    torch.save({'model_state_dict_type_'+str(j_):w for j_,w in enumerate(bestWeight)},\\\n","                '../Model/'+prefix+'_fold'+str(i)+'.tar')\n","    # predict oof for each type\n","    for type_i in range(8):\n","        # load val data and type_id\n","        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n","            test_data = pickle.load(handle)\n","        test_list = [Data(**d) for d in test_data]\n","        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n","        \n","        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'_id.pickle', 'rb') as handle:\n","            test_id = pickle.load(handle)\n","    \n","        # load model\n","        model.load_state_dict(bestWeight[type_i])\n","    \n","        # predict\n","        model.eval()\n","        yhat_list = []\n","        with torch.no_grad():\n","            for data_torch in test_dl:\n","                data_torch = data_torch.to('cuda:0')\n","                yhat_list.append(model(data_torch,False,True))\n","        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n","    \n","        # join\n","        assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n","        submit_ = dict(zip(test_id,yhat))\n","        train_df['fold'+str(i)+'_type'+str(type_i)] = train_df.id.map(submit_)\n","    \n","    # predict test\n","    for type_i in range(8):\n","        # load val data and type_id\n","        with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n","            test_data = pickle.load(handle)\n","        test_list = [Data(**d) for d in test_data]\n","        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n","        \n","        with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(type_i)+'.pickle', 'rb') as handle:\n","            test_id = pickle.load(handle)\n","    \n","        # load model\n","        model.load_state_dict(bestWeight[type_i])\n","    \n","        # predict\n","        model.eval()\n","        yhat_list = []\n","        with torch.no_grad():\n","            for data_torch in test_dl:\n","                data_torch = data_torch.to('cuda:0')\n","                yhat_list.append(model(data_torch,False,True))\n","        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n","    \n","        # join\n","        assert yhat.shape[0]==test_id.shape[0],'yhat and test_id should have same shape'\n","        submit_ = dict(zip(test_id,yhat))\n","        test_df['fold'+str(i)+'_type'+str(type_i)] = test_df.id.map(submit_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EC9W0dOhGi_T","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPYvCbjAGi_U","colab_type":"code","colab":{}},"source":["#assert set(test.iloc[:,5:].isnull().sum(1)) == set([7*5])\n","test_df['yhat'] = np.nanmean(test_df.iloc[:,5:],1)\n","#test = test[['id','yhat']]\n","test_df.to_csv('../Data/test_oof_'+prefix,index=False)\n","\n","#assert set(train.iloc[:,6:].isnull().sum(1)) == set([train.iloc[:,6:].shape[1]-1])\n","train_df['yhat'] = np.nanmean(train_df.iloc[:,6:],1)\n","#train = train[['id','yhat']]\n","train_df.to_csv('../Data/train_oof_'+prefix,index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUMg6lr9Gi_W","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ukJMWq8UGi_X","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}