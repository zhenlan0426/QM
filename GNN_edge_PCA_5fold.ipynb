{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data,DataLoader\n",
    "from functions_refactor import *\n",
    "from pytorch_util import *\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "reuse = False\n",
    "block = MEGNet_block\n",
    "head = feedforwardHead_Update\n",
    "data = '../Data/{}_data_ACSF_expand_PCA.pickle'\n",
    "batch_size = 32\n",
    "dim = 128\n",
    "epochs = 50\n",
    "clip = 0.4\n",
    "layer1 = 3\n",
    "layer2 = 3\n",
    "factor = 2\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '_MEGNet_5folds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/train.csv')\n",
    "test = pd.read_csv('../Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "for f in range(5):\n",
    "    with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(f)+'.pickle', 'rb') as handle:\n",
    "        folds.append(pickle.load(handle))\n",
    "folds = [[Data(**d) for d in fold] for fold in folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start fold 0\n",
      "epoch:0, train_loss: +0.358, val_loss: -0.051, \n",
      "train_vector: +4.22|+0.96|+0.12|-0.42|-0.57|+0.11|-0.61|-0.93, \n",
      "val_vector  : +3.92|+0.45|-0.28|-1.01|-1.10|-0.20|-1.03|-1.16\n",
      "\n",
      "epoch:1, train_loss: -0.298, val_loss: -0.715, \n",
      "train_vector: +2.25|+0.34|-0.33|-0.96|-1.07|-0.25|-1.05|-1.31, \n",
      "val_vector  : +0.77|-0.22|-0.52|-1.43|-1.23|-0.37|-1.27|-1.45\n",
      "\n",
      "epoch:2, train_loss: -0.629, val_loss: -0.569, \n",
      "train_vector: +0.89|+0.12|-0.51|-1.18|-1.27|-0.38|-1.21|-1.50, \n",
      "val_vector  : +0.60|-0.13|-0.59|-0.82|-0.84|-0.44|-0.88|-1.44\n",
      "\n",
      "epoch:3, train_loss: -0.728, val_loss: -0.853, \n",
      "train_vector: +0.81|-0.04|-0.60|-1.31|-1.33|-0.46|-1.30|-1.60, \n",
      "val_vector  : +0.56|-0.38|-0.68|-1.30|-1.54|-0.54|-1.29|-1.66\n",
      "\n",
      "epoch:4, train_loss: -0.825, val_loss: -0.949, \n",
      "train_vector: +0.77|-0.15|-0.69|-1.42|-1.49|-0.53|-1.38|-1.70, \n",
      "val_vector  : +0.17|+0.01|-0.75|-1.60|-1.62|-0.59|-1.44|-1.77\n",
      "\n",
      "epoch:5, train_loss: -0.893, val_loss: -0.926, \n",
      "train_vector: +0.71|-0.23|-0.76|-1.50|-1.56|-0.59|-1.44|-1.77, \n",
      "val_vector  : +0.33|+0.10|-0.84|-1.56|-1.60|-0.65|-1.47|-1.73\n",
      "\n",
      "epoch:6, train_loss: -0.943, val_loss: -0.980, \n",
      "train_vector: +0.68|-0.28|-0.81|-1.57|-1.61|-0.63|-1.49|-1.83, \n",
      "val_vector  : +0.37|+0.02|-0.92|-1.40|-1.76|-0.72|-1.59|-1.84\n",
      "\n",
      "epoch:7, train_loss: -0.986, val_loss: -0.929, \n",
      "train_vector: +0.63|-0.32|-0.86|-1.58|-1.68|-0.67|-1.53|-1.88, \n",
      "val_vector  : +0.67|-0.10|-0.94|-1.18|-1.71|-0.71|-1.54|-1.90\n",
      "\n",
      "epoch:8, train_loss: -1.032, val_loss: -0.990, \n",
      "train_vector: +0.59|-0.38|-0.89|-1.65|-1.73|-0.70|-1.56|-1.93, \n",
      "val_vector  : +0.84|-0.28|-0.93|-1.67|-1.70|-0.75|-1.53|-1.90\n",
      "\n",
      "epoch:9, train_loss: -1.081, val_loss: -1.151, \n",
      "train_vector: +0.50|-0.43|-0.93|-1.73|-1.77|-0.73|-1.60|-1.96, \n",
      "val_vector  : -0.02|-0.34|-0.96|-1.76|-1.77|-0.80|-1.67|-1.88\n",
      "\n",
      "epoch:10, train_loss: -1.117, val_loss: -0.878, \n",
      "train_vector: +0.41|-0.47|-0.96|-1.75|-1.80|-0.76|-1.62|-1.99, \n",
      "val_vector  : +1.39|+0.67|-0.98|-1.88|-1.74|-0.80|-1.67|-2.02\n",
      "\n",
      "epoch:11, train_loss: -1.117, val_loss: -1.218, \n",
      "train_vector: +0.35|-0.31|-0.97|-1.77|-1.82|-0.76|-1.64|-2.02, \n",
      "val_vector  : +0.39|-0.79|-1.03|-2.04|-1.78|-0.82|-1.70|-1.97\n",
      "\n",
      "epoch:12, train_loss: -1.192, val_loss: -1.214, \n",
      "train_vector: +0.26|-0.54|-1.01|-1.82|-1.87|-0.81|-1.69|-2.06, \n",
      "val_vector  : +0.32|-0.82|-1.07|-1.79|-1.84|-0.86|-1.62|-2.02\n",
      "\n",
      "epoch:13, train_loss: -1.216, val_loss: -1.282, \n",
      "train_vector: +0.21|-0.54|-1.04|-1.84|-1.90|-0.83|-1.71|-2.09, \n",
      "val_vector  : -0.04|-0.58|-1.09|-2.00|-1.91|-0.88|-1.67|-2.07\n",
      "\n",
      "epoch:14, train_loss: -1.251, val_loss: -1.328, \n",
      "train_vector: +0.16|-0.61|-1.06|-1.87|-1.93|-0.85|-1.73|-2.12, \n",
      "val_vector  : -0.04|-0.76|-1.15|-1.97|-1.94|-0.89|-1.78|-2.10\n",
      "\n",
      "epoch:15, train_loss: -1.277, val_loss: -1.214, \n",
      "train_vector: +0.12|-0.62|-1.08|-1.89|-1.96|-0.87|-1.76|-2.15, \n",
      "val_vector  : +0.48|-0.47|-1.14|-1.96|-1.85|-0.90|-1.77|-2.10\n",
      "\n",
      "epoch:16, train_loss: -1.296, val_loss: -1.389, \n",
      "train_vector: +0.09|-0.66|-1.10|-1.90|-1.96|-0.89|-1.78|-2.17, \n",
      "val_vector  : -0.07|-0.78|-1.17|-2.10|-2.01|-0.95|-1.88|-2.14\n",
      "\n",
      "epoch:17, train_loss: -1.313, val_loss: -1.342, \n",
      "train_vector: +0.07|-0.65|-1.12|-1.92|-1.99|-0.91|-1.79|-2.20, \n",
      "val_vector  : -0.11|-0.80|-1.14|-1.85|-2.04|-0.93|-1.72|-2.13\n",
      "\n",
      "epoch:18, train_loss: -1.334, val_loss: -1.181, \n",
      "train_vector: +0.05|-0.69|-1.14|-1.94|-2.01|-0.92|-1.81|-2.21, \n",
      "val_vector  : +0.64|-0.07|-1.16|-2.03|-1.96|-0.94|-1.76|-2.16\n",
      "\n",
      "epoch:19, train_loss: -1.352, val_loss: -1.455, \n",
      "train_vector: +0.02|-0.70|-1.16|-1.95|-2.03|-0.94|-1.83|-2.23, \n",
      "val_vector  : -0.28|-0.93|-1.23|-2.20|-2.06|-0.98|-1.81|-2.15\n",
      "\n",
      "epoch:20, train_loss: -1.367, val_loss: -1.297, \n",
      "train_vector: +0.01|-0.71|-1.17|-1.98|-2.05|-0.95|-1.84|-2.25, \n",
      "val_vector  : +0.47|-0.75|-1.20|-1.86|-2.05|-0.99|-1.84|-2.16\n",
      "\n",
      "epoch:21, train_loss: -1.393, val_loss: -1.421, \n",
      "train_vector: -0.03|-0.74|-1.19|-2.01|-2.07|-0.96|-1.87|-2.28, \n",
      "val_vector  : -0.07|-0.83|-1.24|-2.07|-2.08|-1.00|-1.84|-2.23\n",
      "\n",
      "epoch:22, train_loss: -1.406, val_loss: -1.280, \n",
      "train_vector: -0.04|-0.77|-1.20|-1.99|-2.09|-0.98|-1.88|-2.29, \n",
      "val_vector  : +0.35|-0.94|-1.21|-1.69|-1.74|-0.99|-1.79|-2.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('\\nstart fold '+str(i))\n",
    "    # parpare data\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    for j in range(5):\n",
    "        if i == j:\n",
    "            val_list.extend(folds[j])\n",
    "        else:\n",
    "            train_list.extend(folds[j])\n",
    "    \n",
    "    train_dl = DataLoader(train_list,batch_size,shuffle=True)\n",
    "    val_dl = DataLoader(val_list,batch_size,shuffle=False)\n",
    "    \n",
    "    # train model\n",
    "    model = GNN_edgeUpdate(reuse,block,head,dim,layer1,layer2,factor,**data_dict[data]).to('cuda:0')\n",
    "    paras = trainable_parameter(model)\n",
    "    opt = Adam(paras,lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5)\n",
    "    model,train_loss_list,val_loss_list,bestWeight = train_type(opt,model,epochs,train_dl,val_dl,paras,clip,scheduler=scheduler)\n",
    "    \n",
    "    # predict oof for each type\n",
    "    for type_i in range(8):\n",
    "        # load val data and type_id\n",
    "        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "            test_data = pickle.load(handle)\n",
    "        test_list = [Data(**d) for d in test_data]\n",
    "        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n",
    "        \n",
    "        with open(data.format('train').split('pickle')[0][:-1]+'_f'+str(i)+'_type_'+str(type_i)+'_id.pickle', 'rb') as handle:\n",
    "            test_id = pickle.load(handle)\n",
    "    \n",
    "        # load model\n",
    "        model.load_state_dict(bestWeight[type_i])\n",
    "    \n",
    "        # predict\n",
    "        model.eval()\n",
    "        yhat_list = []\n",
    "        with torch.no_grad():\n",
    "            for data_torch in test_dl:\n",
    "                data_torch = data_torch.to('cuda:0')\n",
    "                yhat_list.append(model(data_torch,False,True))\n",
    "        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "    \n",
    "        # join\n",
    "        submit_ = dict(zip(test_id,yhat))\n",
    "        train['fold'+str(i)+'_type'+str(type_i)] = train.id.map(submit_)\n",
    "    \n",
    "    # predict test\n",
    "    for type_i in range(8):\n",
    "        # load val data and type_id\n",
    "        with open(data.format('test').split('pickle')[0][:-1]+'_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "            test_data = pickle.load(handle)\n",
    "        test_list = [Data(**d) for d in test_data]\n",
    "        test_dl = DataLoader(test_list,batch_size,shuffle=False)\n",
    "        \n",
    "        with open(data.format('test').split('pickle')[0][:-1]+'_id_type_'+str(type_i)+'.pickle', 'rb') as handle:\n",
    "            test_id = pickle.load(handle)\n",
    "    \n",
    "        # load model\n",
    "        model.load_state_dict(bestWeight[type_i])\n",
    "    \n",
    "        # predict\n",
    "        model.eval()\n",
    "        yhat_list = []\n",
    "        with torch.no_grad():\n",
    "            for data_torch in test_dl:\n",
    "                data_torch = data_torch.to('cuda:0')\n",
    "                yhat_list.append(model(data_torch,False,True))\n",
    "        yhat = torch.cat(yhat_list).cpu().detach().numpy()        \n",
    "    \n",
    "        # join\n",
    "        submit_ = dict(zip(test_id,yhat))\n",
    "        test['fold'+str(i)+'_type'+str(type_i)] = test.id.map(submit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(test.iloc[:,5:].isnull().sum(1)) == set([7*5])\n",
    "test['yhat'] = np.nanmean(test.iloc[:,5:],1)\n",
    "test = test[['id','yhat']]\n",
    "test.to_csv('../Data/test'+prefix,index=False)\n",
    "\n",
    "assert set(train.iloc[:,6:].isnull().sum(1)) == set([train.iloc[:,6:].shape[1]-1])\n",
    "train['yhat'] = np.nanmean(train.iloc[:,6:],1)\n",
    "train = train[['id','yhat']]\n",
    "train.to_csv('../Data/train'+prefix,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
