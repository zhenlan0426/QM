{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from attention import *\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_util import RAdam,trainable_parameter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import TransformerDecoderLayer as TransformerDecoderLayer_torch\n",
    "#from pytorch_transformers import AdamW,WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "dim = 768\n",
    "head_d = 8\n",
    "head = SimplyInteraction_mol #SimplyHead\n",
    "encoder_layer,decoder_layer = 3,3\n",
    "epochs = 10\n",
    "clip = 0.1\n",
    "logLoss = True\n",
    "EncoderLayer=TransformerEncoderLayer\n",
    "DecoderLayer=TransformerDecoderLayer_torch\n",
    "\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.05\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/train_atom_list.pickle', 'rb') as handle:\n",
    "    train_atom_list=pickle.load(handle)\n",
    "with open('../Data/train_coupling_list.pickle', 'rb') as handle:\n",
    "    train_coupling_list=pickle.load(handle)    \n",
    "with open('../Data/train_index_list.pickle', 'rb') as handle:\n",
    "    train_index_list=pickle.load(handle)  \n",
    "with open('../Data/train_target_list.pickle', 'rb') as handle:\n",
    "    train_target_list=pickle.load(handle)      \n",
    "    \n",
    "with open('../Data/val_atom_list.pickle', 'rb') as handle:\n",
    "    val_atom_list=pickle.load(handle)\n",
    "with open('../Data/val_coupling_list.pickle', 'rb') as handle:\n",
    "    val_coupling_list=pickle.load(handle)    \n",
    "with open('../Data/val_index_list.pickle', 'rb') as handle:\n",
    "    val_index_list=pickle.load(handle)  \n",
    "with open('../Data/val_target_list.pickle', 'rb') as handle:\n",
    "    val_target_list=pickle.load(handle)\n",
    "\n",
    "# with open('../Data/test_atom_list.pickle', 'rb') as handle:\n",
    "#     test_atom_list=pickle.load(handle)\n",
    "# with open('../Data/test_coupling_list.pickle', 'rb') as handle:\n",
    "#     test_coupling_list=pickle.load(handle)    \n",
    "# with open('../Data/test_index_list.pickle', 'rb') as handle:\n",
    "#     test_index_list=pickle.load(handle)  \n",
    "# with open('../Data/test_target_list.pickle', 'rb') as handle:\n",
    "#     test_target_list=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = attentionDataset2(train_atom_list,train_coupling_list,train_index_list,train_target_list)\n",
    "train_dl = DataLoader(train_dl,shuffle=True,batch_size=batch_size,collate_fn=collate_fn4,num_workers=4)\n",
    "val_dl = attentionDataset2(val_atom_list,val_coupling_list,val_index_list,val_target_list)\n",
    "val_dl = DataLoader(val_dl,shuffle=True,batch_size=batch_size,collate_fn=collate_fn4,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %debug\n",
    "# atom,atom_mask,coupling,coupling_mask,index_,target = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "model=Attention_mol(dim,encoder_layer,decoder_layer,head_d,head,EncoderLayer,DecoderLayer,\n",
    "                 dropout=dropout,dim_feedforward=dim_feedforward).to('cuda')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-04)\n",
    "model, opt = amp.initialize(model, opt, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out,mask,edge,y = next(iter(train_dl))\n",
    "# out,mask,edge,y = out.to('cuda:0'),mask.to('cuda:0'),edge.to('cuda:0'),y.to('cuda:0')\n",
    "\n",
    "# opt.zero_grad()\n",
    "# loss,loss_perType = model(out,mask,edge,y,logLoss)\n",
    "# loss.backward()\n",
    "\n",
    "# [(n,p.std()) for n,p in model.named_parameters()]\n",
    "\n",
    "# [(n,p.grad.mean()) for n,p in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +0.965, val_loss: +0.531, \n",
      "train_vector: +2.45|+1.85|+0.77|+0.80|+0.46|+0.77|+0.82|-0.20, \n",
      "val_vector  : +1.59|+1.19|+0.51|+0.51|+0.07|+0.50|+0.33|-0.44\n",
      "\n",
      "epoch:1, train_loss: +0.448, val_loss: +0.145, \n",
      "train_vector: +1.93|+1.13|+0.42|+0.35|-0.20|+0.41|+0.13|-0.59, \n",
      "val_vector  : +1.38|+0.85|+0.22|-0.02|-0.55|+0.24|-0.13|-0.84\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "epoch:2, train_loss: +0.192, val_loss: -0.087, \n",
      "train_vector: +1.66|+0.88|+0.22|+0.03|-0.49|+0.22|-0.14|-0.85, \n",
      "val_vector  : +1.01|+0.62|+0.06|-0.28|-0.71|+0.05|-0.40|-1.04\n",
      "\n",
      "epoch:3, train_loss: -0.042, val_loss: -0.155, \n",
      "train_vector: +1.21|+0.71|+0.04|-0.24|-0.69|+0.04|-0.35|-1.05, \n",
      "val_vector  : +0.92|+0.92|-0.05|-0.47|-0.79|-0.03|-0.55|-1.19\n",
      "\n",
      "epoch:4, train_loss: -0.193, val_loss: -0.341, \n",
      "train_vector: +1.01|+0.60|-0.10|-0.41|-0.84|-0.11|-0.51|-1.19, \n",
      "val_vector  : +0.79|+0.47|-0.23|-0.54|-1.01|-0.24|-0.64|-1.32\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "epoch:5, train_loss: -0.302, val_loss: -0.388, \n",
      "train_vector: +0.89|+0.50|-0.21|-0.53|-0.93|-0.21|-0.63|-1.30, \n",
      "val_vector  : +0.96|+0.46|-0.39|-0.52|-1.00|-0.38|-0.80|-1.43\n",
      "\n",
      "epoch:6, train_loss: -0.383, val_loss: -0.537, \n",
      "train_vector: +0.82|+0.42|-0.29|-0.61|-1.02|-0.29|-0.72|-1.38, \n",
      "val_vector  : +0.68|+0.50|-0.50|-0.87|-1.21|-0.45|-0.93|-1.52\n",
      "\n",
      "epoch:7, train_loss: -0.452, val_loss: -0.445, \n",
      "train_vector: +0.76|+0.37|-0.37|-0.68|-1.09|-0.36|-0.79|-1.45, \n",
      "val_vector  : +1.03|+0.75|-0.51|-0.60|-1.19|-0.51|-0.98|-1.55\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "epoch:8, train_loss: -0.513, val_loss: -0.593, \n",
      "train_vector: +0.68|+0.33|-0.44|-0.74|-1.15|-0.42|-0.87|-1.51, \n",
      "val_vector  : +0.83|+0.40|-0.62|-0.88|-1.30|-0.51|-1.08|-1.58\n",
      "\n",
      "epoch:9, train_loss: -0.572, val_loss: -0.607, \n",
      "train_vector: +0.64|+0.27|-0.51|-0.82|-1.20|-0.47|-0.93|-1.56, \n",
      "val_vector  : +0.94|+0.55|-0.67|-0.98|-1.33|-0.52|-1.10|-1.73\n",
      "\n",
      "Training completed in 646.4732038974762s\n"
     ]
    }
   ],
   "source": [
    "model,bestOpt,bestWeight = train_type_mol(opt,model,epochs,train_dl,val_dl,paras,clip,\\\n",
    "                                       scheduler=scheduler,logLoss=logLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict()\n",
    "            }, '../Model/attention_edge_512_4_10.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../Model/attention_edge_512_4_10.tar')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
